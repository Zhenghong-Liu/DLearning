博客地址：https://zhuanlan.zhihu.com/p/25071913

GAN就存在着训练困难、生成器和判别器的loss无法指示训练进程、生成样本缺乏多样性等问题。


Wasserstein GAN（下面简称WGAN）成功地做到了以下爆炸性的几点：

彻底解决GAN训练不稳定的问题，不再需要小心平衡生成器和判别器的训练程度

基本解决了collapse mode的问题，确保了生成样本的多样性

训练过程中终于有一个像交叉熵、准确率这样的数值来指示训练的进程，这个数值越小代表GAN训练得越好，代表生成器产生的图像质量越高（如题图所示）

以上一切好处不需要精心设计的网络架构，最简单的多层全连接网络就可以做到


### 改进的点

判别器最后一层去掉sigmoid

生成器和判别器的loss不取log

每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c

不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行


注意原始GAN的判别器做的是真假二分类任务，所以最后一层是sigmoid，但是现在WGAN中的判别器fw
做的是近似拟合Wasserstein距离，属于回归任务，所以要把最后一层的sigmoid拿掉。


