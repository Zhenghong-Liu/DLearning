{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d2e6a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79930782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    \"\"\"Defines a fully connected neural network\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, num_hidden_layers):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer = []\n",
    "        self.layer.append(nn.Linear(input_dim, hidden_dim))\n",
    "        self.layer.append(nn.Tanh())\n",
    "        for i in range(num_hidden_layers):\n",
    "            self.layer.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.layer.append(nn.Tanh())\n",
    "        self.layer.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.model = nn.Sequential(*self.layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd929a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn = FCN(input_dim=3, output_dim=2, hidden_dim=20, num_hidden_layers=8)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "pinn.to(device)\n",
    "\n",
    "mse_cost_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(pinn.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876f21d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffab6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c7e098f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "开始训练 PINN (Re=100.0, 纯物理驱动) on cuda:0...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 283\u001b[0m\n\u001b[1;32m    281\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m \n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m开始训练 PINN (Re=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRe\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, 纯物理驱动) on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEVICE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 283\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mpinn_solver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m训练完成。\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 265\u001b[0m, in \u001b[0;36mNavierStokesPINN.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# 重新采样 PDE 点以提高收敛性\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_f, _, _, _, _, _ \u001b[38;5;241m=\u001b[39m sample_points() \n\u001b[0;32m--> 265\u001b[0m loss, loss_f, loss_ic, loss_bc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[9], line 210\u001b[0m, in \u001b[0;36mNavierStokesPINN.loss_func\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mloss_func\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# --- 1. Physics Loss (PDE Loss) ---\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     _, _, _, _, R_u, R_v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_f\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     loss_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmse(R_u, torch\u001b[38;5;241m.\u001b[39mzeros_like(R_u)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmse(R_v, torch\u001b[38;5;241m.\u001b[39mzeros_like(R_v))\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# --- 2. Initial Condition Loss (IC Loss) ---\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 183\u001b[0m, in \u001b[0;36mNavierStokesPINN.net_output\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    178\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m*\u001b[39m grad_psi_x\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# N-S residual derivatives\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# u_t, v_t\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m u_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m v_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_safe_grad(v, t, grad_outputs\u001b[38;5;241m=\u001b[39mones, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# First derivatives (Spatial)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 147\u001b[0m, in \u001b[0;36mNavierStokesPINN._safe_grad\u001b[0;34m(output, input, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_unused\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Calculate gradient\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m grad_result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# If gradient is None (due to graph break), replace it with a zero tensor\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# We return a zero tensor of the same size as the output tensor,\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# which prevents subsequent arithmetic and gradient calls from failing.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------- 0. 设备定义 --------------------------\n",
    "# Ensure CUDA is available and used for high-performance training\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# -------------------------- 1. 物理参数与几何定义 --------------------------\n",
    "\n",
    "Re = 100.0\n",
    "U_INF = 1.0  # Inlet uniform velocity\n",
    "D = 0.1  # Cylinder diameter\n",
    "NU = U_INF * D / Re  # Kinematic viscosity nu = U*D/Re\n",
    "\n",
    "# Domain [X_min, X_max] x [Y_min, Y_max] x [T_min, T_max]\n",
    "DOMAIN_X = [-1.0, 5.0]\n",
    "DOMAIN_Y = [-2.0, 2.0]\n",
    "DOMAIN_T = [0.0, 5.0]\n",
    "\n",
    "# Cylinder center and radius\n",
    "CYL_CENTER = torch.tensor([0.0, 0.0], device=DEVICE)\n",
    "CYL_R = D / 2.0\n",
    "\n",
    "# Sampling point numbers\n",
    "N_f = 20000  # PDE Collocation points\n",
    "N_cyl = 1000 # Cylinder boundary points\n",
    "N_in = 1000  # Inlet boundary points\n",
    "N_wall = 1000# Top/Bottom wall boundary points\n",
    "N_out = 1000 # Outlet boundary points\n",
    "N_ic = 2000  # Initial condition points\n",
    "\n",
    "# -------------------------- 2. 神经网络模型 --------------------------\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Input: (x, y, t) -> 3. Output: (psi, p) -> 2\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3, 30), nn.Tanh(),\n",
    "            nn.Linear(30, 30), nn.Tanh(),\n",
    "            nn.Linear(30, 30), nn.Tanh(),\n",
    "            nn.Linear(30, 30), nn.Tanh(),\n",
    "            nn.Linear(30, 30), nn.Tanh(),\n",
    "            nn.Linear(30, 30), nn.Tanh(),\n",
    "            nn.Linear(30, 30), nn.Tanh(),\n",
    "            nn.Linear(30, 2)\n",
    "        ).to(DEVICE)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        def init(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        self.apply(init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -------------------------- 3. 点采样与数据准备 (更健壮的版本) --------------------------\n",
    "\n",
    "def sample_points():\n",
    "    # --- 1. PDE Collocation Points (X_f) ---\n",
    "    # Generate points in the full domain\n",
    "    x_f_raw = torch.rand(N_f, 1, device=DEVICE) * (DOMAIN_X[1] - DOMAIN_X[0]) + DOMAIN_X[0]\n",
    "    y_f_raw = torch.rand(N_f, 1, device=DEVICE) * (DOMAIN_Y[1] - DOMAIN_Y[0]) + DOMAIN_Y[0]\n",
    "    t_f = torch.rand(N_f, 1, device=DEVICE) * (DOMAIN_T[1] - DOMAIN_T[0]) + DOMAIN_T[0]\n",
    "    \n",
    "    # Filter points outside the cylinder (this step is the known cause of the graph issue)\n",
    "    dist_sq = (x_f_raw - CYL_CENTER[0])**2 + (y_f_raw - CYL_CENTER[1])**2\n",
    "    is_outside = dist_sq >= CYL_R**2\n",
    "    \n",
    "    # Filter and concatenate\n",
    "    X_f = torch.cat([x_f_raw, y_f_raw, t_f], dim=1)[is_outside.squeeze()].requires_grad_(True)\n",
    "\n",
    "    # --- 2. Initial Condition Points (X_ic) ---\n",
    "    x_ic = torch.rand(N_ic, 1, device=DEVICE) * (DOMAIN_X[1] - DOMAIN_X[0]) + DOMAIN_X[0]\n",
    "    y_ic = torch.rand(N_ic, 1, device=DEVICE) * (DOMAIN_Y[1] - DOMAIN_Y[0]) + DOMAIN_Y[0]\n",
    "    t_ic = torch.zeros(N_ic, 1, device=DEVICE)\n",
    "    dist_sq_ic = (x_ic - CYL_CENTER[0])**2 + (y_ic - CYL_CENTER[1])**2\n",
    "    is_outside_ic = dist_sq_ic >= CYL_R**2\n",
    "    X_ic = torch.cat([x_ic, y_ic, t_ic], dim=1)[is_outside_ic.squeeze()].requires_grad_(True)\n",
    "    \n",
    "    # --- 3. Boundary Condition Points (Separated for robustness) ---\n",
    "    \n",
    "    # a) Cylinder Boundary (No-slip, psi=0)\n",
    "    theta = torch.rand(N_cyl, 1, device=DEVICE) * 2 * np.pi\n",
    "    x_cyl = CYL_R * torch.cos(theta) + CYL_CENTER[0]\n",
    "    y_cyl = CYL_R * torch.sin(theta) + CYL_CENTER[1]\n",
    "    t_cyl = torch.rand(N_cyl, 1, device=DEVICE) * (DOMAIN_T[1] - DOMAIN_T[0]) + DOMAIN_T[0]\n",
    "    X_cyl = torch.cat([x_cyl, y_cyl, t_cyl], dim=1).requires_grad_(True)\n",
    "\n",
    "    # b) Inlet Boundary (Uniform flow, x = DOMAIN_X[0])\n",
    "    x_in = torch.ones(N_in, 1, device=DEVICE) * DOMAIN_X[0]\n",
    "    y_in = torch.rand(N_in, 1, device=DEVICE) * (DOMAIN_Y[1] - DOMAIN_Y[0]) + DOMAIN_Y[0]\n",
    "    t_in = torch.rand(N_in, 1, device=DEVICE) * (DOMAIN_T[1] - DOMAIN_T[0]) + DOMAIN_T[0]\n",
    "    X_in = torch.cat([x_in, y_in, t_in], dim=1).requires_grad_(True)\n",
    "\n",
    "    # c) Wall Boundary (Top/Bottom, No-slip)\n",
    "    x_wall_top = torch.rand(N_wall//2, 1, device=DEVICE) * (DOMAIN_X[1] - DOMAIN_X[0]) + DOMAIN_X[0]\n",
    "    y_wall_top = torch.ones(N_wall//2, 1, device=DEVICE) * DOMAIN_Y[1]\n",
    "    t_wall_top = torch.rand(N_wall//2, 1, device=DEVICE) * (DOMAIN_T[1] - DOMAIN_T[0]) + DOMAIN_T[0]\n",
    "\n",
    "    x_wall_bot = torch.rand(N_wall - N_wall//2, 1, device=DEVICE) * (DOMAIN_X[1] - DOMAIN_X[0]) + DOMAIN_X[0]\n",
    "    y_wall_bot = torch.ones(N_wall - N_wall//2, 1, device=DEVICE) * DOMAIN_Y[0]\n",
    "    t_wall_bot = torch.rand(N_wall - N_wall//2, 1, device=DEVICE) * (DOMAIN_T[1] - DOMAIN_T[0]) + DOMAIN_T[0]\n",
    "    \n",
    "    # Concatenate top and bottom wall points\n",
    "    X_wall_top = torch.cat([x_wall_top, y_wall_top, t_wall_top], dim=1)\n",
    "    X_wall_bot = torch.cat([x_wall_bot, y_wall_bot, t_wall_bot], dim=1)\n",
    "    X_wall = torch.cat([X_wall_top, X_wall_bot], dim=0).requires_grad_(True)\n",
    "\n",
    "    # d) Outlet Boundary (Soft boundary, x = DOMAIN_X[1], p_x=0)\n",
    "    x_out = torch.ones(N_out, 1, device=DEVICE) * DOMAIN_X[1]\n",
    "    y_out = torch.rand(N_out, 1, device=DEVICE) * (DOMAIN_Y[1] - DOMAIN_Y[0]) + DOMAIN_Y[0]\n",
    "    t_out = torch.rand(N_out, 1, device=DEVICE) * (DOMAIN_T[1] - DOMAIN_T[0]) + DOMAIN_T[0]\n",
    "    X_out = torch.cat([x_out, y_out, t_out], dim=1).requires_grad_(True)\n",
    "    \n",
    "    return X_f, X_ic, X_cyl, X_in, X_wall, X_out\n",
    "\n",
    "# -------------------------- 4. PINN 核心实现 --------------------------\n",
    "\n",
    "class NavierStokesPINN:\n",
    "    def __init__(self, X_f, X_ic, X_cyl, X_in, X_wall, X_out):\n",
    "        self.model = Net()\n",
    "        self.X_f, self.X_ic = X_f, X_ic\n",
    "        self.X_cyl, self.X_in = X_cyl, X_in\n",
    "        self.X_wall, self.X_out = X_wall, X_out\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    @staticmethod\n",
    "    def _safe_grad(output, input, **kwargs):\n",
    "        \"\"\"\n",
    "        Calculates gradient safely, allowing for unused inputs in the graph \n",
    "        (due to data slicing) and replacing None results with zero tensors.\n",
    "        This prevents RuntimeErrors when performing higher-order differentiation.\n",
    "        \"\"\"\n",
    "        # Ensure allow_unused is always True for robustness with filtered data\n",
    "        kwargs['allow_unused'] = True\n",
    "        \n",
    "        # Calculate gradient\n",
    "        grad_result = torch.autograd.grad(output, input, **kwargs)[0]\n",
    "        \n",
    "        # If gradient is None (due to graph break), replace it with a zero tensor\n",
    "        if grad_result is None:\n",
    "            # We return a zero tensor of the same size as the output tensor,\n",
    "            # which prevents subsequent arithmetic and gradient calls from failing.\n",
    "            return torch.zeros_like(output) \n",
    "        return grad_result\n",
    "\n",
    "    def net_output(self, X):\n",
    "        # Extracts x, y, t\n",
    "        x, y, t = X[:, 0:1], X[:, 1:2], X[:, 2:3]\n",
    "        \n",
    "        # Forward pass: psi, p\n",
    "        res = self.model(X)\n",
    "        psi, p = res[:, 0:1], res[:, 1:2]\n",
    "        \n",
    "        # Create a tensor of ones for grad_outputs\n",
    "        ones = torch.ones_like(psi) \n",
    "\n",
    "        # ------------------ Automatic Differentiation ------------------\n",
    "        \n",
    "        # Velocity u = d(psi)/dy, v = -d(psi)/dx\n",
    "        # Use _safe_grad for all derivatives\n",
    "        \n",
    "        # First-order derivatives of psi\n",
    "        grad_psi_y = self._safe_grad(psi, y, grad_outputs=ones, create_graph=True)\n",
    "        grad_psi_x = self._safe_grad(psi, x, grad_outputs=ones, create_graph=True)\n",
    "\n",
    "        # Calculate velocities\n",
    "        u = grad_psi_y\n",
    "        v = -1. * grad_psi_x\n",
    "        \n",
    "        # N-S residual derivatives\n",
    "        \n",
    "        # u_t, v_t\n",
    "        u_t = self._safe_grad(u, t, grad_outputs=ones, create_graph=True)\n",
    "        v_t = self._safe_grad(v, t, grad_outputs=ones, create_graph=True)\n",
    "\n",
    "        # First derivatives (Spatial)\n",
    "        u_x = self._safe_grad(u, x, grad_outputs=ones, create_graph=True)\n",
    "        u_y = self._safe_grad(u, y, grad_outputs=ones, create_graph=True)\n",
    "        v_x = self._safe_grad(v, x, grad_outputs=ones, create_graph=True)\n",
    "        v_y = self._safe_grad(v, y, grad_outputs=ones, create_graph=True)\n",
    "\n",
    "        # Second derivatives \n",
    "        u_xx = self._safe_grad(u_x, x, grad_outputs=ones, create_graph=True)\n",
    "        u_yy = self._safe_grad(u_y, y, grad_outputs=ones, create_graph=True)\n",
    "        v_xx = self._safe_grad(v_x, x, grad_outputs=ones, create_graph=True)\n",
    "        v_yy = self._safe_grad(v_y, y, grad_outputs=ones, create_graph=True)\n",
    "        \n",
    "        # Pressure derivatives\n",
    "        p_x = self._safe_grad(p, x, grad_outputs=ones, create_graph=True)\n",
    "        p_y = self._safe_grad(p, y, grad_outputs=ones, create_graph=True)\n",
    "\n",
    "        # N-S Momentum Residuals (R_u, R_v)\n",
    "        R_u = u_t + u * u_x + v * u_y + p_x - NU * (u_xx + u_yy)\n",
    "        R_v = v_t + u * v_x + v * v_y + p_y - NU * (v_xx + v_yy)\n",
    "\n",
    "        return u, v, psi, p, R_u, R_v\n",
    "\n",
    "    def loss_func(self):\n",
    "        # --- 1. Physics Loss (PDE Loss) ---\n",
    "        _, _, _, _, R_u, R_v = self.net_output(self.X_f)\n",
    "        loss_f = self.mse(R_u, torch.zeros_like(R_u)) + self.mse(R_v, torch.zeros_like(R_v))\n",
    "        \n",
    "        # --- 2. Initial Condition Loss (IC Loss) ---\n",
    "        u_ic, v_ic, psi_ic, _, _, _ = self.net_output(self.X_ic)\n",
    "        x_ic, y_ic = self.X_ic[:, 0:1], self.X_ic[:, 1:2]\n",
    "        \n",
    "        # IC velocity U_inf, V=0\n",
    "        loss_ic_v = self.mse(u_ic, torch.ones_like(u_ic) * U_INF) + self.mse(v_ic, torch.zeros_like(v_ic))\n",
    "        # IC stream function psi = U_inf * y\n",
    "        loss_ic_psi = self.mse(psi_ic, U_INF * y_ic)\n",
    "        loss_ic = loss_ic_v + loss_ic_psi\n",
    "\n",
    "        # --- 3. Boundary Loss (BC Loss) ---\n",
    "        \n",
    "        # a) Cylinder Boundary (X_cyl) - No-slip (u=0, v=0), psi=0\n",
    "        u_cyl, v_cyl, psi_cyl, _, _, _ = self.net_output(self.X_cyl)\n",
    "        loss_cyl = self.mse(u_cyl, torch.zeros_like(u_cyl)) + self.mse(v_cyl, torch.zeros_like(v_cyl))\n",
    "        loss_cyl += self.mse(psi_cyl, torch.zeros_like(psi_cyl)) \n",
    "\n",
    "        # b) Inlet Boundary (X_in) - Uniform flow (u=U_inf, v=0), psi=U_inf*y\n",
    "        u_in, v_in, psi_in, _, _, _ = self.net_output(self.X_in)\n",
    "        y_in = self.X_in[:, 1:2]\n",
    "        loss_inlet = self.mse(u_in, torch.ones_like(u_in) * U_INF) + self.mse(v_in, torch.zeros_like(v_in))\n",
    "        loss_inlet += self.mse(psi_in, U_INF * y_in)\n",
    "        \n",
    "        # c) Wall Boundary (X_wall) - Top/Bottom No-slip (u=0, v=0)\n",
    "        u_wall, v_wall, _, _, _, _ = self.net_output(self.X_wall)\n",
    "        loss_wall = self.mse(u_wall, torch.zeros_like(u_wall)) + self.mse(v_wall, torch.zeros_like(v_wall))\n",
    "        \n",
    "        # d) Outlet Boundary (X_out) - Soft boundary (p_x=0)\n",
    "        x_out = self.X_out[:, 0:1]\n",
    "        _, _, _, p_out, _, _ = self.net_output(self.X_out)\n",
    "        \n",
    "        # Calculate p_x at the outlet\n",
    "        ones_out = torch.ones_like(p_out)\n",
    "        p_x_out = self._safe_grad(p_out, x_out, grad_outputs=ones_out, create_graph=True)\n",
    "        \n",
    "        loss_outlet = self.mse(p_x_out, torch.zeros_like(p_x_out))\n",
    "        \n",
    "        loss_bc = loss_cyl + loss_inlet + loss_wall + loss_outlet\n",
    "\n",
    "        # --- Total Loss (Weighting can be adjusted) ---\n",
    "        w_f, w_ic, w_bc = 1.0, 10.0, 10.0 \n",
    "        \n",
    "        total_loss = w_f * loss_f + w_ic * loss_ic + w_bc * loss_bc\n",
    "        \n",
    "        return total_loss, loss_f.item(), loss_ic.item(), loss_bc.item()\n",
    "\n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            # 重新采样 PDE 点以提高收敛性\n",
    "            self.X_f, _, _, _, _, _ = sample_points() \n",
    "            \n",
    "            loss, loss_f, loss_ic, loss_bc = self.loss_func()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Total Loss: {loss.item():.6f}, \"\n",
    "                      f\"PDE Loss: {loss_f:.6f}, IC Loss: {loss_ic:.6f}, BC Loss: {loss_bc:.6f}\")\n",
    "        return self.model\n",
    "\n",
    "# -------------------------- 5. 运行模拟 --------------------------\n",
    "\n",
    "# Update sample_points to return all six tensors\n",
    "X_f, X_ic, X_cyl, X_in, X_wall, X_out = sample_points()\n",
    "pinn_solver = NavierStokesPINN(X_f, X_ic, X_cyl, X_in, X_wall, X_out)\n",
    "\n",
    "# Running for 2000 epochs with Adam is often just a start for this complex problem.\n",
    "EPOCHS = 2000 \n",
    "print(f\"开始训练 PINN (Re={Re}, 纯物理驱动) on {DEVICE}...\")\n",
    "trained_model = pinn_solver.train(epochs=EPOCHS) \n",
    "print(\"训练完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afb9202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练PINN模型...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors does not require grad",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 230\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m开始训练PINN模型...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 230\u001b[0m \u001b[43mpinn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 实际应用中可能需要更多epoch\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# 创建网格用于可视化\u001b[39;00m\n\u001b[1;32m    233\u001b[0m x_range \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 212\u001b[0m, in \u001b[0;36mPINN_NavierStokes.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclosure_wrapper\u001b[39m():\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure(x_col, y_col, t_col, x_bc, y_bc, t_bc)\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_wrapper\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/optim/lbfgs.py:330\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    327\u001b[0m state\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_iter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# evaluate initial f(x) and df/dx\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m orig_loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(orig_loss)\n\u001b[1;32m    332\u001b[0m current_evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 210\u001b[0m, in \u001b[0;36mPINN_NavierStokes.train.<locals>.closure_wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclosure_wrapper\u001b[39m():\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_bc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_bc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_bc\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 153\u001b[0m, in \u001b[0;36mPINN_NavierStokes.closure\u001b[0;34m(self, x_col, y_col, t_col, x_bc, y_bc, t_bc)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# 物理方程损失 (残差)\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m _, _, _, f, g, continuity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoverning_equations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m physics_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(f\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(g\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(continuity\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# 边界条件损失\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 62\u001b[0m, in \u001b[0;36mPINN_NavierStokes.governing_equations\u001b[0;34m(self, x, y, t)\u001b[0m\n\u001b[1;32m     59\u001b[0m psi, p \u001b[38;5;241m=\u001b[39m output[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m], output[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# 从流函数得到速度分量\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpsi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpsi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     63\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(psi, x, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(psi), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# 计算速度的偏导数\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors does not require grad"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a7aeef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练PINN模型...\n",
      "Iteration: 10, Total Loss: 0.62801605, Physics Loss: 0.00272674, BC Loss: 0.62528932\n",
      "Iteration: 20, Total Loss: 0.40735930, Physics Loss: 0.03127205, BC Loss: 0.37608725\n",
      "Iteration: 30, Total Loss: 0.36808726, Physics Loss: 0.04974465, BC Loss: 0.31834263\n",
      "Iteration: 40, Total Loss: 0.32990754, Physics Loss: 0.03511898, BC Loss: 0.29478854\n",
      "Iteration: 50, Total Loss: 0.31729162, Physics Loss: 0.03386719, BC Loss: 0.28342444\n",
      "Iteration: 60, Total Loss: 0.30260631, Physics Loss: 0.01600718, BC Loss: 0.28659913\n",
      "Iteration: 70, Total Loss: 0.29214922, Physics Loss: 0.01262953, BC Loss: 0.27951968\n",
      "Iteration: 80, Total Loss: 0.28817731, Physics Loss: 0.00953262, BC Loss: 0.27864468\n",
      "Iteration: 90, Total Loss: 0.28299949, Physics Loss: 0.01097137, BC Loss: 0.27202812\n",
      "Iteration: 100, Total Loss: 0.27912918, Physics Loss: 0.01132842, BC Loss: 0.26780075\n",
      "Iteration: 110, Total Loss: 0.27688932, Physics Loss: 0.00998425, BC Loss: 0.26690507\n",
      "Iteration: 120, Total Loss: 0.27552214, Physics Loss: 0.01005232, BC Loss: 0.26546982\n",
      "Iteration: 130, Total Loss: 0.27111575, Physics Loss: 0.00955101, BC Loss: 0.26156473\n",
      "Iteration: 140, Total Loss: 0.27035403, Physics Loss: 0.01089399, BC Loss: 0.25946003\n",
      "Iteration: 150, Total Loss: 0.26762140, Physics Loss: 0.01040798, BC Loss: 0.25721341\n",
      "Iteration: 160, Total Loss: 0.26587784, Physics Loss: 0.01213253, BC Loss: 0.25374532\n",
      "Iteration: 170, Total Loss: 0.26304686, Physics Loss: 0.01437877, BC Loss: 0.24866810\n",
      "Iteration: 180, Total Loss: 0.26100120, Physics Loss: 0.01315222, BC Loss: 0.24784899\n",
      "Iteration: 190, Total Loss: 0.25707641, Physics Loss: 0.01399677, BC Loss: 0.24307963\n",
      "Iteration: 200, Total Loss: 0.25308233, Physics Loss: 0.01445296, BC Loss: 0.23862939\n",
      "Iteration: 210, Total Loss: 0.24712099, Physics Loss: 0.02238084, BC Loss: 0.22474015\n",
      "Iteration: 220, Total Loss: 0.23073985, Physics Loss: 0.02345605, BC Loss: 0.20728379\n",
      "Iteration: 230, Total Loss: 0.22355190, Physics Loss: 0.02507943, BC Loss: 0.19847247\n",
      "Iteration: 240, Total Loss: 0.21716240, Physics Loss: 0.02205498, BC Loss: 0.19510743\n",
      "Iteration: 250, Total Loss: 0.20890677, Physics Loss: 0.02615084, BC Loss: 0.18275592\n",
      "Iteration: 260, Total Loss: 0.20257795, Physics Loss: 0.02021099, BC Loss: 0.18236697\n",
      "Iteration: 270, Total Loss: 0.19860321, Physics Loss: 0.01980742, BC Loss: 0.17879578\n",
      "Iteration: 280, Total Loss: 0.19552824, Physics Loss: 0.01834837, BC Loss: 0.17717987\n",
      "Iteration: 290, Total Loss: 0.19222251, Physics Loss: 0.01666302, BC Loss: 0.17555949\n",
      "Iteration: 300, Total Loss: 0.19039476, Physics Loss: 0.01593337, BC Loss: 0.17446139\n",
      "Iteration: 310, Total Loss: 0.18847454, Physics Loss: 0.01444661, BC Loss: 0.17402793\n",
      "Iteration: 320, Total Loss: 0.18712510, Physics Loss: 0.01515927, BC Loss: 0.17196584\n",
      "Iteration: 330, Total Loss: 0.18420574, Physics Loss: 0.01509221, BC Loss: 0.16911353\n",
      "Iteration: 340, Total Loss: 0.18294823, Physics Loss: 0.01577496, BC Loss: 0.16717328\n",
      "Iteration: 350, Total Loss: 0.18125926, Physics Loss: 0.01465214, BC Loss: 0.16660711\n",
      "Iteration: 360, Total Loss: 0.17919445, Physics Loss: 0.01553370, BC Loss: 0.16366075\n",
      "Iteration: 370, Total Loss: 0.17836063, Physics Loss: 0.01440810, BC Loss: 0.16395253\n",
      "Iteration: 380, Total Loss: 0.17452446, Physics Loss: 0.01499484, BC Loss: 0.15952963\n",
      "Iteration: 390, Total Loss: 0.17209941, Physics Loss: 0.01427414, BC Loss: 0.15782526\n",
      "Iteration: 400, Total Loss: 0.17068048, Physics Loss: 0.01525597, BC Loss: 0.15542451\n",
      "Iteration: 410, Total Loss: 0.16785562, Physics Loss: 0.01326146, BC Loss: 0.15459415\n",
      "Iteration: 420, Total Loss: 0.16612531, Physics Loss: 0.01324902, BC Loss: 0.15287629\n",
      "Iteration: 430, Total Loss: 0.16368747, Physics Loss: 0.01334092, BC Loss: 0.15034655\n",
      "Iteration: 440, Total Loss: 0.16069518, Physics Loss: 0.01008026, BC Loss: 0.15061492\n",
      "Iteration: 450, Total Loss: 0.15896416, Physics Loss: 0.00903430, BC Loss: 0.14992987\n",
      "Iteration: 460, Total Loss: 0.15731981, Physics Loss: 0.00910809, BC Loss: 0.14821172\n",
      "Iteration: 470, Total Loss: 0.15489659, Physics Loss: 0.00974873, BC Loss: 0.14514786\n",
      "Iteration: 480, Total Loss: 0.15288162, Physics Loss: 0.01128161, BC Loss: 0.14160001\n",
      "Iteration: 490, Total Loss: 0.14940017, Physics Loss: 0.01209157, BC Loss: 0.13730860\n",
      "Iteration: 500, Total Loss: 0.14739126, Physics Loss: 0.01171087, BC Loss: 0.13568039\n",
      "Iteration: 510, Total Loss: 0.14533302, Physics Loss: 0.01100454, BC Loss: 0.13432848\n",
      "Iteration: 520, Total Loss: 0.14316532, Physics Loss: 0.01079166, BC Loss: 0.13237366\n",
      "Iteration: 530, Total Loss: 0.14178230, Physics Loss: 0.01112763, BC Loss: 0.13065466\n",
      "Iteration: 540, Total Loss: 0.13975942, Physics Loss: 0.01193479, BC Loss: 0.12782463\n",
      "Iteration: 550, Total Loss: 0.13809864, Physics Loss: 0.01099141, BC Loss: 0.12710723\n",
      "Iteration: 560, Total Loss: 0.13660260, Physics Loss: 0.01065876, BC Loss: 0.12594384\n",
      "Iteration: 570, Total Loss: 0.13505973, Physics Loss: 0.01058769, BC Loss: 0.12447204\n",
      "Iteration: 580, Total Loss: 0.13399121, Physics Loss: 0.00992666, BC Loss: 0.12406455\n",
      "Iteration: 590, Total Loss: 0.13318902, Physics Loss: 0.01009463, BC Loss: 0.12309439\n",
      "Iteration: 600, Total Loss: 0.13216832, Physics Loss: 0.01052585, BC Loss: 0.12164247\n",
      "Iteration: 610, Total Loss: 0.13123389, Physics Loss: 0.01012306, BC Loss: 0.12111083\n",
      "Iteration: 620, Total Loss: 0.13061531, Physics Loss: 0.01046312, BC Loss: 0.12015218\n",
      "Iteration: 630, Total Loss: 0.13017321, Physics Loss: 0.01000183, BC Loss: 0.12017137\n",
      "Iteration: 640, Total Loss: 0.12963229, Physics Loss: 0.00976007, BC Loss: 0.11987223\n",
      "Iteration: 650, Total Loss: 0.12909272, Physics Loss: 0.00941617, BC Loss: 0.11967656\n",
      "Iteration: 660, Total Loss: 0.12851387, Physics Loss: 0.00944061, BC Loss: 0.11907326\n",
      "Iteration: 670, Total Loss: 0.12819655, Physics Loss: 0.00954664, BC Loss: 0.11864991\n",
      "Iteration: 680, Total Loss: 0.12780879, Physics Loss: 0.00967203, BC Loss: 0.11813677\n",
      "Iteration: 690, Total Loss: 0.12749133, Physics Loss: 0.00989676, BC Loss: 0.11759457\n",
      "Iteration: 700, Total Loss: 0.12703544, Physics Loss: 0.00980760, BC Loss: 0.11722784\n",
      "Iteration: 710, Total Loss: 0.12676859, Physics Loss: 0.00953781, BC Loss: 0.11723077\n",
      "Iteration: 720, Total Loss: 0.12651494, Physics Loss: 0.00920575, BC Loss: 0.11730919\n",
      "Iteration: 730, Total Loss: 0.12603468, Physics Loss: 0.00872428, BC Loss: 0.11731039\n",
      "Iteration: 740, Total Loss: 0.12559994, Physics Loss: 0.00861070, BC Loss: 0.11698924\n",
      "Iteration: 750, Total Loss: 0.12506630, Physics Loss: 0.00917586, BC Loss: 0.11589043\n",
      "Iteration: 760, Total Loss: 0.12483826, Physics Loss: 0.00903042, BC Loss: 0.11580783\n",
      "Iteration: 770, Total Loss: 0.12454006, Physics Loss: 0.00866386, BC Loss: 0.11587620\n",
      "Iteration: 780, Total Loss: 0.12423561, Physics Loss: 0.00879028, BC Loss: 0.11544533\n",
      "Iteration: 790, Total Loss: 0.12391812, Physics Loss: 0.00890194, BC Loss: 0.11501618\n",
      "Iteration: 800, Total Loss: 0.12358891, Physics Loss: 0.00913531, BC Loss: 0.11445361\n",
      "Iteration: 810, Total Loss: 0.12320426, Physics Loss: 0.00894315, BC Loss: 0.11426111\n",
      "Iteration: 820, Total Loss: 0.12296709, Physics Loss: 0.00910866, BC Loss: 0.11385843\n",
      "Iteration: 830, Total Loss: 0.12266367, Physics Loss: 0.00869725, BC Loss: 0.11396642\n",
      "Iteration: 840, Total Loss: 0.12230816, Physics Loss: 0.00876113, BC Loss: 0.11354703\n",
      "Iteration: 850, Total Loss: 0.12207939, Physics Loss: 0.00904225, BC Loss: 0.11303714\n",
      "Iteration: 860, Total Loss: 0.12185589, Physics Loss: 0.00878028, BC Loss: 0.11307561\n",
      "Iteration: 870, Total Loss: 0.12164221, Physics Loss: 0.00894417, BC Loss: 0.11269804\n",
      "Iteration: 880, Total Loss: 0.12143649, Physics Loss: 0.00881019, BC Loss: 0.11262630\n",
      "Iteration: 890, Total Loss: 0.12112336, Physics Loss: 0.00874460, BC Loss: 0.11237875\n",
      "Iteration: 900, Total Loss: 0.12078987, Physics Loss: 0.00856182, BC Loss: 0.11222805\n",
      "Iteration: 910, Total Loss: 0.12054771, Physics Loss: 0.00857139, BC Loss: 0.11197633\n",
      "Iteration: 920, Total Loss: 0.12021282, Physics Loss: 0.00863664, BC Loss: 0.11157618\n",
      "Iteration: 930, Total Loss: 0.12008987, Physics Loss: 0.00853676, BC Loss: 0.11155310\n",
      "Iteration: 940, Total Loss: 0.11986178, Physics Loss: 0.00856556, BC Loss: 0.11129622\n",
      "Iteration: 950, Total Loss: 0.11959476, Physics Loss: 0.00886718, BC Loss: 0.11072758\n",
      "Iteration: 960, Total Loss: 0.11923990, Physics Loss: 0.00818337, BC Loss: 0.11105653\n",
      "Iteration: 970, Total Loss: 0.11898902, Physics Loss: 0.00844366, BC Loss: 0.11054537\n",
      "Iteration: 980, Total Loss: 0.11879959, Physics Loss: 0.00827913, BC Loss: 0.11052046\n",
      "Iteration: 990, Total Loss: 0.11848611, Physics Loss: 0.00866275, BC Loss: 0.10982335\n",
      "Iteration: 1000, Total Loss: 0.11820143, Physics Loss: 0.00859501, BC Loss: 0.10960642\n",
      "Iteration: 1010, Total Loss: 0.10015483, Physics Loss: 0.00855258, BC Loss: 0.09160225\n",
      "Iteration: 1020, Total Loss: 0.09971205, Physics Loss: 0.00866441, BC Loss: 0.09104764\n",
      "Iteration: 1030, Total Loss: 0.09942650, Physics Loss: 0.00834524, BC Loss: 0.09108126\n",
      "Iteration: 1040, Total Loss: 0.09887053, Physics Loss: 0.00830393, BC Loss: 0.09056660\n",
      "Iteration: 1050, Total Loss: 0.09836782, Physics Loss: 0.00822562, BC Loss: 0.09014220\n",
      "Iteration: 1060, Total Loss: 0.09781939, Physics Loss: 0.00776337, BC Loss: 0.09005602\n",
      "Iteration: 1070, Total Loss: 0.09714625, Physics Loss: 0.00724490, BC Loss: 0.08990135\n",
      "Iteration: 1080, Total Loss: 0.09674007, Physics Loss: 0.00674288, BC Loss: 0.08999719\n",
      "Iteration: 1090, Total Loss: 0.09639592, Physics Loss: 0.00645910, BC Loss: 0.08993682\n",
      "Iteration: 1100, Total Loss: 0.09613463, Physics Loss: 0.00632228, BC Loss: 0.08981234\n",
      "Iteration: 1110, Total Loss: 0.09595574, Physics Loss: 0.00624837, BC Loss: 0.08970737\n",
      "Iteration: 1120, Total Loss: 0.09571268, Physics Loss: 0.00598341, BC Loss: 0.08972926\n",
      "Iteration: 1130, Total Loss: 0.09543332, Physics Loss: 0.00581712, BC Loss: 0.08961620\n",
      "Iteration: 1140, Total Loss: 0.09526695, Physics Loss: 0.00592854, BC Loss: 0.08933841\n",
      "Iteration: 1150, Total Loss: 0.09510107, Physics Loss: 0.00596097, BC Loss: 0.08914009\n",
      "Iteration: 1160, Total Loss: 0.09494685, Physics Loss: 0.00601505, BC Loss: 0.08893180\n",
      "Iteration: 1170, Total Loss: 0.09475872, Physics Loss: 0.00596309, BC Loss: 0.08879563\n",
      "Iteration: 1180, Total Loss: 0.09455545, Physics Loss: 0.00603142, BC Loss: 0.08852403\n",
      "Iteration: 1190, Total Loss: 0.09413642, Physics Loss: 0.00593096, BC Loss: 0.08820546\n",
      "Iteration: 1200, Total Loss: 0.09381866, Physics Loss: 0.00597816, BC Loss: 0.08784050\n",
      "Iteration: 1210, Total Loss: 0.09355261, Physics Loss: 0.00634635, BC Loss: 0.08720626\n",
      "Iteration: 1220, Total Loss: 0.09327680, Physics Loss: 0.00653086, BC Loss: 0.08674594\n",
      "Iteration: 1230, Total Loss: 0.09301465, Physics Loss: 0.00665930, BC Loss: 0.08635534\n",
      "Iteration: 1240, Total Loss: 0.09283232, Physics Loss: 0.00705973, BC Loss: 0.08577259\n",
      "Iteration: 1250, Total Loss: 0.09247467, Physics Loss: 0.00713008, BC Loss: 0.08534459\n",
      "Iteration: 1260, Total Loss: 0.09218659, Physics Loss: 0.00703425, BC Loss: 0.08515233\n",
      "Iteration: 1270, Total Loss: 0.09197983, Physics Loss: 0.00745705, BC Loss: 0.08452278\n",
      "Iteration: 1280, Total Loss: 0.09155757, Physics Loss: 0.00729693, BC Loss: 0.08426064\n",
      "Iteration: 1290, Total Loss: 0.09122699, Physics Loss: 0.00719648, BC Loss: 0.08403052\n",
      "Iteration: 1300, Total Loss: 0.09087855, Physics Loss: 0.00742420, BC Loss: 0.08345435\n",
      "Iteration: 1310, Total Loss: 0.09054840, Physics Loss: 0.00709161, BC Loss: 0.08345679\n",
      "Iteration: 1320, Total Loss: 0.08992360, Physics Loss: 0.00724242, BC Loss: 0.08268118\n",
      "Iteration: 1330, Total Loss: 0.08953919, Physics Loss: 0.00702319, BC Loss: 0.08251600\n",
      "Iteration: 1340, Total Loss: 0.08908629, Physics Loss: 0.00750414, BC Loss: 0.08158214\n",
      "Iteration: 1350, Total Loss: 0.08885480, Physics Loss: 0.00741312, BC Loss: 0.08144169\n",
      "Iteration: 1360, Total Loss: 0.08851889, Physics Loss: 0.00743059, BC Loss: 0.08108830\n",
      "Iteration: 1370, Total Loss: 0.08830252, Physics Loss: 0.00743961, BC Loss: 0.08086291\n",
      "Iteration: 1380, Total Loss: 0.08800387, Physics Loss: 0.00730241, BC Loss: 0.08070146\n",
      "Iteration: 1390, Total Loss: 0.08766783, Physics Loss: 0.00701925, BC Loss: 0.08064858\n",
      "Iteration: 1400, Total Loss: 0.08711905, Physics Loss: 0.00742062, BC Loss: 0.07969843\n",
      "Iteration: 1410, Total Loss: 0.08689071, Physics Loss: 0.00703751, BC Loss: 0.07985321\n",
      "Iteration: 1420, Total Loss: 0.08651146, Physics Loss: 0.00713894, BC Loss: 0.07937251\n",
      "Iteration: 1430, Total Loss: 0.08622109, Physics Loss: 0.00702072, BC Loss: 0.07920037\n",
      "Iteration: 1440, Total Loss: 0.08580603, Physics Loss: 0.00716304, BC Loss: 0.07864300\n",
      "Iteration: 1450, Total Loss: 0.08539455, Physics Loss: 0.00746026, BC Loss: 0.07793429\n",
      "Iteration: 1460, Total Loss: 0.08510149, Physics Loss: 0.00712961, BC Loss: 0.07797188\n",
      "Iteration: 1470, Total Loss: 0.08464140, Physics Loss: 0.00724601, BC Loss: 0.07739539\n",
      "Iteration: 1480, Total Loss: 0.08442603, Physics Loss: 0.00727600, BC Loss: 0.07715003\n",
      "Iteration: 1490, Total Loss: 0.08406308, Physics Loss: 0.00748493, BC Loss: 0.07657815\n",
      "Iteration: 1500, Total Loss: 0.08376450, Physics Loss: 0.00764172, BC Loss: 0.07612278\n",
      "Iteration: 1510, Total Loss: 0.08345404, Physics Loss: 0.00781768, BC Loss: 0.07563636\n",
      "Iteration: 1520, Total Loss: 0.08317987, Physics Loss: 0.00768958, BC Loss: 0.07549029\n",
      "Iteration: 1530, Total Loss: 0.08291906, Physics Loss: 0.00765117, BC Loss: 0.07526789\n",
      "Iteration: 1540, Total Loss: 0.08270155, Physics Loss: 0.00779114, BC Loss: 0.07491040\n",
      "Iteration: 1550, Total Loss: 0.08244395, Physics Loss: 0.00781790, BC Loss: 0.07462605\n",
      "Iteration: 1560, Total Loss: 0.08213168, Physics Loss: 0.00757860, BC Loss: 0.07455309\n",
      "Iteration: 1570, Total Loss: 0.08191609, Physics Loss: 0.00756609, BC Loss: 0.07435000\n",
      "Iteration: 1580, Total Loss: 0.08161537, Physics Loss: 0.00735779, BC Loss: 0.07425758\n",
      "Iteration: 1590, Total Loss: 0.08140945, Physics Loss: 0.00734958, BC Loss: 0.07405987\n",
      "Iteration: 1600, Total Loss: 0.08119032, Physics Loss: 0.00702623, BC Loss: 0.07416409\n",
      "Iteration: 1610, Total Loss: 0.08083209, Physics Loss: 0.00669134, BC Loss: 0.07414074\n",
      "Iteration: 1620, Total Loss: 0.08057154, Physics Loss: 0.00673926, BC Loss: 0.07383228\n",
      "Iteration: 1630, Total Loss: 0.08029079, Physics Loss: 0.00674103, BC Loss: 0.07354976\n",
      "Iteration: 1640, Total Loss: 0.08010259, Physics Loss: 0.00671606, BC Loss: 0.07338653\n",
      "Iteration: 1650, Total Loss: 0.07978016, Physics Loss: 0.00651881, BC Loss: 0.07326135\n",
      "Iteration: 1660, Total Loss: 0.07958996, Physics Loss: 0.00646811, BC Loss: 0.07312185\n",
      "Iteration: 1670, Total Loss: 0.07950167, Physics Loss: 0.00650819, BC Loss: 0.07299347\n",
      "Iteration: 1680, Total Loss: 0.07939807, Physics Loss: 0.00661219, BC Loss: 0.07278588\n",
      "Iteration: 1690, Total Loss: 0.07924228, Physics Loss: 0.00667161, BC Loss: 0.07257067\n",
      "Iteration: 1700, Total Loss: 0.07899324, Physics Loss: 0.00659406, BC Loss: 0.07239918\n",
      "Iteration: 1710, Total Loss: 0.07872294, Physics Loss: 0.00663541, BC Loss: 0.07208753\n",
      "Iteration: 1720, Total Loss: 0.07850327, Physics Loss: 0.00647655, BC Loss: 0.07202672\n",
      "Iteration: 1730, Total Loss: 0.07824192, Physics Loss: 0.00647396, BC Loss: 0.07176796\n",
      "Iteration: 1740, Total Loss: 0.07804412, Physics Loss: 0.00645319, BC Loss: 0.07159092\n",
      "Iteration: 1750, Total Loss: 0.07787685, Physics Loss: 0.00632994, BC Loss: 0.07154691\n",
      "Iteration: 1760, Total Loss: 0.07762635, Physics Loss: 0.00627884, BC Loss: 0.07134752\n",
      "Iteration: 1770, Total Loss: 0.07742436, Physics Loss: 0.00643200, BC Loss: 0.07099237\n",
      "Iteration: 1780, Total Loss: 0.07719839, Physics Loss: 0.00635086, BC Loss: 0.07084753\n",
      "Iteration: 1790, Total Loss: 0.07705107, Physics Loss: 0.00621188, BC Loss: 0.07083918\n",
      "Iteration: 1800, Total Loss: 0.07685732, Physics Loss: 0.00642697, BC Loss: 0.07043035\n",
      "Iteration: 1810, Total Loss: 0.07669646, Physics Loss: 0.00646335, BC Loss: 0.07023311\n",
      "Iteration: 1820, Total Loss: 0.07644400, Physics Loss: 0.00682588, BC Loss: 0.06961812\n",
      "Iteration: 1830, Total Loss: 0.07609402, Physics Loss: 0.00678587, BC Loss: 0.06930815\n",
      "Iteration: 1840, Total Loss: 0.07586171, Physics Loss: 0.00668375, BC Loss: 0.06917796\n",
      "Iteration: 1850, Total Loss: 0.07552598, Physics Loss: 0.00680714, BC Loss: 0.06871884\n",
      "Iteration: 1860, Total Loss: 0.07535564, Physics Loss: 0.00689651, BC Loss: 0.06845913\n",
      "Iteration: 1870, Total Loss: 0.07503317, Physics Loss: 0.00699972, BC Loss: 0.06803346\n",
      "Iteration: 1880, Total Loss: 0.07474511, Physics Loss: 0.00700227, BC Loss: 0.06774284\n",
      "Iteration: 1890, Total Loss: 0.07437592, Physics Loss: 0.00707653, BC Loss: 0.06729940\n",
      "Iteration: 1900, Total Loss: 0.07422169, Physics Loss: 0.00715873, BC Loss: 0.06706296\n",
      "Iteration: 1910, Total Loss: 0.07404941, Physics Loss: 0.00720204, BC Loss: 0.06684737\n",
      "Iteration: 1920, Total Loss: 0.07382257, Physics Loss: 0.00725761, BC Loss: 0.06656496\n",
      "Iteration: 1930, Total Loss: 0.07363579, Physics Loss: 0.00711364, BC Loss: 0.06652216\n",
      "Iteration: 1940, Total Loss: 0.07344157, Physics Loss: 0.00724659, BC Loss: 0.06619497\n",
      "Iteration: 1950, Total Loss: 0.07330266, Physics Loss: 0.00725347, BC Loss: 0.06604919\n",
      "Iteration: 1960, Total Loss: 0.07305700, Physics Loss: 0.00742433, BC Loss: 0.06563268\n",
      "Iteration: 1970, Total Loss: 0.07288231, Physics Loss: 0.00750751, BC Loss: 0.06537479\n",
      "Iteration: 1980, Total Loss: 0.07255571, Physics Loss: 0.00764117, BC Loss: 0.06491454\n",
      "Iteration: 1990, Total Loss: 0.07214924, Physics Loss: 0.00745993, BC Loss: 0.06468931\n",
      "Iteration: 2000, Total Loss: 0.07177629, Physics Loss: 0.00753945, BC Loss: 0.06423684\n",
      "Iteration: 2010, Total Loss: 0.17804430, Physics Loss: 0.07550937, BC Loss: 0.10253493\n",
      "Iteration: 2020, Total Loss: 0.13068262, Physics Loss: 0.02990896, BC Loss: 0.10077366\n",
      "Iteration: 2030, Total Loss: 0.11822302, Physics Loss: 0.01763390, BC Loss: 0.10058912\n",
      "Iteration: 2040, Total Loss: 0.11382242, Physics Loss: 0.01363198, BC Loss: 0.10019045\n",
      "Iteration: 2050, Total Loss: 0.11097950, Physics Loss: 0.01155994, BC Loss: 0.09941955\n",
      "Iteration: 2060, Total Loss: 0.10946825, Physics Loss: 0.01073195, BC Loss: 0.09873630\n",
      "Iteration: 2070, Total Loss: 0.10838160, Physics Loss: 0.01101710, BC Loss: 0.09736450\n",
      "Iteration: 2080, Total Loss: 0.10728803, Physics Loss: 0.01133433, BC Loss: 0.09595370\n",
      "Iteration: 2090, Total Loss: 0.10548603, Physics Loss: 0.01071284, BC Loss: 0.09477319\n",
      "Iteration: 2100, Total Loss: 0.10374159, Physics Loss: 0.01162866, BC Loss: 0.09211292\n",
      "Iteration: 2110, Total Loss: 0.10272936, Physics Loss: 0.01163507, BC Loss: 0.09109429\n",
      "Iteration: 2120, Total Loss: 0.10196200, Physics Loss: 0.01168865, BC Loss: 0.09027335\n",
      "Iteration: 2130, Total Loss: 0.10073245, Physics Loss: 0.01164438, BC Loss: 0.08908807\n",
      "Iteration: 2140, Total Loss: 0.09971160, Physics Loss: 0.01185865, BC Loss: 0.08785295\n",
      "Iteration: 2150, Total Loss: 0.09861967, Physics Loss: 0.01269691, BC Loss: 0.08592276\n",
      "Iteration: 2160, Total Loss: 0.09809911, Physics Loss: 0.01196445, BC Loss: 0.08613466\n",
      "Iteration: 2170, Total Loss: 0.09690137, Physics Loss: 0.01142829, BC Loss: 0.08547308\n",
      "Iteration: 2180, Total Loss: 0.09603332, Physics Loss: 0.01133886, BC Loss: 0.08469446\n",
      "Iteration: 2190, Total Loss: 0.09502038, Physics Loss: 0.01185154, BC Loss: 0.08316884\n",
      "Iteration: 2200, Total Loss: 0.09414887, Physics Loss: 0.01132821, BC Loss: 0.08282065\n",
      "Iteration: 2210, Total Loss: 0.09363247, Physics Loss: 0.01126692, BC Loss: 0.08236555\n",
      "Iteration: 2220, Total Loss: 0.09272648, Physics Loss: 0.01074590, BC Loss: 0.08198058\n",
      "Iteration: 2230, Total Loss: 0.09171892, Physics Loss: 0.01043743, BC Loss: 0.08128149\n",
      "Iteration: 2240, Total Loss: 0.09062952, Physics Loss: 0.01011079, BC Loss: 0.08051872\n",
      "Iteration: 2250, Total Loss: 0.08977266, Physics Loss: 0.00987362, BC Loss: 0.07989904\n",
      "Iteration: 2260, Total Loss: 0.08897403, Physics Loss: 0.00959738, BC Loss: 0.07937665\n",
      "Iteration: 2270, Total Loss: 0.08830681, Physics Loss: 0.00936548, BC Loss: 0.07894133\n",
      "Iteration: 2280, Total Loss: 0.08758006, Physics Loss: 0.00952788, BC Loss: 0.07805219\n",
      "Iteration: 2290, Total Loss: 0.08704621, Physics Loss: 0.00990226, BC Loss: 0.07714395\n",
      "Iteration: 2300, Total Loss: 0.08633887, Physics Loss: 0.00973371, BC Loss: 0.07660516\n",
      "Iteration: 2310, Total Loss: 0.08577117, Physics Loss: 0.00994951, BC Loss: 0.07582165\n",
      "Iteration: 2320, Total Loss: 0.08540127, Physics Loss: 0.00979167, BC Loss: 0.07560960\n",
      "Iteration: 2330, Total Loss: 0.08510711, Physics Loss: 0.01002859, BC Loss: 0.07507852\n",
      "Iteration: 2340, Total Loss: 0.08446882, Physics Loss: 0.00979728, BC Loss: 0.07467154\n",
      "Iteration: 2350, Total Loss: 0.08393652, Physics Loss: 0.00990179, BC Loss: 0.07403474\n",
      "Iteration: 2360, Total Loss: 0.08347154, Physics Loss: 0.01000095, BC Loss: 0.07347059\n",
      "Iteration: 2370, Total Loss: 0.08282030, Physics Loss: 0.01019274, BC Loss: 0.07262756\n",
      "Iteration: 2380, Total Loss: 0.08230948, Physics Loss: 0.01041115, BC Loss: 0.07189833\n",
      "Iteration: 2390, Total Loss: 0.08148836, Physics Loss: 0.01014950, BC Loss: 0.07133886\n",
      "Iteration: 2400, Total Loss: 0.08078225, Physics Loss: 0.00994424, BC Loss: 0.07083800\n",
      "Iteration: 2410, Total Loss: 0.08033132, Physics Loss: 0.01001904, BC Loss: 0.07031228\n",
      "Iteration: 2420, Total Loss: 0.07984690, Physics Loss: 0.00964027, BC Loss: 0.07020663\n",
      "Iteration: 2430, Total Loss: 0.07930018, Physics Loss: 0.00990101, BC Loss: 0.06939916\n",
      "Iteration: 2440, Total Loss: 0.07904387, Physics Loss: 0.00953181, BC Loss: 0.06951205\n",
      "Iteration: 2450, Total Loss: 0.07846846, Physics Loss: 0.00980405, BC Loss: 0.06866441\n",
      "Iteration: 2460, Total Loss: 0.07793833, Physics Loss: 0.01014134, BC Loss: 0.06779698\n",
      "Iteration: 2470, Total Loss: 0.07711270, Physics Loss: 0.01041091, BC Loss: 0.06670178\n",
      "Iteration: 2480, Total Loss: 0.07644081, Physics Loss: 0.01032094, BC Loss: 0.06611987\n",
      "Iteration: 2490, Total Loss: 0.07559302, Physics Loss: 0.01029867, BC Loss: 0.06529434\n",
      "Iteration: 2500, Total Loss: 0.07480557, Physics Loss: 0.01077516, BC Loss: 0.06403041\n",
      "Iteration: 2510, Total Loss: 0.07424466, Physics Loss: 0.01089650, BC Loss: 0.06334817\n",
      "Iteration: 2520, Total Loss: 0.07364126, Physics Loss: 0.01081698, BC Loss: 0.06282427\n",
      "Iteration: 2530, Total Loss: 0.07294762, Physics Loss: 0.01013786, BC Loss: 0.06280977\n",
      "Iteration: 2540, Total Loss: 0.07263219, Physics Loss: 0.01012890, BC Loss: 0.06250329\n",
      "Iteration: 2550, Total Loss: 0.07229606, Physics Loss: 0.01049312, BC Loss: 0.06180294\n",
      "Iteration: 2560, Total Loss: 0.07182547, Physics Loss: 0.01018764, BC Loss: 0.06163783\n",
      "Iteration: 2570, Total Loss: 0.07127048, Physics Loss: 0.01017283, BC Loss: 0.06109765\n",
      "Iteration: 2580, Total Loss: 0.07074457, Physics Loss: 0.01042902, BC Loss: 0.06031556\n",
      "Iteration: 2590, Total Loss: 0.07011352, Physics Loss: 0.01060124, BC Loss: 0.05951229\n",
      "Iteration: 2600, Total Loss: 0.06978472, Physics Loss: 0.01095791, BC Loss: 0.05882681\n",
      "Iteration: 2610, Total Loss: 0.06885098, Physics Loss: 0.01091737, BC Loss: 0.05793361\n",
      "Iteration: 2620, Total Loss: 0.06811478, Physics Loss: 0.01102635, BC Loss: 0.05708843\n",
      "Iteration: 2630, Total Loss: 0.06734148, Physics Loss: 0.01128699, BC Loss: 0.05605449\n",
      "Iteration: 2640, Total Loss: 0.06677330, Physics Loss: 0.01062215, BC Loss: 0.05615115\n",
      "Iteration: 2650, Total Loss: 0.06628448, Physics Loss: 0.01046564, BC Loss: 0.05581884\n",
      "Iteration: 2660, Total Loss: 0.06559388, Physics Loss: 0.01030257, BC Loss: 0.05529131\n",
      "Iteration: 2670, Total Loss: 0.06483612, Physics Loss: 0.00992134, BC Loss: 0.05491479\n",
      "Iteration: 2680, Total Loss: 0.06433728, Physics Loss: 0.00972568, BC Loss: 0.05461161\n",
      "Iteration: 2690, Total Loss: 0.06389382, Physics Loss: 0.00965913, BC Loss: 0.05423469\n",
      "Iteration: 2700, Total Loss: 0.06355004, Physics Loss: 0.00981013, BC Loss: 0.05373991\n",
      "Iteration: 2710, Total Loss: 0.06312815, Physics Loss: 0.00996919, BC Loss: 0.05315896\n",
      "Iteration: 2720, Total Loss: 0.06242982, Physics Loss: 0.00946070, BC Loss: 0.05296912\n",
      "Iteration: 2730, Total Loss: 0.06183821, Physics Loss: 0.00934972, BC Loss: 0.05248849\n",
      "Iteration: 2740, Total Loss: 0.06123401, Physics Loss: 0.00929101, BC Loss: 0.05194300\n",
      "Iteration: 2750, Total Loss: 0.06060188, Physics Loss: 0.00956139, BC Loss: 0.05104049\n",
      "Iteration: 2760, Total Loss: 0.06018159, Physics Loss: 0.00901024, BC Loss: 0.05117134\n",
      "Iteration: 2770, Total Loss: 0.05981635, Physics Loss: 0.00918571, BC Loss: 0.05063064\n",
      "Iteration: 2780, Total Loss: 0.05944935, Physics Loss: 0.00897532, BC Loss: 0.05047403\n",
      "Iteration: 2790, Total Loss: 0.05918461, Physics Loss: 0.00922076, BC Loss: 0.04996385\n",
      "Iteration: 2800, Total Loss: 0.05885084, Physics Loss: 0.00914413, BC Loss: 0.04970671\n",
      "Iteration: 2810, Total Loss: 0.05852831, Physics Loss: 0.00903444, BC Loss: 0.04949388\n",
      "Iteration: 2820, Total Loss: 0.05808230, Physics Loss: 0.00880578, BC Loss: 0.04927653\n",
      "Iteration: 2830, Total Loss: 0.05773065, Physics Loss: 0.00859831, BC Loss: 0.04913234\n",
      "Iteration: 2840, Total Loss: 0.05747420, Physics Loss: 0.00851400, BC Loss: 0.04896020\n",
      "Iteration: 2850, Total Loss: 0.05713764, Physics Loss: 0.00863866, BC Loss: 0.04849898\n",
      "Iteration: 2860, Total Loss: 0.05689725, Physics Loss: 0.00852586, BC Loss: 0.04837139\n",
      "Iteration: 2870, Total Loss: 0.05666200, Physics Loss: 0.00865522, BC Loss: 0.04800678\n",
      "Iteration: 2880, Total Loss: 0.05650419, Physics Loss: 0.00873009, BC Loss: 0.04777410\n",
      "Iteration: 2890, Total Loss: 0.05628061, Physics Loss: 0.00878503, BC Loss: 0.04749558\n",
      "Iteration: 2900, Total Loss: 0.05609293, Physics Loss: 0.00903547, BC Loss: 0.04705746\n",
      "Iteration: 2910, Total Loss: 0.05588897, Physics Loss: 0.00934152, BC Loss: 0.04654744\n",
      "Iteration: 2920, Total Loss: 0.05559533, Physics Loss: 0.00894210, BC Loss: 0.04665322\n",
      "Iteration: 2930, Total Loss: 0.05532260, Physics Loss: 0.00897376, BC Loss: 0.04634884\n",
      "Iteration: 2940, Total Loss: 0.05499091, Physics Loss: 0.00937141, BC Loss: 0.04561950\n",
      "Iteration: 2950, Total Loss: 0.05462244, Physics Loss: 0.00915151, BC Loss: 0.04547093\n",
      "Iteration: 2960, Total Loss: 0.05435072, Physics Loss: 0.00897669, BC Loss: 0.04537403\n",
      "Iteration: 2970, Total Loss: 0.05406145, Physics Loss: 0.00897114, BC Loss: 0.04509031\n",
      "Iteration: 2980, Total Loss: 0.05371094, Physics Loss: 0.00906331, BC Loss: 0.04464763\n",
      "Iteration: 2990, Total Loss: 0.05349791, Physics Loss: 0.00910107, BC Loss: 0.04439684\n",
      "Iteration: 3000, Total Loss: 0.05318366, Physics Loss: 0.00938540, BC Loss: 0.04379826\n",
      "Iteration: 3010, Total Loss: 2.05397272, Physics Loss: 1.90330577, BC Loss: 0.15066707\n",
      "Iteration: 3020, Total Loss: 0.51093155, Physics Loss: 0.37437558, BC Loss: 0.13655598\n",
      "Iteration: 3030, Total Loss: 0.30431941, Physics Loss: 0.18672167, BC Loss: 0.11759775\n",
      "Iteration: 3040, Total Loss: 0.18100297, Physics Loss: 0.10295704, BC Loss: 0.07804593\n",
      "Iteration: 3050, Total Loss: 0.12256628, Physics Loss: 0.04583906, BC Loss: 0.07672721\n",
      "Iteration: 3060, Total Loss: 0.11135113, Physics Loss: 0.03255897, BC Loss: 0.07879215\n",
      "Iteration: 3070, Total Loss: 0.10268992, Physics Loss: 0.02675645, BC Loss: 0.07593347\n",
      "Iteration: 3080, Total Loss: 0.10014087, Physics Loss: 0.02635610, BC Loss: 0.07378477\n",
      "Iteration: 3090, Total Loss: 0.09623605, Physics Loss: 0.02156251, BC Loss: 0.07467355\n",
      "Iteration: 3100, Total Loss: 0.09276670, Physics Loss: 0.02148838, BC Loss: 0.07127832\n",
      "Iteration: 3110, Total Loss: 0.08889689, Physics Loss: 0.01945583, BC Loss: 0.06944107\n",
      "Iteration: 3120, Total Loss: 0.08559613, Physics Loss: 0.01857876, BC Loss: 0.06701737\n",
      "Iteration: 3130, Total Loss: 0.08228317, Physics Loss: 0.01748886, BC Loss: 0.06479431\n",
      "Iteration: 3140, Total Loss: 0.08016737, Physics Loss: 0.01585829, BC Loss: 0.06430908\n",
      "Iteration: 3150, Total Loss: 0.07822477, Physics Loss: 0.01529169, BC Loss: 0.06293309\n",
      "Iteration: 3160, Total Loss: 0.07640143, Physics Loss: 0.01365602, BC Loss: 0.06274541\n",
      "Iteration: 3170, Total Loss: 0.07509468, Physics Loss: 0.01360061, BC Loss: 0.06149407\n",
      "Iteration: 3180, Total Loss: 0.07346883, Physics Loss: 0.01341566, BC Loss: 0.06005317\n",
      "Iteration: 3190, Total Loss: 0.07246836, Physics Loss: 0.01324502, BC Loss: 0.05922334\n",
      "Iteration: 3200, Total Loss: 0.07194106, Physics Loss: 0.01323243, BC Loss: 0.05870863\n",
      "Iteration: 3210, Total Loss: 0.07062178, Physics Loss: 0.01308855, BC Loss: 0.05753323\n",
      "Iteration: 3220, Total Loss: 0.06922670, Physics Loss: 0.01253895, BC Loss: 0.05668776\n",
      "Iteration: 3230, Total Loss: 0.06820900, Physics Loss: 0.01287352, BC Loss: 0.05533548\n",
      "Iteration: 3240, Total Loss: 0.06724530, Physics Loss: 0.01282426, BC Loss: 0.05442104\n",
      "Iteration: 3250, Total Loss: 0.06567761, Physics Loss: 0.01322518, BC Loss: 0.05245244\n",
      "Iteration: 3260, Total Loss: 0.06451757, Physics Loss: 0.01366894, BC Loss: 0.05084863\n",
      "Iteration: 3270, Total Loss: 0.06313094, Physics Loss: 0.01296163, BC Loss: 0.05016932\n",
      "Iteration: 3280, Total Loss: 0.06212719, Physics Loss: 0.01364441, BC Loss: 0.04848278\n",
      "Iteration: 3290, Total Loss: 0.06095953, Physics Loss: 0.01286865, BC Loss: 0.04809088\n",
      "Iteration: 3300, Total Loss: 0.05991100, Physics Loss: 0.01249357, BC Loss: 0.04741742\n",
      "Iteration: 3310, Total Loss: 0.05882393, Physics Loss: 0.01208120, BC Loss: 0.04674273\n",
      "Iteration: 3320, Total Loss: 0.05794761, Physics Loss: 0.01205895, BC Loss: 0.04588866\n",
      "Iteration: 3330, Total Loss: 0.05658311, Physics Loss: 0.01170647, BC Loss: 0.04487663\n",
      "Iteration: 3340, Total Loss: 0.05616246, Physics Loss: 0.01142189, BC Loss: 0.04474057\n",
      "Iteration: 3350, Total Loss: 0.05567317, Physics Loss: 0.01102794, BC Loss: 0.04464523\n",
      "Iteration: 3360, Total Loss: 0.05521115, Physics Loss: 0.01096656, BC Loss: 0.04424459\n",
      "Iteration: 3370, Total Loss: 0.05468726, Physics Loss: 0.01079568, BC Loss: 0.04389158\n",
      "Iteration: 3380, Total Loss: 0.05399926, Physics Loss: 0.01055101, BC Loss: 0.04344825\n",
      "Iteration: 3390, Total Loss: 0.05324657, Physics Loss: 0.01025786, BC Loss: 0.04298871\n",
      "Iteration: 3400, Total Loss: 0.05264420, Physics Loss: 0.01008774, BC Loss: 0.04255646\n",
      "Iteration: 3410, Total Loss: 0.05197480, Physics Loss: 0.00950445, BC Loss: 0.04247035\n",
      "Iteration: 3420, Total Loss: 0.05140779, Physics Loss: 0.00944334, BC Loss: 0.04196446\n",
      "Iteration: 3430, Total Loss: 0.05053369, Physics Loss: 0.00927977, BC Loss: 0.04125392\n",
      "Iteration: 3440, Total Loss: 0.05013991, Physics Loss: 0.00906195, BC Loss: 0.04107796\n",
      "Iteration: 3450, Total Loss: 0.04968277, Physics Loss: 0.00889735, BC Loss: 0.04078542\n",
      "Iteration: 3460, Total Loss: 0.04927218, Physics Loss: 0.00859686, BC Loss: 0.04067532\n",
      "Iteration: 3470, Total Loss: 0.04862936, Physics Loss: 0.00856615, BC Loss: 0.04006321\n",
      "Iteration: 3480, Total Loss: 0.04814912, Physics Loss: 0.00857888, BC Loss: 0.03957025\n",
      "Iteration: 3490, Total Loss: 0.04782308, Physics Loss: 0.00875218, BC Loss: 0.03907090\n",
      "Iteration: 3500, Total Loss: 0.04736385, Physics Loss: 0.00895230, BC Loss: 0.03841155\n",
      "Iteration: 3510, Total Loss: 0.04704469, Physics Loss: 0.00899526, BC Loss: 0.03804942\n",
      "Iteration: 3520, Total Loss: 0.04673576, Physics Loss: 0.00930723, BC Loss: 0.03742854\n",
      "Iteration: 3530, Total Loss: 0.04615895, Physics Loss: 0.00920912, BC Loss: 0.03694983\n",
      "Iteration: 3540, Total Loss: 0.04550328, Physics Loss: 0.00874965, BC Loss: 0.03675364\n",
      "Iteration: 3550, Total Loss: 0.04485127, Physics Loss: 0.00884388, BC Loss: 0.03600739\n",
      "Iteration: 3560, Total Loss: 0.04449096, Physics Loss: 0.00875010, BC Loss: 0.03574086\n",
      "Iteration: 3570, Total Loss: 0.04394778, Physics Loss: 0.00876879, BC Loss: 0.03517900\n",
      "Iteration: 3580, Total Loss: 0.04344362, Physics Loss: 0.00847422, BC Loss: 0.03496940\n",
      "Iteration: 3590, Total Loss: 0.04266148, Physics Loss: 0.00847009, BC Loss: 0.03419140\n",
      "Iteration: 3600, Total Loss: 0.04232717, Physics Loss: 0.00855974, BC Loss: 0.03376742\n",
      "Iteration: 3610, Total Loss: 0.04195429, Physics Loss: 0.00846517, BC Loss: 0.03348911\n",
      "Iteration: 3620, Total Loss: 0.04155950, Physics Loss: 0.00860665, BC Loss: 0.03295285\n",
      "Iteration: 3630, Total Loss: 0.04093361, Physics Loss: 0.00840181, BC Loss: 0.03253179\n",
      "Iteration: 3640, Total Loss: 0.04043828, Physics Loss: 0.00823881, BC Loss: 0.03219947\n",
      "Iteration: 3650, Total Loss: 0.04013623, Physics Loss: 0.00816524, BC Loss: 0.03197099\n",
      "Iteration: 3660, Total Loss: 0.03984652, Physics Loss: 0.00773558, BC Loss: 0.03211094\n",
      "Iteration: 3670, Total Loss: 0.03948837, Physics Loss: 0.00767086, BC Loss: 0.03181751\n",
      "Iteration: 3680, Total Loss: 0.03892964, Physics Loss: 0.00750904, BC Loss: 0.03142060\n",
      "Iteration: 3690, Total Loss: 0.03858755, Physics Loss: 0.00731288, BC Loss: 0.03127467\n",
      "Iteration: 3700, Total Loss: 0.03822764, Physics Loss: 0.00725627, BC Loss: 0.03097137\n",
      "Iteration: 3710, Total Loss: 0.03794273, Physics Loss: 0.00721666, BC Loss: 0.03072607\n",
      "Iteration: 3720, Total Loss: 0.03750160, Physics Loss: 0.00718888, BC Loss: 0.03031272\n",
      "Iteration: 3730, Total Loss: 0.03710002, Physics Loss: 0.00697355, BC Loss: 0.03012647\n",
      "Iteration: 3740, Total Loss: 0.03681178, Physics Loss: 0.00681846, BC Loss: 0.02999333\n",
      "Iteration: 3750, Total Loss: 0.03642659, Physics Loss: 0.00678094, BC Loss: 0.02964565\n",
      "Iteration: 3760, Total Loss: 0.03602359, Physics Loss: 0.00654033, BC Loss: 0.02948326\n",
      "Iteration: 3770, Total Loss: 0.03575115, Physics Loss: 0.00631395, BC Loss: 0.02943720\n",
      "Iteration: 3780, Total Loss: 0.03544018, Physics Loss: 0.00618038, BC Loss: 0.02925980\n",
      "Iteration: 3790, Total Loss: 0.03498010, Physics Loss: 0.00609160, BC Loss: 0.02888849\n",
      "Iteration: 3800, Total Loss: 0.03480640, Physics Loss: 0.00606013, BC Loss: 0.02874627\n",
      "Iteration: 3810, Total Loss: 0.03455135, Physics Loss: 0.00598129, BC Loss: 0.02857006\n",
      "Iteration: 3820, Total Loss: 0.03435400, Physics Loss: 0.00571592, BC Loss: 0.02863808\n",
      "Iteration: 3830, Total Loss: 0.03415021, Physics Loss: 0.00556377, BC Loss: 0.02858645\n",
      "Iteration: 3840, Total Loss: 0.03386171, Physics Loss: 0.00553265, BC Loss: 0.02832906\n",
      "Iteration: 3850, Total Loss: 0.03371176, Physics Loss: 0.00562469, BC Loss: 0.02808707\n",
      "Iteration: 3860, Total Loss: 0.03352309, Physics Loss: 0.00564748, BC Loss: 0.02787561\n",
      "Iteration: 3870, Total Loss: 0.03326587, Physics Loss: 0.00572257, BC Loss: 0.02754330\n",
      "Iteration: 3880, Total Loss: 0.03301712, Physics Loss: 0.00574838, BC Loss: 0.02726874\n",
      "Iteration: 3890, Total Loss: 0.03286060, Physics Loss: 0.00563651, BC Loss: 0.02722409\n",
      "Iteration: 3900, Total Loss: 0.03268066, Physics Loss: 0.00557906, BC Loss: 0.02710160\n",
      "Iteration: 3910, Total Loss: 0.03247304, Physics Loss: 0.00559246, BC Loss: 0.02688058\n",
      "Iteration: 3920, Total Loss: 0.03226709, Physics Loss: 0.00546616, BC Loss: 0.02680093\n",
      "Iteration: 3930, Total Loss: 0.03198833, Physics Loss: 0.00529113, BC Loss: 0.02669720\n",
      "Iteration: 3940, Total Loss: 0.03162310, Physics Loss: 0.00517442, BC Loss: 0.02644868\n",
      "Iteration: 3950, Total Loss: 0.03143547, Physics Loss: 0.00518510, BC Loss: 0.02625037\n",
      "Iteration: 3960, Total Loss: 0.03134372, Physics Loss: 0.00514363, BC Loss: 0.02620008\n",
      "Iteration: 3970, Total Loss: 0.03120024, Physics Loss: 0.00504537, BC Loss: 0.02615488\n",
      "Iteration: 3980, Total Loss: 0.03101436, Physics Loss: 0.00502654, BC Loss: 0.02598782\n",
      "Iteration: 3990, Total Loss: 0.03091158, Physics Loss: 0.00507412, BC Loss: 0.02583747\n",
      "Iteration: 4000, Total Loss: 0.03070514, Physics Loss: 0.00502022, BC Loss: 0.02568491\n",
      "Iteration: 4010, Total Loss: 0.62201101, Physics Loss: 0.32618213, BC Loss: 0.29582888\n",
      "Iteration: 4020, Total Loss: 0.15793526, Physics Loss: 0.06838421, BC Loss: 0.08955106\n",
      "Iteration: 4030, Total Loss: 0.11703460, Physics Loss: 0.04147787, BC Loss: 0.07555673\n",
      "Iteration: 4040, Total Loss: 0.10393157, Physics Loss: 0.03182894, BC Loss: 0.07210263\n",
      "Iteration: 4050, Total Loss: 0.09413112, Physics Loss: 0.02339639, BC Loss: 0.07073473\n",
      "Iteration: 4060, Total Loss: 0.09117444, Physics Loss: 0.02101973, BC Loss: 0.07015470\n",
      "Iteration: 4070, Total Loss: 0.08645976, Physics Loss: 0.01651248, BC Loss: 0.06994728\n",
      "Iteration: 4080, Total Loss: 0.08332665, Physics Loss: 0.01477857, BC Loss: 0.06854807\n",
      "Iteration: 4090, Total Loss: 0.08091045, Physics Loss: 0.01435343, BC Loss: 0.06655702\n",
      "Iteration: 4100, Total Loss: 0.07849590, Physics Loss: 0.01450479, BC Loss: 0.06399111\n",
      "Iteration: 4110, Total Loss: 0.07596915, Physics Loss: 0.01268598, BC Loss: 0.06328318\n",
      "Iteration: 4120, Total Loss: 0.07414439, Physics Loss: 0.01245959, BC Loss: 0.06168480\n",
      "Iteration: 4130, Total Loss: 0.07266859, Physics Loss: 0.01138722, BC Loss: 0.06128137\n",
      "Iteration: 4140, Total Loss: 0.07105734, Physics Loss: 0.01127010, BC Loss: 0.05978724\n",
      "Iteration: 4150, Total Loss: 0.06978172, Physics Loss: 0.01040617, BC Loss: 0.05937556\n",
      "Iteration: 4160, Total Loss: 0.06789673, Physics Loss: 0.00837180, BC Loss: 0.05952493\n",
      "Iteration: 4170, Total Loss: 0.06641117, Physics Loss: 0.00817415, BC Loss: 0.05823702\n",
      "Iteration: 4180, Total Loss: 0.06499554, Physics Loss: 0.00842759, BC Loss: 0.05656795\n",
      "Iteration: 4190, Total Loss: 0.06338627, Physics Loss: 0.00861734, BC Loss: 0.05476893\n",
      "Iteration: 4200, Total Loss: 0.06223363, Physics Loss: 0.00910065, BC Loss: 0.05313298\n",
      "Iteration: 4210, Total Loss: 0.06083588, Physics Loss: 0.00876795, BC Loss: 0.05206794\n",
      "Iteration: 4220, Total Loss: 0.06011797, Physics Loss: 0.00866745, BC Loss: 0.05145052\n",
      "Iteration: 4230, Total Loss: 0.05910470, Physics Loss: 0.00766608, BC Loss: 0.05143862\n",
      "Iteration: 4240, Total Loss: 0.05819837, Physics Loss: 0.00761307, BC Loss: 0.05058530\n",
      "Iteration: 4250, Total Loss: 0.05765480, Physics Loss: 0.00761508, BC Loss: 0.05003972\n",
      "Iteration: 4260, Total Loss: 0.05680209, Physics Loss: 0.00771594, BC Loss: 0.04908615\n",
      "Iteration: 4270, Total Loss: 0.05581585, Physics Loss: 0.00754693, BC Loss: 0.04826891\n",
      "Iteration: 4280, Total Loss: 0.05506993, Physics Loss: 0.00725474, BC Loss: 0.04781519\n",
      "Iteration: 4290, Total Loss: 0.05433372, Physics Loss: 0.00713389, BC Loss: 0.04719982\n",
      "Iteration: 4300, Total Loss: 0.05369172, Physics Loss: 0.00671532, BC Loss: 0.04697639\n",
      "Iteration: 4310, Total Loss: 0.05312224, Physics Loss: 0.00648309, BC Loss: 0.04663914\n",
      "Iteration: 4320, Total Loss: 0.05248085, Physics Loss: 0.00613092, BC Loss: 0.04634994\n",
      "Iteration: 4330, Total Loss: 0.05168074, Physics Loss: 0.00575454, BC Loss: 0.04592620\n",
      "Iteration: 4340, Total Loss: 0.05112688, Physics Loss: 0.00566662, BC Loss: 0.04546025\n",
      "Iteration: 4350, Total Loss: 0.05077364, Physics Loss: 0.00561411, BC Loss: 0.04515953\n",
      "Iteration: 4360, Total Loss: 0.05043156, Physics Loss: 0.00577211, BC Loss: 0.04465945\n",
      "Iteration: 4370, Total Loss: 0.04998830, Physics Loss: 0.00579203, BC Loss: 0.04419626\n",
      "Iteration: 4380, Total Loss: 0.04936811, Physics Loss: 0.00547897, BC Loss: 0.04388914\n",
      "Iteration: 4390, Total Loss: 0.04896307, Physics Loss: 0.00545414, BC Loss: 0.04350894\n",
      "Iteration: 4400, Total Loss: 0.04842351, Physics Loss: 0.00541475, BC Loss: 0.04300876\n",
      "Iteration: 4410, Total Loss: 0.04768115, Physics Loss: 0.00548840, BC Loss: 0.04219275\n",
      "Iteration: 4420, Total Loss: 0.04712762, Physics Loss: 0.00540990, BC Loss: 0.04171772\n",
      "Iteration: 4430, Total Loss: 0.04658033, Physics Loss: 0.00511740, BC Loss: 0.04146293\n",
      "Iteration: 4440, Total Loss: 0.04599577, Physics Loss: 0.00483862, BC Loss: 0.04115716\n",
      "Iteration: 4450, Total Loss: 0.04561599, Physics Loss: 0.00462193, BC Loss: 0.04099406\n",
      "Iteration: 4460, Total Loss: 0.04516016, Physics Loss: 0.00440288, BC Loss: 0.04075728\n",
      "Iteration: 4470, Total Loss: 0.04484767, Physics Loss: 0.00424084, BC Loss: 0.04060683\n",
      "Iteration: 4480, Total Loss: 0.04466248, Physics Loss: 0.00420301, BC Loss: 0.04045948\n",
      "Iteration: 4490, Total Loss: 0.04435897, Physics Loss: 0.00416327, BC Loss: 0.04019570\n",
      "Iteration: 4500, Total Loss: 0.04387855, Physics Loss: 0.00398208, BC Loss: 0.03989647\n",
      "Iteration: 4510, Total Loss: 0.04356761, Physics Loss: 0.00389836, BC Loss: 0.03966925\n",
      "Iteration: 4520, Total Loss: 0.04317231, Physics Loss: 0.00393372, BC Loss: 0.03923859\n",
      "Iteration: 4530, Total Loss: 0.04263247, Physics Loss: 0.00383665, BC Loss: 0.03879583\n",
      "Iteration: 4540, Total Loss: 0.04234787, Physics Loss: 0.00373982, BC Loss: 0.03860805\n",
      "Iteration: 4550, Total Loss: 0.04219520, Physics Loss: 0.00372387, BC Loss: 0.03847133\n",
      "Iteration: 4560, Total Loss: 0.04198309, Physics Loss: 0.00372125, BC Loss: 0.03826184\n",
      "Iteration: 4570, Total Loss: 0.04150226, Physics Loss: 0.00373641, BC Loss: 0.03776585\n",
      "Iteration: 4580, Total Loss: 0.04120792, Physics Loss: 0.00368670, BC Loss: 0.03752122\n",
      "Iteration: 4590, Total Loss: 0.04102286, Physics Loss: 0.00362558, BC Loss: 0.03739728\n",
      "Iteration: 4600, Total Loss: 0.04078988, Physics Loss: 0.00355910, BC Loss: 0.03723079\n",
      "Iteration: 4610, Total Loss: 0.04047041, Physics Loss: 0.00350276, BC Loss: 0.03696766\n",
      "Iteration: 4620, Total Loss: 0.04030610, Physics Loss: 0.00345359, BC Loss: 0.03685251\n",
      "Iteration: 4630, Total Loss: 0.04015362, Physics Loss: 0.00341641, BC Loss: 0.03673721\n",
      "Iteration: 4640, Total Loss: 0.03992201, Physics Loss: 0.00333502, BC Loss: 0.03658699\n",
      "Iteration: 4650, Total Loss: 0.03979235, Physics Loss: 0.00322698, BC Loss: 0.03656537\n",
      "Iteration: 4660, Total Loss: 0.03971312, Physics Loss: 0.00322746, BC Loss: 0.03648566\n",
      "Iteration: 4670, Total Loss: 0.03955033, Physics Loss: 0.00331958, BC Loss: 0.03623074\n",
      "Iteration: 4680, Total Loss: 0.03944299, Physics Loss: 0.00334364, BC Loss: 0.03609935\n",
      "Iteration: 4690, Total Loss: 0.03927853, Physics Loss: 0.00348853, BC Loss: 0.03579000\n",
      "Iteration: 4700, Total Loss: 0.03902368, Physics Loss: 0.00340338, BC Loss: 0.03562030\n",
      "Iteration: 4710, Total Loss: 0.03890780, Physics Loss: 0.00327729, BC Loss: 0.03563051\n",
      "Iteration: 4720, Total Loss: 0.03873369, Physics Loss: 0.00326413, BC Loss: 0.03546956\n",
      "Iteration: 4730, Total Loss: 0.03863479, Physics Loss: 0.00316660, BC Loss: 0.03546819\n",
      "Iteration: 4740, Total Loss: 0.03848473, Physics Loss: 0.00301311, BC Loss: 0.03547162\n",
      "Iteration: 4750, Total Loss: 0.03829722, Physics Loss: 0.00289848, BC Loss: 0.03539875\n",
      "Iteration: 4760, Total Loss: 0.03820429, Physics Loss: 0.00286579, BC Loss: 0.03533850\n",
      "Iteration: 4770, Total Loss: 0.03811127, Physics Loss: 0.00279213, BC Loss: 0.03531913\n",
      "Iteration: 4780, Total Loss: 0.03794499, Physics Loss: 0.00264527, BC Loss: 0.03529972\n",
      "Iteration: 4790, Total Loss: 0.03780233, Physics Loss: 0.00256905, BC Loss: 0.03523329\n",
      "Iteration: 4800, Total Loss: 0.03760594, Physics Loss: 0.00249166, BC Loss: 0.03511428\n",
      "Iteration: 4810, Total Loss: 0.03739088, Physics Loss: 0.00241474, BC Loss: 0.03497614\n",
      "Iteration: 4820, Total Loss: 0.03718834, Physics Loss: 0.00230954, BC Loss: 0.03487880\n",
      "Iteration: 4830, Total Loss: 0.03705961, Physics Loss: 0.00214872, BC Loss: 0.03491089\n",
      "Iteration: 4840, Total Loss: 0.03697121, Physics Loss: 0.00205959, BC Loss: 0.03491162\n",
      "Iteration: 4850, Total Loss: 0.03686262, Physics Loss: 0.00198747, BC Loss: 0.03487515\n",
      "Iteration: 4860, Total Loss: 0.03677237, Physics Loss: 0.00193865, BC Loss: 0.03483372\n",
      "Iteration: 4870, Total Loss: 0.03669344, Physics Loss: 0.00190011, BC Loss: 0.03479333\n",
      "Iteration: 4880, Total Loss: 0.03663122, Physics Loss: 0.00191784, BC Loss: 0.03471338\n",
      "Iteration: 4890, Total Loss: 0.03655482, Physics Loss: 0.00191551, BC Loss: 0.03463931\n",
      "Iteration: 4900, Total Loss: 0.03646947, Physics Loss: 0.00188760, BC Loss: 0.03458187\n",
      "Iteration: 4910, Total Loss: 0.03642052, Physics Loss: 0.00187746, BC Loss: 0.03454307\n",
      "Iteration: 4920, Total Loss: 0.03633316, Physics Loss: 0.00181226, BC Loss: 0.03452090\n",
      "Iteration: 4930, Total Loss: 0.03616824, Physics Loss: 0.00181661, BC Loss: 0.03435163\n",
      "Iteration: 4940, Total Loss: 0.03610270, Physics Loss: 0.00184393, BC Loss: 0.03425877\n",
      "Iteration: 4950, Total Loss: 0.03606693, Physics Loss: 0.00184837, BC Loss: 0.03421856\n",
      "Iteration: 4960, Total Loss: 0.03602324, Physics Loss: 0.00185401, BC Loss: 0.03416922\n",
      "Iteration: 4970, Total Loss: 0.03586806, Physics Loss: 0.00181167, BC Loss: 0.03405638\n",
      "Iteration: 4980, Total Loss: 0.03568858, Physics Loss: 0.00180825, BC Loss: 0.03388032\n",
      "Iteration: 4990, Total Loss: 0.03552770, Physics Loss: 0.00174297, BC Loss: 0.03378473\n",
      "Iteration: 5000, Total Loss: 0.03544851, Physics Loss: 0.00172596, BC Loss: 0.03372255\n",
      "Iteration: 5010, Total Loss: 219.57095337, Physics Loss: 215.85052490, BC Loss: 3.72042537\n",
      "Iteration: 5020, Total Loss: 3.96043205, Physics Loss: 2.81897759, BC Loss: 1.14145434\n",
      "Iteration: 5030, Total Loss: 1.28105497, Physics Loss: 0.66342956, BC Loss: 0.61762547\n",
      "Iteration: 5040, Total Loss: 0.52463067, Physics Loss: 0.19687617, BC Loss: 0.32775450\n",
      "Iteration: 5050, Total Loss: 0.30594254, Physics Loss: 0.08303972, BC Loss: 0.22290280\n",
      "Iteration: 5060, Total Loss: 0.26242593, Physics Loss: 0.04721604, BC Loss: 0.21520990\n",
      "Iteration: 5070, Total Loss: 0.23650229, Physics Loss: 0.02737667, BC Loss: 0.20912562\n",
      "Iteration: 5080, Total Loss: 0.22356546, Physics Loss: 0.02468370, BC Loss: 0.19888176\n",
      "Iteration: 5090, Total Loss: 0.21078438, Physics Loss: 0.02859209, BC Loss: 0.18219230\n",
      "Iteration: 5100, Total Loss: 0.19859354, Physics Loss: 0.02803225, BC Loss: 0.17056128\n",
      "Iteration: 5110, Total Loss: 0.18513018, Physics Loss: 0.02439411, BC Loss: 0.16073607\n",
      "Iteration: 5120, Total Loss: 0.17241627, Physics Loss: 0.02861457, BC Loss: 0.14380169\n",
      "Iteration: 5130, Total Loss: 0.15944536, Physics Loss: 0.02734126, BC Loss: 0.13210410\n",
      "Iteration: 5140, Total Loss: 0.14920458, Physics Loss: 0.02998949, BC Loss: 0.11921510\n",
      "Iteration: 5150, Total Loss: 0.13999508, Physics Loss: 0.02453668, BC Loss: 0.11545840\n",
      "Iteration: 5160, Total Loss: 0.13552071, Physics Loss: 0.02285901, BC Loss: 0.11266170\n",
      "Iteration: 5170, Total Loss: 0.13038486, Physics Loss: 0.02092992, BC Loss: 0.10945494\n",
      "Iteration: 5180, Total Loss: 0.12303724, Physics Loss: 0.01901059, BC Loss: 0.10402665\n",
      "Iteration: 5190, Total Loss: 0.12113123, Physics Loss: 0.01973948, BC Loss: 0.10139176\n",
      "Iteration: 5200, Total Loss: 0.11913333, Physics Loss: 0.01901916, BC Loss: 0.10011417\n",
      "Iteration: 5210, Total Loss: 0.11474433, Physics Loss: 0.01934547, BC Loss: 0.09539886\n",
      "Iteration: 5220, Total Loss: 0.11246864, Physics Loss: 0.01852246, BC Loss: 0.09394618\n",
      "Iteration: 5230, Total Loss: 0.10937119, Physics Loss: 0.01779726, BC Loss: 0.09157392\n",
      "Iteration: 5240, Total Loss: 0.10655989, Physics Loss: 0.01780367, BC Loss: 0.08875622\n",
      "Iteration: 5250, Total Loss: 0.10478391, Physics Loss: 0.01743938, BC Loss: 0.08734453\n",
      "Iteration: 5260, Total Loss: 0.10296372, Physics Loss: 0.01727195, BC Loss: 0.08569176\n",
      "Iteration: 5270, Total Loss: 0.10034174, Physics Loss: 0.01664234, BC Loss: 0.08369941\n",
      "Iteration: 5280, Total Loss: 0.09798898, Physics Loss: 0.01466386, BC Loss: 0.08332513\n",
      "Iteration: 5290, Total Loss: 0.09586054, Physics Loss: 0.01420901, BC Loss: 0.08165153\n",
      "Iteration: 5300, Total Loss: 0.09346650, Physics Loss: 0.01462138, BC Loss: 0.07884512\n",
      "Iteration: 5310, Total Loss: 0.09154530, Physics Loss: 0.01413519, BC Loss: 0.07741011\n",
      "Iteration: 5320, Total Loss: 0.09021375, Physics Loss: 0.01365976, BC Loss: 0.07655399\n",
      "Iteration: 5330, Total Loss: 0.08827330, Physics Loss: 0.01311001, BC Loss: 0.07516330\n",
      "Iteration: 5340, Total Loss: 0.08736637, Physics Loss: 0.01265648, BC Loss: 0.07470989\n",
      "Iteration: 5350, Total Loss: 0.08649938, Physics Loss: 0.01251892, BC Loss: 0.07398046\n",
      "Iteration: 5360, Total Loss: 0.08543948, Physics Loss: 0.01200433, BC Loss: 0.07343516\n",
      "Iteration: 5370, Total Loss: 0.08412248, Physics Loss: 0.01144261, BC Loss: 0.07267987\n",
      "Iteration: 5380, Total Loss: 0.08286223, Physics Loss: 0.01201099, BC Loss: 0.07085124\n",
      "Iteration: 5390, Total Loss: 0.08091821, Physics Loss: 0.01231863, BC Loss: 0.06859958\n",
      "Iteration: 5400, Total Loss: 0.08005655, Physics Loss: 0.01267784, BC Loss: 0.06737871\n",
      "Iteration: 5410, Total Loss: 0.07924768, Physics Loss: 0.01261158, BC Loss: 0.06663610\n",
      "Iteration: 5420, Total Loss: 0.07862525, Physics Loss: 0.01250929, BC Loss: 0.06611597\n",
      "Iteration: 5430, Total Loss: 0.07787213, Physics Loss: 0.01193164, BC Loss: 0.06594048\n",
      "Iteration: 5440, Total Loss: 0.07675188, Physics Loss: 0.01258921, BC Loss: 0.06416267\n",
      "Iteration: 5450, Total Loss: 0.07553480, Physics Loss: 0.01210033, BC Loss: 0.06343447\n",
      "Iteration: 5460, Total Loss: 0.07469209, Physics Loss: 0.01199940, BC Loss: 0.06269269\n",
      "Iteration: 5470, Total Loss: 0.07372256, Physics Loss: 0.01189729, BC Loss: 0.06182527\n",
      "Iteration: 5480, Total Loss: 0.07252864, Physics Loss: 0.01194329, BC Loss: 0.06058535\n",
      "Iteration: 5490, Total Loss: 0.07158616, Physics Loss: 0.01185131, BC Loss: 0.05973486\n",
      "Iteration: 5500, Total Loss: 0.07062176, Physics Loss: 0.01147800, BC Loss: 0.05914376\n",
      "Iteration: 5510, Total Loss: 0.06976803, Physics Loss: 0.01073309, BC Loss: 0.05903494\n",
      "Iteration: 5520, Total Loss: 0.06897764, Physics Loss: 0.01101884, BC Loss: 0.05795880\n",
      "Iteration: 5530, Total Loss: 0.06782222, Physics Loss: 0.01058967, BC Loss: 0.05723254\n",
      "Iteration: 5540, Total Loss: 0.06705815, Physics Loss: 0.01129111, BC Loss: 0.05576704\n",
      "Iteration: 5550, Total Loss: 0.06628516, Physics Loss: 0.01133217, BC Loss: 0.05495300\n",
      "Iteration: 5560, Total Loss: 0.06545911, Physics Loss: 0.01122310, BC Loss: 0.05423601\n",
      "Iteration: 5570, Total Loss: 0.06453758, Physics Loss: 0.01073751, BC Loss: 0.05380007\n",
      "Iteration: 5580, Total Loss: 0.06401423, Physics Loss: 0.01067694, BC Loss: 0.05333729\n",
      "Iteration: 5590, Total Loss: 0.06334410, Physics Loss: 0.01021716, BC Loss: 0.05312694\n",
      "Iteration: 5600, Total Loss: 0.06286768, Physics Loss: 0.01013562, BC Loss: 0.05273206\n",
      "Iteration: 5610, Total Loss: 0.06195789, Physics Loss: 0.00974197, BC Loss: 0.05221592\n",
      "Iteration: 5620, Total Loss: 0.06134465, Physics Loss: 0.01020685, BC Loss: 0.05113780\n",
      "Iteration: 5630, Total Loss: 0.06045267, Physics Loss: 0.01012161, BC Loss: 0.05033106\n",
      "Iteration: 5640, Total Loss: 0.05979855, Physics Loss: 0.00980725, BC Loss: 0.04999129\n",
      "Iteration: 5650, Total Loss: 0.05953198, Physics Loss: 0.00961126, BC Loss: 0.04992073\n",
      "Iteration: 5660, Total Loss: 0.05886716, Physics Loss: 0.00958044, BC Loss: 0.04928672\n",
      "Iteration: 5670, Total Loss: 0.05809926, Physics Loss: 0.00957294, BC Loss: 0.04852632\n",
      "Iteration: 5680, Total Loss: 0.05759743, Physics Loss: 0.00920156, BC Loss: 0.04839587\n",
      "Iteration: 5690, Total Loss: 0.05722246, Physics Loss: 0.00952500, BC Loss: 0.04769746\n",
      "Iteration: 5700, Total Loss: 0.05677687, Physics Loss: 0.00970793, BC Loss: 0.04706894\n",
      "Iteration: 5710, Total Loss: 0.05642559, Physics Loss: 0.01033699, BC Loss: 0.04608861\n",
      "Iteration: 5720, Total Loss: 0.05607423, Physics Loss: 0.01016140, BC Loss: 0.04591282\n",
      "Iteration: 5730, Total Loss: 0.05583464, Physics Loss: 0.01039432, BC Loss: 0.04544032\n",
      "Iteration: 5740, Total Loss: 0.05551698, Physics Loss: 0.01013268, BC Loss: 0.04538430\n",
      "Iteration: 5750, Total Loss: 0.05503611, Physics Loss: 0.01011235, BC Loss: 0.04492376\n",
      "Iteration: 5760, Total Loss: 0.05469529, Physics Loss: 0.01011329, BC Loss: 0.04458199\n",
      "Iteration: 5770, Total Loss: 0.05439087, Physics Loss: 0.00968275, BC Loss: 0.04470812\n",
      "Iteration: 5780, Total Loss: 0.05419311, Physics Loss: 0.00975580, BC Loss: 0.04443731\n",
      "Iteration: 5790, Total Loss: 0.05407558, Physics Loss: 0.00973888, BC Loss: 0.04433670\n",
      "Iteration: 5800, Total Loss: 0.05376051, Physics Loss: 0.00975129, BC Loss: 0.04400922\n",
      "Iteration: 5810, Total Loss: 0.05353555, Physics Loss: 0.00958107, BC Loss: 0.04395448\n",
      "Iteration: 5820, Total Loss: 0.05328458, Physics Loss: 0.00961328, BC Loss: 0.04367130\n",
      "Iteration: 5830, Total Loss: 0.05305172, Physics Loss: 0.00972935, BC Loss: 0.04332238\n",
      "Iteration: 5840, Total Loss: 0.05270755, Physics Loss: 0.00972358, BC Loss: 0.04298397\n",
      "Iteration: 5850, Total Loss: 0.05211129, Physics Loss: 0.00998318, BC Loss: 0.04212811\n",
      "Iteration: 5860, Total Loss: 0.05160462, Physics Loss: 0.01021177, BC Loss: 0.04139285\n",
      "Iteration: 5870, Total Loss: 0.05121284, Physics Loss: 0.01017232, BC Loss: 0.04104052\n",
      "Iteration: 5880, Total Loss: 0.05093382, Physics Loss: 0.01004317, BC Loss: 0.04089064\n",
      "Iteration: 5890, Total Loss: 0.05077491, Physics Loss: 0.01023364, BC Loss: 0.04054128\n",
      "Iteration: 5900, Total Loss: 0.05048102, Physics Loss: 0.01017545, BC Loss: 0.04030557\n",
      "Iteration: 5910, Total Loss: 0.05015240, Physics Loss: 0.01001473, BC Loss: 0.04013767\n",
      "Iteration: 5920, Total Loss: 0.04983560, Physics Loss: 0.00994752, BC Loss: 0.03988808\n",
      "Iteration: 5930, Total Loss: 0.04952683, Physics Loss: 0.00945783, BC Loss: 0.04006899\n",
      "Iteration: 5940, Total Loss: 0.04924512, Physics Loss: 0.00947813, BC Loss: 0.03976699\n",
      "Iteration: 5950, Total Loss: 0.04888837, Physics Loss: 0.00953795, BC Loss: 0.03935042\n",
      "Iteration: 5960, Total Loss: 0.04867351, Physics Loss: 0.00962927, BC Loss: 0.03904424\n",
      "Iteration: 5970, Total Loss: 0.04852783, Physics Loss: 0.00983379, BC Loss: 0.03869404\n",
      "Iteration: 5980, Total Loss: 0.04839232, Physics Loss: 0.00995068, BC Loss: 0.03844164\n",
      "Iteration: 5990, Total Loss: 0.04814067, Physics Loss: 0.00997615, BC Loss: 0.03816452\n",
      "Iteration: 6000, Total Loss: 0.04798905, Physics Loss: 0.00994575, BC Loss: 0.03804330\n",
      "Iteration: 6010, Total Loss: 1.65941763, Physics Loss: 1.50174308, BC Loss: 0.15767458\n",
      "Iteration: 6020, Total Loss: 0.35708553, Physics Loss: 0.19514962, BC Loss: 0.16193590\n",
      "Iteration: 6030, Total Loss: 0.17753179, Physics Loss: 0.05384341, BC Loss: 0.12368838\n",
      "Iteration: 6040, Total Loss: 0.15580928, Physics Loss: 0.03761367, BC Loss: 0.11819562\n",
      "Iteration: 6050, Total Loss: 0.14065909, Physics Loss: 0.02294355, BC Loss: 0.11771554\n",
      "Iteration: 6060, Total Loss: 0.13370949, Physics Loss: 0.01975412, BC Loss: 0.11395536\n",
      "Iteration: 6070, Total Loss: 0.12881029, Physics Loss: 0.01700389, BC Loss: 0.11180641\n",
      "Iteration: 6080, Total Loss: 0.12609044, Physics Loss: 0.01733642, BC Loss: 0.10875402\n",
      "Iteration: 6090, Total Loss: 0.12243596, Physics Loss: 0.01512698, BC Loss: 0.10730897\n",
      "Iteration: 6100, Total Loss: 0.11969036, Physics Loss: 0.01428412, BC Loss: 0.10540624\n",
      "Iteration: 6110, Total Loss: 0.11790939, Physics Loss: 0.01271102, BC Loss: 0.10519837\n",
      "Iteration: 6120, Total Loss: 0.11630559, Physics Loss: 0.01361763, BC Loss: 0.10268795\n",
      "Iteration: 6130, Total Loss: 0.11538015, Physics Loss: 0.01374269, BC Loss: 0.10163745\n",
      "Iteration: 6140, Total Loss: 0.11462903, Physics Loss: 0.01341462, BC Loss: 0.10121441\n",
      "Iteration: 6150, Total Loss: 0.11376449, Physics Loss: 0.01257107, BC Loss: 0.10119343\n",
      "Iteration: 6160, Total Loss: 0.11284879, Physics Loss: 0.01198937, BC Loss: 0.10085941\n",
      "Iteration: 6170, Total Loss: 0.11205304, Physics Loss: 0.01103225, BC Loss: 0.10102080\n",
      "Iteration: 6180, Total Loss: 0.11137464, Physics Loss: 0.00993550, BC Loss: 0.10143914\n",
      "Iteration: 6190, Total Loss: 0.11058915, Physics Loss: 0.00977089, BC Loss: 0.10081826\n",
      "Iteration: 6200, Total Loss: 0.10982960, Physics Loss: 0.00957892, BC Loss: 0.10025069\n",
      "Iteration: 6210, Total Loss: 0.10949956, Physics Loss: 0.00906890, BC Loss: 0.10043066\n",
      "Iteration: 6220, Total Loss: 0.10896096, Physics Loss: 0.00961967, BC Loss: 0.09934129\n",
      "Iteration: 6230, Total Loss: 0.10836802, Physics Loss: 0.00948934, BC Loss: 0.09887868\n",
      "Iteration: 6240, Total Loss: 0.10799772, Physics Loss: 0.00989025, BC Loss: 0.09810747\n",
      "Iteration: 6250, Total Loss: 0.10750005, Physics Loss: 0.00999356, BC Loss: 0.09750648\n",
      "Iteration: 6260, Total Loss: 0.10636735, Physics Loss: 0.00983106, BC Loss: 0.09653629\n",
      "Iteration: 6270, Total Loss: 0.10544834, Physics Loss: 0.01082498, BC Loss: 0.09462336\n",
      "Iteration: 6280, Total Loss: 0.10499328, Physics Loss: 0.01061482, BC Loss: 0.09437846\n",
      "Iteration: 6290, Total Loss: 0.10397469, Physics Loss: 0.00965133, BC Loss: 0.09432335\n",
      "Iteration: 6300, Total Loss: 0.10357299, Physics Loss: 0.00954154, BC Loss: 0.09403145\n",
      "Iteration: 6310, Total Loss: 0.10310932, Physics Loss: 0.00934877, BC Loss: 0.09376055\n",
      "Iteration: 6320, Total Loss: 0.10280138, Physics Loss: 0.00902698, BC Loss: 0.09377440\n",
      "Iteration: 6330, Total Loss: 0.10223090, Physics Loss: 0.00876569, BC Loss: 0.09346521\n",
      "Iteration: 6340, Total Loss: 0.10188103, Physics Loss: 0.00927790, BC Loss: 0.09260312\n",
      "Iteration: 6350, Total Loss: 0.10154373, Physics Loss: 0.00920795, BC Loss: 0.09233578\n",
      "Iteration: 6360, Total Loss: 0.10103340, Physics Loss: 0.00910688, BC Loss: 0.09192652\n",
      "Iteration: 6370, Total Loss: 0.10054241, Physics Loss: 0.00916628, BC Loss: 0.09137613\n",
      "Iteration: 6380, Total Loss: 0.10012780, Physics Loss: 0.00902994, BC Loss: 0.09109786\n",
      "Iteration: 6390, Total Loss: 0.09977503, Physics Loss: 0.00930872, BC Loss: 0.09046631\n",
      "Iteration: 6400, Total Loss: 0.09949532, Physics Loss: 0.00893690, BC Loss: 0.09055842\n",
      "Iteration: 6410, Total Loss: 0.09902620, Physics Loss: 0.00918182, BC Loss: 0.08984438\n",
      "Iteration: 6420, Total Loss: 0.09877415, Physics Loss: 0.00919485, BC Loss: 0.08957930\n",
      "Iteration: 6430, Total Loss: 0.09829160, Physics Loss: 0.00881944, BC Loss: 0.08947216\n",
      "Iteration: 6440, Total Loss: 0.09746733, Physics Loss: 0.00879713, BC Loss: 0.08867020\n",
      "Iteration: 6450, Total Loss: 0.09710635, Physics Loss: 0.00894131, BC Loss: 0.08816504\n",
      "Iteration: 6460, Total Loss: 0.09665922, Physics Loss: 0.00870479, BC Loss: 0.08795443\n",
      "Iteration: 6470, Total Loss: 0.09621148, Physics Loss: 0.00872826, BC Loss: 0.08748322\n",
      "Iteration: 6480, Total Loss: 0.09589539, Physics Loss: 0.00888967, BC Loss: 0.08700573\n",
      "Iteration: 6490, Total Loss: 0.09547994, Physics Loss: 0.00925294, BC Loss: 0.08622699\n",
      "Iteration: 6500, Total Loss: 0.09511094, Physics Loss: 0.00906976, BC Loss: 0.08604117\n",
      "Iteration: 6510, Total Loss: 0.09480698, Physics Loss: 0.00926271, BC Loss: 0.08554427\n",
      "Iteration: 6520, Total Loss: 0.09426483, Physics Loss: 0.00926144, BC Loss: 0.08500339\n",
      "Iteration: 6530, Total Loss: 0.09393382, Physics Loss: 0.00909810, BC Loss: 0.08483572\n",
      "Iteration: 6540, Total Loss: 0.09356328, Physics Loss: 0.00901188, BC Loss: 0.08455140\n",
      "Iteration: 6550, Total Loss: 0.09315909, Physics Loss: 0.00879267, BC Loss: 0.08436642\n",
      "Iteration: 6560, Total Loss: 0.09274456, Physics Loss: 0.00908497, BC Loss: 0.08365959\n",
      "Iteration: 6570, Total Loss: 0.09248365, Physics Loss: 0.00854163, BC Loss: 0.08394202\n",
      "Iteration: 6580, Total Loss: 0.09203771, Physics Loss: 0.00856462, BC Loss: 0.08347309\n",
      "Iteration: 6590, Total Loss: 0.09149309, Physics Loss: 0.00852414, BC Loss: 0.08296895\n",
      "Iteration: 6600, Total Loss: 0.09120034, Physics Loss: 0.00839242, BC Loss: 0.08280793\n",
      "Iteration: 6610, Total Loss: 0.09098154, Physics Loss: 0.00836294, BC Loss: 0.08261861\n",
      "Iteration: 6620, Total Loss: 0.09061491, Physics Loss: 0.00837437, BC Loss: 0.08224054\n",
      "Iteration: 6630, Total Loss: 0.09031123, Physics Loss: 0.00812928, BC Loss: 0.08218195\n",
      "Iteration: 6640, Total Loss: 0.08996161, Physics Loss: 0.00819848, BC Loss: 0.08176313\n",
      "Iteration: 6650, Total Loss: 0.08966764, Physics Loss: 0.00807934, BC Loss: 0.08158830\n",
      "Iteration: 6660, Total Loss: 0.08938488, Physics Loss: 0.00799620, BC Loss: 0.08138867\n",
      "Iteration: 6670, Total Loss: 0.08915471, Physics Loss: 0.00786104, BC Loss: 0.08129367\n",
      "Iteration: 6680, Total Loss: 0.08893716, Physics Loss: 0.00788586, BC Loss: 0.08105130\n",
      "Iteration: 6690, Total Loss: 0.08858921, Physics Loss: 0.00816941, BC Loss: 0.08041981\n",
      "Iteration: 6700, Total Loss: 0.08832747, Physics Loss: 0.00815798, BC Loss: 0.08016949\n",
      "Iteration: 6710, Total Loss: 0.08798986, Physics Loss: 0.00826156, BC Loss: 0.07972830\n",
      "Iteration: 6720, Total Loss: 0.08760984, Physics Loss: 0.00817262, BC Loss: 0.07943722\n",
      "Iteration: 6730, Total Loss: 0.08737811, Physics Loss: 0.00790767, BC Loss: 0.07947044\n",
      "Iteration: 6740, Total Loss: 0.08701328, Physics Loss: 0.00824986, BC Loss: 0.07876343\n",
      "Iteration: 6750, Total Loss: 0.08664539, Physics Loss: 0.00834357, BC Loss: 0.07830182\n",
      "Iteration: 6760, Total Loss: 0.08632979, Physics Loss: 0.00846256, BC Loss: 0.07786723\n",
      "Iteration: 6770, Total Loss: 0.08607911, Physics Loss: 0.00850748, BC Loss: 0.07757163\n",
      "Iteration: 6780, Total Loss: 0.08577943, Physics Loss: 0.00845459, BC Loss: 0.07732484\n",
      "Iteration: 6790, Total Loss: 0.08553530, Physics Loss: 0.00839857, BC Loss: 0.07713673\n",
      "Iteration: 6800, Total Loss: 0.08531757, Physics Loss: 0.00857338, BC Loss: 0.07674419\n",
      "Iteration: 6810, Total Loss: 0.08510400, Physics Loss: 0.00877108, BC Loss: 0.07633291\n",
      "Iteration: 6820, Total Loss: 0.08477124, Physics Loss: 0.00884019, BC Loss: 0.07593105\n",
      "Iteration: 6830, Total Loss: 0.08462031, Physics Loss: 0.00857249, BC Loss: 0.07604782\n",
      "Iteration: 6840, Total Loss: 0.08452535, Physics Loss: 0.00848918, BC Loss: 0.07603617\n",
      "Iteration: 6850, Total Loss: 0.08434931, Physics Loss: 0.00844403, BC Loss: 0.07590528\n",
      "Iteration: 6860, Total Loss: 0.08407016, Physics Loss: 0.00853324, BC Loss: 0.07553693\n",
      "Iteration: 6870, Total Loss: 0.08389946, Physics Loss: 0.00838181, BC Loss: 0.07551765\n",
      "Iteration: 6880, Total Loss: 0.08378361, Physics Loss: 0.00857463, BC Loss: 0.07520898\n",
      "Iteration: 6890, Total Loss: 0.08358008, Physics Loss: 0.00855654, BC Loss: 0.07502354\n",
      "Iteration: 6900, Total Loss: 0.08347092, Physics Loss: 0.00831923, BC Loss: 0.07515168\n",
      "Iteration: 6910, Total Loss: 0.08335026, Physics Loss: 0.00838290, BC Loss: 0.07496735\n",
      "Iteration: 6920, Total Loss: 0.08320022, Physics Loss: 0.00835982, BC Loss: 0.07484040\n",
      "Iteration: 6930, Total Loss: 0.08294906, Physics Loss: 0.00812118, BC Loss: 0.07482788\n",
      "Iteration: 6940, Total Loss: 0.08277741, Physics Loss: 0.00780606, BC Loss: 0.07497135\n",
      "Iteration: 6950, Total Loss: 0.08256017, Physics Loss: 0.00762403, BC Loss: 0.07493614\n",
      "Iteration: 6960, Total Loss: 0.08233763, Physics Loss: 0.00763575, BC Loss: 0.07470188\n",
      "Iteration: 6970, Total Loss: 0.08218253, Physics Loss: 0.00773052, BC Loss: 0.07445201\n",
      "Iteration: 6980, Total Loss: 0.08205669, Physics Loss: 0.00774394, BC Loss: 0.07431275\n",
      "Iteration: 6990, Total Loss: 0.08190987, Physics Loss: 0.00742773, BC Loss: 0.07448214\n",
      "Iteration: 7000, Total Loss: 0.08176500, Physics Loss: 0.00740261, BC Loss: 0.07436238\n",
      "Iteration: 7010, Total Loss: 971903.75000000, Physics Loss: 971354.00000000, BC Loss: 549.72760010\n",
      "Iteration: 7020, Total Loss: 1.19578552, Physics Loss: 1.05450833, BC Loss: 0.14127725\n",
      "Iteration: 7030, Total Loss: 0.29215056, Physics Loss: 0.17756689, BC Loss: 0.11458366\n",
      "Iteration: 7040, Total Loss: 0.17380603, Physics Loss: 0.07075050, BC Loss: 0.10305552\n",
      "Iteration: 7050, Total Loss: 0.14144988, Physics Loss: 0.05649959, BC Loss: 0.08495029\n",
      "Iteration: 7060, Total Loss: 0.12482296, Physics Loss: 0.04149839, BC Loss: 0.08332457\n",
      "Iteration: 7070, Total Loss: 0.11022767, Physics Loss: 0.03611346, BC Loss: 0.07411421\n",
      "Iteration: 7080, Total Loss: 0.09967215, Physics Loss: 0.02439771, BC Loss: 0.07527444\n",
      "Iteration: 7090, Total Loss: 0.09263619, Physics Loss: 0.01979624, BC Loss: 0.07283995\n",
      "Iteration: 7100, Total Loss: 0.08953759, Physics Loss: 0.01885041, BC Loss: 0.07068718\n",
      "Iteration: 7110, Total Loss: 0.08611120, Physics Loss: 0.01555532, BC Loss: 0.07055588\n",
      "Iteration: 7120, Total Loss: 0.08342300, Physics Loss: 0.01573059, BC Loss: 0.06769241\n",
      "Iteration: 7130, Total Loss: 0.08150375, Physics Loss: 0.01371055, BC Loss: 0.06779320\n",
      "Iteration: 7140, Total Loss: 0.07933327, Physics Loss: 0.01251114, BC Loss: 0.06682213\n",
      "Iteration: 7150, Total Loss: 0.07835512, Physics Loss: 0.01211151, BC Loss: 0.06624360\n",
      "Iteration: 7160, Total Loss: 0.07741791, Physics Loss: 0.01226677, BC Loss: 0.06515114\n",
      "Iteration: 7170, Total Loss: 0.07622448, Physics Loss: 0.01134713, BC Loss: 0.06487735\n",
      "Iteration: 7180, Total Loss: 0.07498315, Physics Loss: 0.01086556, BC Loss: 0.06411759\n",
      "Iteration: 7190, Total Loss: 0.07416350, Physics Loss: 0.01068125, BC Loss: 0.06348224\n",
      "Iteration: 7200, Total Loss: 0.07353489, Physics Loss: 0.01071168, BC Loss: 0.06282321\n",
      "Iteration: 7210, Total Loss: 0.07264026, Physics Loss: 0.00987321, BC Loss: 0.06276706\n",
      "Iteration: 7220, Total Loss: 0.07215422, Physics Loss: 0.00941283, BC Loss: 0.06274138\n",
      "Iteration: 7230, Total Loss: 0.07173606, Physics Loss: 0.00911330, BC Loss: 0.06262276\n",
      "Iteration: 7240, Total Loss: 0.07062005, Physics Loss: 0.00936235, BC Loss: 0.06125771\n",
      "Iteration: 7250, Total Loss: 0.07018048, Physics Loss: 0.00967043, BC Loss: 0.06051005\n",
      "Iteration: 7260, Total Loss: 0.06976621, Physics Loss: 0.00923949, BC Loss: 0.06052672\n",
      "Iteration: 7270, Total Loss: 0.06951859, Physics Loss: 0.00898142, BC Loss: 0.06053717\n",
      "Iteration: 7280, Total Loss: 0.06916367, Physics Loss: 0.00887889, BC Loss: 0.06028477\n",
      "Iteration: 7290, Total Loss: 0.06880224, Physics Loss: 0.00881223, BC Loss: 0.05999000\n",
      "Iteration: 7300, Total Loss: 0.06839310, Physics Loss: 0.00884113, BC Loss: 0.05955197\n",
      "Iteration: 7310, Total Loss: 0.06802207, Physics Loss: 0.00858014, BC Loss: 0.05944193\n",
      "Iteration: 7320, Total Loss: 0.06765974, Physics Loss: 0.00868268, BC Loss: 0.05897705\n",
      "Iteration: 7330, Total Loss: 0.06733186, Physics Loss: 0.00846978, BC Loss: 0.05886208\n",
      "Iteration: 7340, Total Loss: 0.06699020, Physics Loss: 0.00852328, BC Loss: 0.05846691\n",
      "Iteration: 7350, Total Loss: 0.06683452, Physics Loss: 0.00850355, BC Loss: 0.05833098\n",
      "Iteration: 7360, Total Loss: 0.06648978, Physics Loss: 0.00808720, BC Loss: 0.05840258\n",
      "Iteration: 7370, Total Loss: 0.06617859, Physics Loss: 0.00799316, BC Loss: 0.05818544\n",
      "Iteration: 7380, Total Loss: 0.06601148, Physics Loss: 0.00780484, BC Loss: 0.05820665\n",
      "Iteration: 7390, Total Loss: 0.06582993, Physics Loss: 0.00770474, BC Loss: 0.05812519\n",
      "Iteration: 7400, Total Loss: 0.06549750, Physics Loss: 0.00766427, BC Loss: 0.05783323\n",
      "Iteration: 7410, Total Loss: 0.06531612, Physics Loss: 0.00762232, BC Loss: 0.05769380\n",
      "Iteration: 7420, Total Loss: 0.06522243, Physics Loss: 0.00760615, BC Loss: 0.05761629\n",
      "Iteration: 7430, Total Loss: 0.06504203, Physics Loss: 0.00779770, BC Loss: 0.05724433\n",
      "Iteration: 7440, Total Loss: 0.06489778, Physics Loss: 0.00789010, BC Loss: 0.05700768\n",
      "Iteration: 7450, Total Loss: 0.06476001, Physics Loss: 0.00785935, BC Loss: 0.05690065\n",
      "Iteration: 7460, Total Loss: 0.06461506, Physics Loss: 0.00788847, BC Loss: 0.05672659\n",
      "Iteration: 7470, Total Loss: 0.06433327, Physics Loss: 0.00786470, BC Loss: 0.05646858\n",
      "Iteration: 7480, Total Loss: 0.06418133, Physics Loss: 0.00768902, BC Loss: 0.05649231\n",
      "Iteration: 7490, Total Loss: 0.06392220, Physics Loss: 0.00762138, BC Loss: 0.05630083\n",
      "Iteration: 7500, Total Loss: 0.06372588, Physics Loss: 0.00743274, BC Loss: 0.05629314\n",
      "Iteration: 7510, Total Loss: 0.06357560, Physics Loss: 0.00738587, BC Loss: 0.05618973\n",
      "Iteration: 7520, Total Loss: 0.06330044, Physics Loss: 0.00768891, BC Loss: 0.05561153\n",
      "Iteration: 7530, Total Loss: 0.06314383, Physics Loss: 0.00784320, BC Loss: 0.05530063\n",
      "Iteration: 7540, Total Loss: 0.06300923, Physics Loss: 0.00780178, BC Loss: 0.05520745\n",
      "Iteration: 7550, Total Loss: 0.06282679, Physics Loss: 0.00791263, BC Loss: 0.05491416\n",
      "Iteration: 7560, Total Loss: 0.06254864, Physics Loss: 0.00781229, BC Loss: 0.05473635\n",
      "Iteration: 7570, Total Loss: 0.06233751, Physics Loss: 0.00779214, BC Loss: 0.05454537\n",
      "Iteration: 7580, Total Loss: 0.06221576, Physics Loss: 0.00777119, BC Loss: 0.05444457\n",
      "Iteration: 7590, Total Loss: 0.06216524, Physics Loss: 0.00780340, BC Loss: 0.05436184\n",
      "Iteration: 7600, Total Loss: 0.06211568, Physics Loss: 0.00798305, BC Loss: 0.05413263\n",
      "Iteration: 7610, Total Loss: 0.06203669, Physics Loss: 0.00800345, BC Loss: 0.05403324\n",
      "Iteration: 7620, Total Loss: 0.06190207, Physics Loss: 0.00796551, BC Loss: 0.05393656\n",
      "Iteration: 7630, Total Loss: 0.06177676, Physics Loss: 0.00797473, BC Loss: 0.05380203\n",
      "Iteration: 7640, Total Loss: 0.06166732, Physics Loss: 0.00801288, BC Loss: 0.05365445\n",
      "Iteration: 7650, Total Loss: 0.06149470, Physics Loss: 0.00820119, BC Loss: 0.05329351\n",
      "Iteration: 7660, Total Loss: 0.06119022, Physics Loss: 0.00818487, BC Loss: 0.05300535\n",
      "Iteration: 7670, Total Loss: 0.06098485, Physics Loss: 0.00807893, BC Loss: 0.05290592\n",
      "Iteration: 7680, Total Loss: 0.06075618, Physics Loss: 0.00811985, BC Loss: 0.05263633\n",
      "Iteration: 7690, Total Loss: 0.06051681, Physics Loss: 0.00804898, BC Loss: 0.05246783\n",
      "Iteration: 7700, Total Loss: 0.06029069, Physics Loss: 0.00804101, BC Loss: 0.05224968\n",
      "Iteration: 7710, Total Loss: 0.06016710, Physics Loss: 0.00796512, BC Loss: 0.05220198\n",
      "Iteration: 7720, Total Loss: 0.06001732, Physics Loss: 0.00774010, BC Loss: 0.05227723\n",
      "Iteration: 7730, Total Loss: 0.05988941, Physics Loss: 0.00776162, BC Loss: 0.05212779\n",
      "Iteration: 7740, Total Loss: 0.05979833, Physics Loss: 0.00772431, BC Loss: 0.05207402\n",
      "Iteration: 7750, Total Loss: 0.05965200, Physics Loss: 0.00750403, BC Loss: 0.05214797\n",
      "Iteration: 7760, Total Loss: 0.05953673, Physics Loss: 0.00739540, BC Loss: 0.05214132\n",
      "Iteration: 7770, Total Loss: 0.05929023, Physics Loss: 0.00748415, BC Loss: 0.05180608\n",
      "Iteration: 7780, Total Loss: 0.05913655, Physics Loss: 0.00773900, BC Loss: 0.05139755\n",
      "Iteration: 7790, Total Loss: 0.05895011, Physics Loss: 0.00766307, BC Loss: 0.05128704\n",
      "Iteration: 7800, Total Loss: 0.05864030, Physics Loss: 0.00750816, BC Loss: 0.05113215\n",
      "Iteration: 7810, Total Loss: 0.05846876, Physics Loss: 0.00777148, BC Loss: 0.05069729\n",
      "Iteration: 7820, Total Loss: 0.05839242, Physics Loss: 0.00771432, BC Loss: 0.05067810\n",
      "Iteration: 7830, Total Loss: 0.05824405, Physics Loss: 0.00762559, BC Loss: 0.05061845\n",
      "Iteration: 7840, Total Loss: 0.05805067, Physics Loss: 0.00760061, BC Loss: 0.05045006\n",
      "Iteration: 7850, Total Loss: 0.05786213, Physics Loss: 0.00764108, BC Loss: 0.05022105\n",
      "Iteration: 7860, Total Loss: 0.05774388, Physics Loss: 0.00760401, BC Loss: 0.05013987\n",
      "Iteration: 7870, Total Loss: 0.05763031, Physics Loss: 0.00753100, BC Loss: 0.05009931\n",
      "Iteration: 7880, Total Loss: 0.05749025, Physics Loss: 0.00771619, BC Loss: 0.04977406\n",
      "Iteration: 7890, Total Loss: 0.05719348, Physics Loss: 0.00818130, BC Loss: 0.04901218\n",
      "Iteration: 7900, Total Loss: 0.05695007, Physics Loss: 0.00788746, BC Loss: 0.04906261\n",
      "Iteration: 7910, Total Loss: 0.05687175, Physics Loss: 0.00796325, BC Loss: 0.04890850\n",
      "Iteration: 7920, Total Loss: 0.05662747, Physics Loss: 0.00774428, BC Loss: 0.04888319\n",
      "Iteration: 7930, Total Loss: 0.05646705, Physics Loss: 0.00750328, BC Loss: 0.04896377\n",
      "Iteration: 7940, Total Loss: 0.05631924, Physics Loss: 0.00750330, BC Loss: 0.04881594\n",
      "Iteration: 7950, Total Loss: 0.05618211, Physics Loss: 0.00770199, BC Loss: 0.04848012\n",
      "Iteration: 7960, Total Loss: 0.05608437, Physics Loss: 0.00775261, BC Loss: 0.04833176\n",
      "Iteration: 7970, Total Loss: 0.05597993, Physics Loss: 0.00773108, BC Loss: 0.04824885\n",
      "Iteration: 7980, Total Loss: 0.05588629, Physics Loss: 0.00773515, BC Loss: 0.04815114\n",
      "Iteration: 7990, Total Loss: 0.05577523, Physics Loss: 0.00755781, BC Loss: 0.04821742\n",
      "Iteration: 8000, Total Loss: 0.05567165, Physics Loss: 0.00767848, BC Loss: 0.04799317\n",
      "Iteration: 8010, Total Loss: 122.01757050, Physics Loss: 109.26130676, BC Loss: 12.75626659\n",
      "Iteration: 8020, Total Loss: 2.13983583, Physics Loss: 1.93647552, BC Loss: 0.20336026\n",
      "Iteration: 8030, Total Loss: 0.65081805, Physics Loss: 0.46734881, BC Loss: 0.18346924\n",
      "Iteration: 8040, Total Loss: 0.25344491, Physics Loss: 0.10298763, BC Loss: 0.15045726\n",
      "Iteration: 8050, Total Loss: 0.17741334, Physics Loss: 0.04793668, BC Loss: 0.12947667\n",
      "Iteration: 8060, Total Loss: 0.14542609, Physics Loss: 0.03831702, BC Loss: 0.10710906\n",
      "Iteration: 8070, Total Loss: 0.12430667, Physics Loss: 0.02626455, BC Loss: 0.09804212\n",
      "Iteration: 8080, Total Loss: 0.11659646, Physics Loss: 0.02243904, BC Loss: 0.09415742\n",
      "Iteration: 8090, Total Loss: 0.10917288, Physics Loss: 0.01928366, BC Loss: 0.08988922\n",
      "Iteration: 8100, Total Loss: 0.10606053, Physics Loss: 0.01912674, BC Loss: 0.08693379\n",
      "Iteration: 8110, Total Loss: 0.10339416, Physics Loss: 0.01617757, BC Loss: 0.08721659\n",
      "Iteration: 8120, Total Loss: 0.10013971, Physics Loss: 0.01597853, BC Loss: 0.08416117\n",
      "Iteration: 8130, Total Loss: 0.09753549, Physics Loss: 0.01374542, BC Loss: 0.08379007\n",
      "Iteration: 8140, Total Loss: 0.09557963, Physics Loss: 0.01363407, BC Loss: 0.08194556\n",
      "Iteration: 8150, Total Loss: 0.09407747, Physics Loss: 0.01266824, BC Loss: 0.08140922\n",
      "Iteration: 8160, Total Loss: 0.09259482, Physics Loss: 0.01240908, BC Loss: 0.08018573\n",
      "Iteration: 8170, Total Loss: 0.08977030, Physics Loss: 0.01199062, BC Loss: 0.07777968\n",
      "Iteration: 8180, Total Loss: 0.08899830, Physics Loss: 0.01211951, BC Loss: 0.07687879\n",
      "Iteration: 8190, Total Loss: 0.08772193, Physics Loss: 0.01128578, BC Loss: 0.07643615\n",
      "Iteration: 8200, Total Loss: 0.08652216, Physics Loss: 0.01163421, BC Loss: 0.07488795\n",
      "Iteration: 8210, Total Loss: 0.08554611, Physics Loss: 0.01095396, BC Loss: 0.07459216\n",
      "Iteration: 8220, Total Loss: 0.08435325, Physics Loss: 0.01047849, BC Loss: 0.07387476\n",
      "Iteration: 8230, Total Loss: 0.08368387, Physics Loss: 0.01000776, BC Loss: 0.07367611\n",
      "Iteration: 8240, Total Loss: 0.08293085, Physics Loss: 0.01000447, BC Loss: 0.07292638\n",
      "Iteration: 8250, Total Loss: 0.08212963, Physics Loss: 0.00938935, BC Loss: 0.07274027\n",
      "Iteration: 8260, Total Loss: 0.08152866, Physics Loss: 0.00892140, BC Loss: 0.07260726\n",
      "Iteration: 8270, Total Loss: 0.08095655, Physics Loss: 0.00853411, BC Loss: 0.07242244\n",
      "Iteration: 8280, Total Loss: 0.08067739, Physics Loss: 0.00850778, BC Loss: 0.07216962\n",
      "Iteration: 8290, Total Loss: 0.08042022, Physics Loss: 0.00866829, BC Loss: 0.07175193\n",
      "Iteration: 8300, Total Loss: 0.08013091, Physics Loss: 0.00859363, BC Loss: 0.07153729\n",
      "Iteration: 8310, Total Loss: 0.07945892, Physics Loss: 0.00853711, BC Loss: 0.07092182\n",
      "Iteration: 8320, Total Loss: 0.07867001, Physics Loss: 0.00833078, BC Loss: 0.07033923\n",
      "Iteration: 8330, Total Loss: 0.07817457, Physics Loss: 0.00778048, BC Loss: 0.07039408\n",
      "Iteration: 8340, Total Loss: 0.07760547, Physics Loss: 0.00787256, BC Loss: 0.06973292\n",
      "Iteration: 8350, Total Loss: 0.07716551, Physics Loss: 0.00828546, BC Loss: 0.06888005\n",
      "Iteration: 8360, Total Loss: 0.07693961, Physics Loss: 0.00828393, BC Loss: 0.06865567\n",
      "Iteration: 8370, Total Loss: 0.07648598, Physics Loss: 0.00853474, BC Loss: 0.06795125\n",
      "Iteration: 8380, Total Loss: 0.07577062, Physics Loss: 0.00857416, BC Loss: 0.06719647\n",
      "Iteration: 8390, Total Loss: 0.07515582, Physics Loss: 0.00882380, BC Loss: 0.06633202\n",
      "Iteration: 8400, Total Loss: 0.07472216, Physics Loss: 0.00892040, BC Loss: 0.06580176\n",
      "Iteration: 8410, Total Loss: 0.07419608, Physics Loss: 0.00943700, BC Loss: 0.06475908\n",
      "Iteration: 8420, Total Loss: 0.07385994, Physics Loss: 0.00917975, BC Loss: 0.06468019\n",
      "Iteration: 8430, Total Loss: 0.07342032, Physics Loss: 0.00924024, BC Loss: 0.06418008\n",
      "Iteration: 8440, Total Loss: 0.07292284, Physics Loss: 0.00949781, BC Loss: 0.06342503\n",
      "Iteration: 8450, Total Loss: 0.07259511, Physics Loss: 0.00967603, BC Loss: 0.06291908\n",
      "Iteration: 8460, Total Loss: 0.07221899, Physics Loss: 0.00891940, BC Loss: 0.06329960\n",
      "Iteration: 8470, Total Loss: 0.07160036, Physics Loss: 0.00842167, BC Loss: 0.06317868\n",
      "Iteration: 8480, Total Loss: 0.07126062, Physics Loss: 0.00820869, BC Loss: 0.06305192\n",
      "Iteration: 8490, Total Loss: 0.07106462, Physics Loss: 0.00807578, BC Loss: 0.06298884\n",
      "Iteration: 8500, Total Loss: 0.07074875, Physics Loss: 0.00794755, BC Loss: 0.06280120\n",
      "Iteration: 8510, Total Loss: 0.07039651, Physics Loss: 0.00776938, BC Loss: 0.06262712\n",
      "Iteration: 8520, Total Loss: 0.06988865, Physics Loss: 0.00747945, BC Loss: 0.06240920\n",
      "Iteration: 8530, Total Loss: 0.06964076, Physics Loss: 0.00751586, BC Loss: 0.06212490\n",
      "Iteration: 8540, Total Loss: 0.06943680, Physics Loss: 0.00743236, BC Loss: 0.06200443\n",
      "Iteration: 8550, Total Loss: 0.06922755, Physics Loss: 0.00728708, BC Loss: 0.06194048\n",
      "Iteration: 8560, Total Loss: 0.06871384, Physics Loss: 0.00740237, BC Loss: 0.06131147\n",
      "Iteration: 8570, Total Loss: 0.06808410, Physics Loss: 0.00700917, BC Loss: 0.06107493\n",
      "Iteration: 8580, Total Loss: 0.06786269, Physics Loss: 0.00671190, BC Loss: 0.06115078\n",
      "Iteration: 8590, Total Loss: 0.06731416, Physics Loss: 0.00660152, BC Loss: 0.06071264\n",
      "Iteration: 8600, Total Loss: 0.06663147, Physics Loss: 0.00657537, BC Loss: 0.06005609\n",
      "Iteration: 8610, Total Loss: 0.06628428, Physics Loss: 0.00640192, BC Loss: 0.05988236\n",
      "Iteration: 8620, Total Loss: 0.06615809, Physics Loss: 0.00634344, BC Loss: 0.05981464\n",
      "Iteration: 8630, Total Loss: 0.06582786, Physics Loss: 0.00619465, BC Loss: 0.05963321\n",
      "Iteration: 8640, Total Loss: 0.06547374, Physics Loss: 0.00609318, BC Loss: 0.05938055\n",
      "Iteration: 8650, Total Loss: 0.06521285, Physics Loss: 0.00628978, BC Loss: 0.05892306\n",
      "Iteration: 8660, Total Loss: 0.06501855, Physics Loss: 0.00589467, BC Loss: 0.05912388\n",
      "Iteration: 8670, Total Loss: 0.06488341, Physics Loss: 0.00595733, BC Loss: 0.05892608\n",
      "Iteration: 8680, Total Loss: 0.06468272, Physics Loss: 0.00615376, BC Loss: 0.05852896\n",
      "Iteration: 8690, Total Loss: 0.06443372, Physics Loss: 0.00620233, BC Loss: 0.05823138\n",
      "Iteration: 8700, Total Loss: 0.06426270, Physics Loss: 0.00628529, BC Loss: 0.05797741\n",
      "Iteration: 8710, Total Loss: 0.06398878, Physics Loss: 0.00640339, BC Loss: 0.05758540\n",
      "Iteration: 8720, Total Loss: 0.06375611, Physics Loss: 0.00637798, BC Loss: 0.05737812\n",
      "Iteration: 8730, Total Loss: 0.06346077, Physics Loss: 0.00632733, BC Loss: 0.05713344\n",
      "Iteration: 8740, Total Loss: 0.06305654, Physics Loss: 0.00634126, BC Loss: 0.05671529\n",
      "Iteration: 8750, Total Loss: 0.06290260, Physics Loss: 0.00646353, BC Loss: 0.05643907\n",
      "Iteration: 8760, Total Loss: 0.06259647, Physics Loss: 0.00648112, BC Loss: 0.05611535\n",
      "Iteration: 8770, Total Loss: 0.06230589, Physics Loss: 0.00653321, BC Loss: 0.05577268\n",
      "Iteration: 8780, Total Loss: 0.06198489, Physics Loss: 0.00663888, BC Loss: 0.05534601\n",
      "Iteration: 8790, Total Loss: 0.06185429, Physics Loss: 0.00667432, BC Loss: 0.05517997\n",
      "Iteration: 8800, Total Loss: 0.06156311, Physics Loss: 0.00641908, BC Loss: 0.05514403\n",
      "Iteration: 8810, Total Loss: 0.06136824, Physics Loss: 0.00647378, BC Loss: 0.05489445\n",
      "Iteration: 8820, Total Loss: 0.06111205, Physics Loss: 0.00661389, BC Loss: 0.05449816\n",
      "Iteration: 8830, Total Loss: 0.06077911, Physics Loss: 0.00650324, BC Loss: 0.05427587\n",
      "Iteration: 8840, Total Loss: 0.06050587, Physics Loss: 0.00659818, BC Loss: 0.05390769\n",
      "Iteration: 8850, Total Loss: 0.06034328, Physics Loss: 0.00663620, BC Loss: 0.05370708\n",
      "Iteration: 8860, Total Loss: 0.06007421, Physics Loss: 0.00656093, BC Loss: 0.05351328\n",
      "Iteration: 8870, Total Loss: 0.05987905, Physics Loss: 0.00655257, BC Loss: 0.05332648\n",
      "Iteration: 8880, Total Loss: 0.05971435, Physics Loss: 0.00655150, BC Loss: 0.05316286\n",
      "Iteration: 8890, Total Loss: 0.05939611, Physics Loss: 0.00667027, BC Loss: 0.05272584\n",
      "Iteration: 8900, Total Loss: 0.05911459, Physics Loss: 0.00663396, BC Loss: 0.05248063\n",
      "Iteration: 8910, Total Loss: 0.05893110, Physics Loss: 0.00654955, BC Loss: 0.05238155\n",
      "Iteration: 8920, Total Loss: 0.05866152, Physics Loss: 0.00666897, BC Loss: 0.05199255\n",
      "Iteration: 8930, Total Loss: 0.05828663, Physics Loss: 0.00625766, BC Loss: 0.05202896\n",
      "Iteration: 8940, Total Loss: 0.05796016, Physics Loss: 0.00606880, BC Loss: 0.05189137\n",
      "Iteration: 8950, Total Loss: 0.05765756, Physics Loss: 0.00619497, BC Loss: 0.05146259\n",
      "Iteration: 8960, Total Loss: 0.05748574, Physics Loss: 0.00609370, BC Loss: 0.05139205\n",
      "Iteration: 8970, Total Loss: 0.05723625, Physics Loss: 0.00595355, BC Loss: 0.05128270\n",
      "Iteration: 8980, Total Loss: 0.05691389, Physics Loss: 0.00632876, BC Loss: 0.05058513\n",
      "Iteration: 8990, Total Loss: 0.05665020, Physics Loss: 0.00632106, BC Loss: 0.05032913\n",
      "Iteration: 9000, Total Loss: 0.05641333, Physics Loss: 0.00639018, BC Loss: 0.05002315\n",
      "Iteration: 9010, Total Loss: 85.61122131, Physics Loss: 79.63775635, BC Loss: 5.97346497\n",
      "Iteration: 9020, Total Loss: 34.24501801, Physics Loss: 28.20222092, BC Loss: 6.04279709\n",
      "Iteration: 9030, Total Loss: 5.22539330, Physics Loss: 4.14602852, BC Loss: 1.07936502\n",
      "Iteration: 9040, Total Loss: 1.00907815, Physics Loss: 0.37585273, BC Loss: 0.63322538\n",
      "Iteration: 9050, Total Loss: 0.80464816, Physics Loss: 0.26984805, BC Loss: 0.53480011\n",
      "Iteration: 9060, Total Loss: 0.50333130, Physics Loss: 0.15067047, BC Loss: 0.35266083\n",
      "Iteration: 9070, Total Loss: 0.41092283, Physics Loss: 0.14317982, BC Loss: 0.26774302\n",
      "Iteration: 9080, Total Loss: 0.34559530, Physics Loss: 0.11870620, BC Loss: 0.22688912\n",
      "Iteration: 9090, Total Loss: 0.28683618, Physics Loss: 0.08631712, BC Loss: 0.20051906\n",
      "Iteration: 9100, Total Loss: 0.24841665, Physics Loss: 0.06914298, BC Loss: 0.17927366\n",
      "Iteration: 9110, Total Loss: 0.22652052, Physics Loss: 0.04945276, BC Loss: 0.17706776\n",
      "Iteration: 9120, Total Loss: 0.21531102, Physics Loss: 0.04441854, BC Loss: 0.17089249\n",
      "Iteration: 9130, Total Loss: 0.20564777, Physics Loss: 0.04653403, BC Loss: 0.15911373\n",
      "Iteration: 9140, Total Loss: 0.19914460, Physics Loss: 0.04027939, BC Loss: 0.15886521\n",
      "Iteration: 9150, Total Loss: 0.18977566, Physics Loss: 0.04350765, BC Loss: 0.14626801\n",
      "Iteration: 9160, Total Loss: 0.18126339, Physics Loss: 0.04021525, BC Loss: 0.14104813\n",
      "Iteration: 9170, Total Loss: 0.17259063, Physics Loss: 0.03996503, BC Loss: 0.13262559\n",
      "Iteration: 9180, Total Loss: 0.16731346, Physics Loss: 0.03468972, BC Loss: 0.13262373\n",
      "Iteration: 9190, Total Loss: 0.16275004, Physics Loss: 0.03054649, BC Loss: 0.13220355\n",
      "Iteration: 9200, Total Loss: 0.15733373, Physics Loss: 0.02817094, BC Loss: 0.12916279\n",
      "Iteration: 9210, Total Loss: 0.15487485, Physics Loss: 0.02614565, BC Loss: 0.12872919\n",
      "Iteration: 9220, Total Loss: 0.15222859, Physics Loss: 0.02576105, BC Loss: 0.12646754\n",
      "Iteration: 9230, Total Loss: 0.14735733, Physics Loss: 0.02357822, BC Loss: 0.12377911\n",
      "Iteration: 9240, Total Loss: 0.14391072, Physics Loss: 0.02360259, BC Loss: 0.12030813\n",
      "Iteration: 9250, Total Loss: 0.14122710, Physics Loss: 0.02041188, BC Loss: 0.12081522\n",
      "Iteration: 9260, Total Loss: 0.13913776, Physics Loss: 0.02136626, BC Loss: 0.11777151\n",
      "Iteration: 9270, Total Loss: 0.13696969, Physics Loss: 0.02096109, BC Loss: 0.11600859\n",
      "Iteration: 9280, Total Loss: 0.13559803, Physics Loss: 0.02071876, BC Loss: 0.11487927\n",
      "Iteration: 9290, Total Loss: 0.13230942, Physics Loss: 0.02205409, BC Loss: 0.11025533\n",
      "Iteration: 9300, Total Loss: 0.12970375, Physics Loss: 0.02205614, BC Loss: 0.10764760\n",
      "Iteration: 9310, Total Loss: 0.12636229, Physics Loss: 0.02330559, BC Loss: 0.10305669\n",
      "Iteration: 9320, Total Loss: 0.12386545, Physics Loss: 0.02104411, BC Loss: 0.10282134\n",
      "Iteration: 9330, Total Loss: 0.12168013, Physics Loss: 0.02092247, BC Loss: 0.10075766\n",
      "Iteration: 9340, Total Loss: 0.11885446, Physics Loss: 0.02167572, BC Loss: 0.09717874\n",
      "Iteration: 9350, Total Loss: 0.11554525, Physics Loss: 0.02134517, BC Loss: 0.09420008\n",
      "Iteration: 9360, Total Loss: 0.11172792, Physics Loss: 0.01978036, BC Loss: 0.09194756\n",
      "Iteration: 9370, Total Loss: 0.10887022, Physics Loss: 0.01967438, BC Loss: 0.08919584\n",
      "Iteration: 9380, Total Loss: 0.10593752, Physics Loss: 0.01829825, BC Loss: 0.08763927\n",
      "Iteration: 9390, Total Loss: 0.10388238, Physics Loss: 0.01769924, BC Loss: 0.08618314\n",
      "Iteration: 9400, Total Loss: 0.10175539, Physics Loss: 0.01729187, BC Loss: 0.08446352\n",
      "Iteration: 9410, Total Loss: 0.09939583, Physics Loss: 0.01649740, BC Loss: 0.08289842\n",
      "Iteration: 9420, Total Loss: 0.09822594, Physics Loss: 0.01535141, BC Loss: 0.08287453\n",
      "Iteration: 9430, Total Loss: 0.09713122, Physics Loss: 0.01465379, BC Loss: 0.08247744\n",
      "Iteration: 9440, Total Loss: 0.09619065, Physics Loss: 0.01399243, BC Loss: 0.08219822\n",
      "Iteration: 9450, Total Loss: 0.09568219, Physics Loss: 0.01321475, BC Loss: 0.08246744\n",
      "Iteration: 9460, Total Loss: 0.09493919, Physics Loss: 0.01347917, BC Loss: 0.08146002\n",
      "Iteration: 9470, Total Loss: 0.09423911, Physics Loss: 0.01270151, BC Loss: 0.08153760\n",
      "Iteration: 9480, Total Loss: 0.09298141, Physics Loss: 0.01335193, BC Loss: 0.07962948\n",
      "Iteration: 9490, Total Loss: 0.09241244, Physics Loss: 0.01236636, BC Loss: 0.08004609\n",
      "Iteration: 9500, Total Loss: 0.09104822, Physics Loss: 0.01136050, BC Loss: 0.07968772\n",
      "Iteration: 9510, Total Loss: 0.09022611, Physics Loss: 0.01087869, BC Loss: 0.07934742\n",
      "Iteration: 9520, Total Loss: 0.08968871, Physics Loss: 0.01036881, BC Loss: 0.07931989\n",
      "Iteration: 9530, Total Loss: 0.08897389, Physics Loss: 0.01020386, BC Loss: 0.07877003\n",
      "Iteration: 9540, Total Loss: 0.08808568, Physics Loss: 0.01009796, BC Loss: 0.07798772\n",
      "Iteration: 9550, Total Loss: 0.08717379, Physics Loss: 0.00954668, BC Loss: 0.07762711\n",
      "Iteration: 9560, Total Loss: 0.08639533, Physics Loss: 0.00884435, BC Loss: 0.07755098\n",
      "Iteration: 9570, Total Loss: 0.08592543, Physics Loss: 0.00890958, BC Loss: 0.07701585\n",
      "Iteration: 9580, Total Loss: 0.08537529, Physics Loss: 0.00928151, BC Loss: 0.07609378\n",
      "Iteration: 9590, Total Loss: 0.08502145, Physics Loss: 0.00866590, BC Loss: 0.07635555\n",
      "Iteration: 9600, Total Loss: 0.08461105, Physics Loss: 0.00881823, BC Loss: 0.07579282\n",
      "Iteration: 9610, Total Loss: 0.08427155, Physics Loss: 0.00874736, BC Loss: 0.07552419\n",
      "Iteration: 9620, Total Loss: 0.08351301, Physics Loss: 0.00811321, BC Loss: 0.07539979\n",
      "Iteration: 9630, Total Loss: 0.08284475, Physics Loss: 0.00825888, BC Loss: 0.07458587\n",
      "Iteration: 9640, Total Loss: 0.08236965, Physics Loss: 0.00824854, BC Loss: 0.07412111\n",
      "Iteration: 9650, Total Loss: 0.08204234, Physics Loss: 0.00812629, BC Loss: 0.07391605\n",
      "Iteration: 9660, Total Loss: 0.08176144, Physics Loss: 0.00824096, BC Loss: 0.07352048\n",
      "Iteration: 9670, Total Loss: 0.08126999, Physics Loss: 0.00774183, BC Loss: 0.07352816\n",
      "Iteration: 9680, Total Loss: 0.08084372, Physics Loss: 0.00835985, BC Loss: 0.07248387\n",
      "Iteration: 9690, Total Loss: 0.08061971, Physics Loss: 0.00871470, BC Loss: 0.07190501\n",
      "Iteration: 9700, Total Loss: 0.08027714, Physics Loss: 0.00829769, BC Loss: 0.07197946\n",
      "Iteration: 9710, Total Loss: 0.08007891, Physics Loss: 0.00852058, BC Loss: 0.07155833\n",
      "Iteration: 9720, Total Loss: 0.07982026, Physics Loss: 0.00854738, BC Loss: 0.07127288\n",
      "Iteration: 9730, Total Loss: 0.07938658, Physics Loss: 0.00784296, BC Loss: 0.07154362\n",
      "Iteration: 9740, Total Loss: 0.07900650, Physics Loss: 0.00797405, BC Loss: 0.07103245\n",
      "Iteration: 9750, Total Loss: 0.07867470, Physics Loss: 0.00821779, BC Loss: 0.07045692\n",
      "Iteration: 9760, Total Loss: 0.07823844, Physics Loss: 0.00791013, BC Loss: 0.07032831\n",
      "Iteration: 9770, Total Loss: 0.07798453, Physics Loss: 0.00781948, BC Loss: 0.07016505\n",
      "Iteration: 9780, Total Loss: 0.07758511, Physics Loss: 0.00799274, BC Loss: 0.06959237\n",
      "Iteration: 9790, Total Loss: 0.07727221, Physics Loss: 0.00735406, BC Loss: 0.06991814\n",
      "Iteration: 9800, Total Loss: 0.07693795, Physics Loss: 0.00747509, BC Loss: 0.06946287\n",
      "Iteration: 9810, Total Loss: 0.07674922, Physics Loss: 0.00734230, BC Loss: 0.06940693\n",
      "Iteration: 9820, Total Loss: 0.07657155, Physics Loss: 0.00720906, BC Loss: 0.06936249\n",
      "Iteration: 9830, Total Loss: 0.07644518, Physics Loss: 0.00750057, BC Loss: 0.06894461\n",
      "Iteration: 9840, Total Loss: 0.07612317, Physics Loss: 0.00719466, BC Loss: 0.06892851\n",
      "Iteration: 9850, Total Loss: 0.07583627, Physics Loss: 0.00733659, BC Loss: 0.06849968\n",
      "Iteration: 9860, Total Loss: 0.07560690, Physics Loss: 0.00716715, BC Loss: 0.06843974\n",
      "Iteration: 9870, Total Loss: 0.07516016, Physics Loss: 0.00755128, BC Loss: 0.06760888\n",
      "Iteration: 9880, Total Loss: 0.07489138, Physics Loss: 0.00769692, BC Loss: 0.06719446\n",
      "Iteration: 9890, Total Loss: 0.07453090, Physics Loss: 0.00766281, BC Loss: 0.06686809\n",
      "Iteration: 9900, Total Loss: 0.07440071, Physics Loss: 0.00755727, BC Loss: 0.06684344\n",
      "Iteration: 9910, Total Loss: 0.07418650, Physics Loss: 0.00753829, BC Loss: 0.06664821\n",
      "Iteration: 9920, Total Loss: 0.07397725, Physics Loss: 0.00713763, BC Loss: 0.06683962\n",
      "Iteration: 9930, Total Loss: 0.07382270, Physics Loss: 0.00719872, BC Loss: 0.06662398\n",
      "Iteration: 9940, Total Loss: 0.07349958, Physics Loss: 0.00709824, BC Loss: 0.06640135\n",
      "Iteration: 9950, Total Loss: 0.07328411, Physics Loss: 0.00698142, BC Loss: 0.06630269\n",
      "Iteration: 9960, Total Loss: 0.07316834, Physics Loss: 0.00720786, BC Loss: 0.06596048\n",
      "Iteration: 9970, Total Loss: 0.07285594, Physics Loss: 0.00725540, BC Loss: 0.06560054\n",
      "Iteration: 9980, Total Loss: 0.07258020, Physics Loss: 0.00712509, BC Loss: 0.06545512\n",
      "Iteration: 9990, Total Loss: 0.07223348, Physics Loss: 0.00709761, BC Loss: 0.06513587\n",
      "Iteration: 10000, Total Loss: 0.07199045, Physics Loss: 0.00709681, BC Loss: 0.06489363\n",
      "Iteration: 10010, Total Loss: 5.99859524, Physics Loss: 5.92460918, BC Loss: 0.07398608\n",
      "Iteration: 10020, Total Loss: 5.04974461, Physics Loss: 4.94276333, BC Loss: 0.10698129\n",
      "Iteration: 10030, Total Loss: 1.34687471, Physics Loss: 1.17838430, BC Loss: 0.16849047\n",
      "Iteration: 10040, Total Loss: 0.38543439, Physics Loss: 0.25221306, BC Loss: 0.13322134\n",
      "Iteration: 10050, Total Loss: 0.26635504, Physics Loss: 0.10186899, BC Loss: 0.16448604\n",
      "Iteration: 10060, Total Loss: 0.21378163, Physics Loss: 0.09592401, BC Loss: 0.11785761\n",
      "Iteration: 10070, Total Loss: 0.18940732, Physics Loss: 0.07544266, BC Loss: 0.11396466\n",
      "Iteration: 10080, Total Loss: 0.16773121, Physics Loss: 0.05678949, BC Loss: 0.11094172\n",
      "Iteration: 10090, Total Loss: 0.15566866, Physics Loss: 0.04591509, BC Loss: 0.10975357\n",
      "Iteration: 10100, Total Loss: 0.14633414, Physics Loss: 0.04435385, BC Loss: 0.10198029\n",
      "Iteration: 10110, Total Loss: 0.13728601, Physics Loss: 0.03920578, BC Loss: 0.09808023\n",
      "Iteration: 10120, Total Loss: 0.13156366, Physics Loss: 0.03184380, BC Loss: 0.09971985\n",
      "Iteration: 10130, Total Loss: 0.12351386, Physics Loss: 0.03282943, BC Loss: 0.09068444\n",
      "Iteration: 10140, Total Loss: 0.11969462, Physics Loss: 0.03082862, BC Loss: 0.08886600\n",
      "Iteration: 10150, Total Loss: 0.11579482, Physics Loss: 0.03091368, BC Loss: 0.08488114\n",
      "Iteration: 10160, Total Loss: 0.11199857, Physics Loss: 0.02756799, BC Loss: 0.08443058\n",
      "Iteration: 10170, Total Loss: 0.10846934, Physics Loss: 0.02348410, BC Loss: 0.08498524\n",
      "Iteration: 10180, Total Loss: 0.10670631, Physics Loss: 0.02296211, BC Loss: 0.08374420\n",
      "Iteration: 10190, Total Loss: 0.10252541, Physics Loss: 0.01887364, BC Loss: 0.08365178\n",
      "Iteration: 10200, Total Loss: 0.10037633, Physics Loss: 0.01925035, BC Loss: 0.08112598\n",
      "Iteration: 10210, Total Loss: 0.09863211, Physics Loss: 0.01697705, BC Loss: 0.08165506\n",
      "Iteration: 10220, Total Loss: 0.09682062, Physics Loss: 0.01683734, BC Loss: 0.07998327\n",
      "Iteration: 10230, Total Loss: 0.09522089, Physics Loss: 0.01464248, BC Loss: 0.08057841\n",
      "Iteration: 10240, Total Loss: 0.09344992, Physics Loss: 0.01506606, BC Loss: 0.07838386\n",
      "Iteration: 10250, Total Loss: 0.09148339, Physics Loss: 0.01538808, BC Loss: 0.07609531\n",
      "Iteration: 10260, Total Loss: 0.09004153, Physics Loss: 0.01445299, BC Loss: 0.07558853\n",
      "Iteration: 10270, Total Loss: 0.08881623, Physics Loss: 0.01505715, BC Loss: 0.07375908\n",
      "Iteration: 10280, Total Loss: 0.08769829, Physics Loss: 0.01455761, BC Loss: 0.07314068\n",
      "Iteration: 10290, Total Loss: 0.08681883, Physics Loss: 0.01398321, BC Loss: 0.07283562\n",
      "Iteration: 10300, Total Loss: 0.08580467, Physics Loss: 0.01439663, BC Loss: 0.07140804\n",
      "Iteration: 10310, Total Loss: 0.08528239, Physics Loss: 0.01409482, BC Loss: 0.07118756\n",
      "Iteration: 10320, Total Loss: 0.08448067, Physics Loss: 0.01417617, BC Loss: 0.07030449\n",
      "Iteration: 10330, Total Loss: 0.08367432, Physics Loss: 0.01377894, BC Loss: 0.06989537\n",
      "Iteration: 10340, Total Loss: 0.08284365, Physics Loss: 0.01362509, BC Loss: 0.06921855\n",
      "Iteration: 10350, Total Loss: 0.08176311, Physics Loss: 0.01236319, BC Loss: 0.06939992\n",
      "Iteration: 10360, Total Loss: 0.08116133, Physics Loss: 0.01258507, BC Loss: 0.06857626\n",
      "Iteration: 10370, Total Loss: 0.07989252, Physics Loss: 0.01291832, BC Loss: 0.06697419\n",
      "Iteration: 10380, Total Loss: 0.07944720, Physics Loss: 0.01289663, BC Loss: 0.06655057\n",
      "Iteration: 10390, Total Loss: 0.07880431, Physics Loss: 0.01255103, BC Loss: 0.06625328\n",
      "Iteration: 10400, Total Loss: 0.07811734, Physics Loss: 0.01182600, BC Loss: 0.06629135\n",
      "Iteration: 10410, Total Loss: 0.07757787, Physics Loss: 0.01179878, BC Loss: 0.06577909\n",
      "Iteration: 10420, Total Loss: 0.07719782, Physics Loss: 0.01157156, BC Loss: 0.06562626\n",
      "Iteration: 10430, Total Loss: 0.07669904, Physics Loss: 0.01107750, BC Loss: 0.06562154\n",
      "Iteration: 10440, Total Loss: 0.07627822, Physics Loss: 0.01046473, BC Loss: 0.06581349\n",
      "Iteration: 10450, Total Loss: 0.07603334, Physics Loss: 0.01008832, BC Loss: 0.06594502\n",
      "Iteration: 10460, Total Loss: 0.07565443, Physics Loss: 0.01018346, BC Loss: 0.06547098\n",
      "Iteration: 10470, Total Loss: 0.07518153, Physics Loss: 0.01048342, BC Loss: 0.06469811\n",
      "Iteration: 10480, Total Loss: 0.07478032, Physics Loss: 0.01060647, BC Loss: 0.06417385\n",
      "Iteration: 10490, Total Loss: 0.07421093, Physics Loss: 0.01060354, BC Loss: 0.06360739\n",
      "Iteration: 10500, Total Loss: 0.07373642, Physics Loss: 0.01015945, BC Loss: 0.06357697\n",
      "Iteration: 10510, Total Loss: 0.07346880, Physics Loss: 0.00967262, BC Loss: 0.06379618\n",
      "Iteration: 10520, Total Loss: 0.07282326, Physics Loss: 0.00977943, BC Loss: 0.06304383\n",
      "Iteration: 10530, Total Loss: 0.07211896, Physics Loss: 0.00982032, BC Loss: 0.06229864\n",
      "Iteration: 10540, Total Loss: 0.07146844, Physics Loss: 0.00934905, BC Loss: 0.06211939\n",
      "Iteration: 10550, Total Loss: 0.07061939, Physics Loss: 0.00972901, BC Loss: 0.06089038\n",
      "Iteration: 10560, Total Loss: 0.07009213, Physics Loss: 0.01014861, BC Loss: 0.05994352\n",
      "Iteration: 10570, Total Loss: 0.06972236, Physics Loss: 0.01047600, BC Loss: 0.05924637\n",
      "Iteration: 10580, Total Loss: 0.06906989, Physics Loss: 0.01042284, BC Loss: 0.05864706\n",
      "Iteration: 10590, Total Loss: 0.06845590, Physics Loss: 0.01029387, BC Loss: 0.05816203\n",
      "Iteration: 10600, Total Loss: 0.06744464, Physics Loss: 0.01045035, BC Loss: 0.05699430\n",
      "Iteration: 10610, Total Loss: 0.06689748, Physics Loss: 0.01017337, BC Loss: 0.05672411\n",
      "Iteration: 10620, Total Loss: 0.06644754, Physics Loss: 0.00984649, BC Loss: 0.05660105\n",
      "Iteration: 10630, Total Loss: 0.06601568, Physics Loss: 0.00960705, BC Loss: 0.05640863\n",
      "Iteration: 10640, Total Loss: 0.06554984, Physics Loss: 0.00918354, BC Loss: 0.05636629\n",
      "Iteration: 10650, Total Loss: 0.06535133, Physics Loss: 0.00919678, BC Loss: 0.05615456\n",
      "Iteration: 10660, Total Loss: 0.06515983, Physics Loss: 0.00900213, BC Loss: 0.05615769\n",
      "Iteration: 10670, Total Loss: 0.06471242, Physics Loss: 0.00858018, BC Loss: 0.05613224\n",
      "Iteration: 10680, Total Loss: 0.06441738, Physics Loss: 0.00859550, BC Loss: 0.05582187\n",
      "Iteration: 10690, Total Loss: 0.06411364, Physics Loss: 0.00863361, BC Loss: 0.05548003\n",
      "Iteration: 10700, Total Loss: 0.06384870, Physics Loss: 0.00866811, BC Loss: 0.05518059\n",
      "Iteration: 10710, Total Loss: 0.06342413, Physics Loss: 0.00885075, BC Loss: 0.05457339\n",
      "Iteration: 10720, Total Loss: 0.06304332, Physics Loss: 0.00882981, BC Loss: 0.05421351\n",
      "Iteration: 10730, Total Loss: 0.06273670, Physics Loss: 0.00875579, BC Loss: 0.05398090\n",
      "Iteration: 10740, Total Loss: 0.06246222, Physics Loss: 0.00840053, BC Loss: 0.05406169\n",
      "Iteration: 10750, Total Loss: 0.06213147, Physics Loss: 0.00805763, BC Loss: 0.05407384\n",
      "Iteration: 10760, Total Loss: 0.06188296, Physics Loss: 0.00805067, BC Loss: 0.05383229\n",
      "Iteration: 10770, Total Loss: 0.06154269, Physics Loss: 0.00765392, BC Loss: 0.05388877\n",
      "Iteration: 10780, Total Loss: 0.06132048, Physics Loss: 0.00748115, BC Loss: 0.05383933\n",
      "Iteration: 10790, Total Loss: 0.06112670, Physics Loss: 0.00743639, BC Loss: 0.05369031\n",
      "Iteration: 10800, Total Loss: 0.06087463, Physics Loss: 0.00726905, BC Loss: 0.05360558\n",
      "Iteration: 10810, Total Loss: 0.06054613, Physics Loss: 0.00730124, BC Loss: 0.05324489\n",
      "Iteration: 10820, Total Loss: 0.06020628, Physics Loss: 0.00719572, BC Loss: 0.05301056\n",
      "Iteration: 10830, Total Loss: 0.06003911, Physics Loss: 0.00744275, BC Loss: 0.05259636\n",
      "Iteration: 10840, Total Loss: 0.05991198, Physics Loss: 0.00748041, BC Loss: 0.05243157\n",
      "Iteration: 10850, Total Loss: 0.05974464, Physics Loss: 0.00748016, BC Loss: 0.05226448\n",
      "Iteration: 10860, Total Loss: 0.05941022, Physics Loss: 0.00805047, BC Loss: 0.05135975\n",
      "Iteration: 10870, Total Loss: 0.05916616, Physics Loss: 0.00800945, BC Loss: 0.05115670\n",
      "Iteration: 10880, Total Loss: 0.05900350, Physics Loss: 0.00806180, BC Loss: 0.05094170\n",
      "Iteration: 10890, Total Loss: 0.05879951, Physics Loss: 0.00790521, BC Loss: 0.05089429\n",
      "Iteration: 10900, Total Loss: 0.05851689, Physics Loss: 0.00784010, BC Loss: 0.05067679\n",
      "Iteration: 10910, Total Loss: 0.05826055, Physics Loss: 0.00790215, BC Loss: 0.05035840\n",
      "Iteration: 10920, Total Loss: 0.05802712, Physics Loss: 0.00814855, BC Loss: 0.04987857\n",
      "Iteration: 10930, Total Loss: 0.05781219, Physics Loss: 0.00826178, BC Loss: 0.04955041\n",
      "Iteration: 10940, Total Loss: 0.05748772, Physics Loss: 0.00805607, BC Loss: 0.04943164\n",
      "Iteration: 10950, Total Loss: 0.05721802, Physics Loss: 0.00794369, BC Loss: 0.04927433\n",
      "Iteration: 10960, Total Loss: 0.05699148, Physics Loss: 0.00775020, BC Loss: 0.04924128\n",
      "Iteration: 10970, Total Loss: 0.05670476, Physics Loss: 0.00766652, BC Loss: 0.04903825\n",
      "Iteration: 10980, Total Loss: 0.05647241, Physics Loss: 0.00748789, BC Loss: 0.04898451\n",
      "Iteration: 10990, Total Loss: 0.05631360, Physics Loss: 0.00745778, BC Loss: 0.04885582\n",
      "Iteration: 11000, Total Loss: 0.05606857, Physics Loss: 0.00727510, BC Loss: 0.04879347\n",
      "Iteration: 11010, Total Loss: 73.76973724, Physics Loss: 73.63795471, BC Loss: 0.13177951\n",
      "Iteration: 11020, Total Loss: 17.67427063, Physics Loss: 17.00347519, BC Loss: 0.67079484\n",
      "Iteration: 11030, Total Loss: 2.46138597, Physics Loss: 1.89266777, BC Loss: 0.56871819\n",
      "Iteration: 11040, Total Loss: 0.99331844, Physics Loss: 0.50434744, BC Loss: 0.48897099\n",
      "Iteration: 11050, Total Loss: 0.59506416, Physics Loss: 0.13229221, BC Loss: 0.46277192\n",
      "Iteration: 11060, Total Loss: 0.48632208, Physics Loss: 0.05952094, BC Loss: 0.42680115\n",
      "Iteration: 11070, Total Loss: 0.43346477, Physics Loss: 0.07641315, BC Loss: 0.35705161\n",
      "Iteration: 11080, Total Loss: 0.40123469, Physics Loss: 0.05409310, BC Loss: 0.34714159\n",
      "Iteration: 11090, Total Loss: 0.37732765, Physics Loss: 0.06340933, BC Loss: 0.31391832\n",
      "Iteration: 11100, Total Loss: 0.36074686, Physics Loss: 0.06503516, BC Loss: 0.29571170\n",
      "Iteration: 11110, Total Loss: 0.33939263, Physics Loss: 0.06960288, BC Loss: 0.26978976\n",
      "Iteration: 11120, Total Loss: 0.31892484, Physics Loss: 0.07093169, BC Loss: 0.24799317\n",
      "Iteration: 11130, Total Loss: 0.30798754, Physics Loss: 0.05826131, BC Loss: 0.24972624\n",
      "Iteration: 11140, Total Loss: 0.30190283, Physics Loss: 0.06565493, BC Loss: 0.23624790\n",
      "Iteration: 11150, Total Loss: 0.29445934, Physics Loss: 0.06419520, BC Loss: 0.23026416\n",
      "Iteration: 11160, Total Loss: 0.27904043, Physics Loss: 0.06968918, BC Loss: 0.20935124\n",
      "Iteration: 11170, Total Loss: 0.26109388, Physics Loss: 0.07014725, BC Loss: 0.19094664\n",
      "Iteration: 11180, Total Loss: 0.24540783, Physics Loss: 0.05075469, BC Loss: 0.19465314\n",
      "Iteration: 11190, Total Loss: 0.24101275, Physics Loss: 0.05520937, BC Loss: 0.18580338\n",
      "Iteration: 11200, Total Loss: 0.23311271, Physics Loss: 0.05582018, BC Loss: 0.17729253\n",
      "Iteration: 11210, Total Loss: 0.22555359, Physics Loss: 0.06221189, BC Loss: 0.16334170\n",
      "Iteration: 11220, Total Loss: 0.21826459, Physics Loss: 0.06101773, BC Loss: 0.15724686\n",
      "Iteration: 11230, Total Loss: 0.21324193, Physics Loss: 0.06254825, BC Loss: 0.15069368\n",
      "Iteration: 11240, Total Loss: 0.20590423, Physics Loss: 0.05387308, BC Loss: 0.15203115\n",
      "Iteration: 11250, Total Loss: 0.19741137, Physics Loss: 0.05581075, BC Loss: 0.14160062\n",
      "Iteration: 11260, Total Loss: 0.19208434, Physics Loss: 0.04935736, BC Loss: 0.14272699\n",
      "Iteration: 11270, Total Loss: 0.18177336, Physics Loss: 0.05442469, BC Loss: 0.12734868\n",
      "Iteration: 11280, Total Loss: 0.17665376, Physics Loss: 0.05943932, BC Loss: 0.11721443\n",
      "Iteration: 11290, Total Loss: 0.16199303, Physics Loss: 0.05697788, BC Loss: 0.10501514\n",
      "Iteration: 11300, Total Loss: 0.14532921, Physics Loss: 0.05075138, BC Loss: 0.09457783\n",
      "Iteration: 11310, Total Loss: 0.12534475, Physics Loss: 0.04843397, BC Loss: 0.07691079\n",
      "Iteration: 11320, Total Loss: 0.11101472, Physics Loss: 0.05331181, BC Loss: 0.05770291\n",
      "Iteration: 11330, Total Loss: 0.10066237, Physics Loss: 0.04956575, BC Loss: 0.05109662\n",
      "Iteration: 11340, Total Loss: 0.09175661, Physics Loss: 0.04849353, BC Loss: 0.04326307\n",
      "Iteration: 11350, Total Loss: 0.07905644, Physics Loss: 0.04096715, BC Loss: 0.03808929\n",
      "Iteration: 11360, Total Loss: 0.06785809, Physics Loss: 0.03437319, BC Loss: 0.03348490\n",
      "Iteration: 11370, Total Loss: 0.06174406, Physics Loss: 0.02964199, BC Loss: 0.03210207\n",
      "Iteration: 11380, Total Loss: 0.04945260, Physics Loss: 0.02389231, BC Loss: 0.02556029\n",
      "Iteration: 11390, Total Loss: 0.03544073, Physics Loss: 0.01465490, BC Loss: 0.02078584\n",
      "Iteration: 11400, Total Loss: 0.03114419, Physics Loss: 0.01247652, BC Loss: 0.01866767\n",
      "Iteration: 11410, Total Loss: 0.02632614, Physics Loss: 0.01036288, BC Loss: 0.01596326\n",
      "Iteration: 11420, Total Loss: 0.02290356, Physics Loss: 0.00734154, BC Loss: 0.01556202\n",
      "Iteration: 11430, Total Loss: 0.02120621, Physics Loss: 0.00676810, BC Loss: 0.01443811\n",
      "Iteration: 11440, Total Loss: 0.01856438, Physics Loss: 0.00492813, BC Loss: 0.01363626\n",
      "Iteration: 11450, Total Loss: 0.01691152, Physics Loss: 0.00330234, BC Loss: 0.01360918\n",
      "Iteration: 11460, Total Loss: 0.01574131, Physics Loss: 0.00236819, BC Loss: 0.01337312\n",
      "Iteration: 11470, Total Loss: 0.01486551, Physics Loss: 0.00162366, BC Loss: 0.01324185\n",
      "Iteration: 11480, Total Loss: 0.01424323, Physics Loss: 0.00104748, BC Loss: 0.01319574\n",
      "Iteration: 11490, Total Loss: 0.01390977, Physics Loss: 0.00075526, BC Loss: 0.01315451\n",
      "Iteration: 11500, Total Loss: 0.01379686, Physics Loss: 0.00064797, BC Loss: 0.01314889\n",
      "Iteration: 11510, Total Loss: 0.01370266, Physics Loss: 0.00055588, BC Loss: 0.01314677\n",
      "Iteration: 11520, Total Loss: 0.01357733, Physics Loss: 0.00046794, BC Loss: 0.01310939\n",
      "Iteration: 11530, Total Loss: 0.01342653, Physics Loss: 0.00032351, BC Loss: 0.01310302\n",
      "Iteration: 11540, Total Loss: 0.01338279, Physics Loss: 0.00029043, BC Loss: 0.01309236\n",
      "Iteration: 11550, Total Loss: 0.01335112, Physics Loss: 0.00027147, BC Loss: 0.01307965\n",
      "Iteration: 11560, Total Loss: 0.01328959, Physics Loss: 0.00023135, BC Loss: 0.01305824\n",
      "Iteration: 11570, Total Loss: 0.01323966, Physics Loss: 0.00019642, BC Loss: 0.01304324\n",
      "Iteration: 11580, Total Loss: 0.01319512, Physics Loss: 0.00016198, BC Loss: 0.01303314\n",
      "Iteration: 11590, Total Loss: 0.01316605, Physics Loss: 0.00014252, BC Loss: 0.01302353\n",
      "Iteration: 11600, Total Loss: 0.01315470, Physics Loss: 0.00013558, BC Loss: 0.01301911\n",
      "Iteration: 11610, Total Loss: 0.01314499, Physics Loss: 0.00012930, BC Loss: 0.01301570\n",
      "Iteration: 11620, Total Loss: 0.01313587, Physics Loss: 0.00012301, BC Loss: 0.01301286\n",
      "Iteration: 11630, Total Loss: 0.01311270, Physics Loss: 0.00010449, BC Loss: 0.01300821\n",
      "Iteration: 11640, Total Loss: 0.01309481, Physics Loss: 0.00009081, BC Loss: 0.01300400\n",
      "Iteration: 11650, Total Loss: 0.01308163, Physics Loss: 0.00008575, BC Loss: 0.01299588\n",
      "Iteration: 11660, Total Loss: 0.01305844, Physics Loss: 0.00007289, BC Loss: 0.01298555\n",
      "Iteration: 11670, Total Loss: 0.01304602, Physics Loss: 0.00006490, BC Loss: 0.01298112\n",
      "Iteration: 11680, Total Loss: 0.01304085, Physics Loss: 0.00006284, BC Loss: 0.01297801\n",
      "Iteration: 11690, Total Loss: 0.01303034, Physics Loss: 0.00005339, BC Loss: 0.01297695\n",
      "Iteration: 11700, Total Loss: 0.01301925, Physics Loss: 0.00004010, BC Loss: 0.01297915\n",
      "Iteration: 11710, Total Loss: 0.01301349, Physics Loss: 0.00003374, BC Loss: 0.01297974\n",
      "Iteration: 11720, Total Loss: 0.01301052, Physics Loss: 0.00003078, BC Loss: 0.01297974\n",
      "Iteration: 11730, Total Loss: 0.01300482, Physics Loss: 0.00002732, BC Loss: 0.01297750\n",
      "Iteration: 11740, Total Loss: 0.01299605, Physics Loss: 0.00002367, BC Loss: 0.01297238\n",
      "Iteration: 11750, Total Loss: 0.01299262, Physics Loss: 0.00002374, BC Loss: 0.01296888\n",
      "Iteration: 11760, Total Loss: 0.01299047, Physics Loss: 0.00002459, BC Loss: 0.01296588\n",
      "Iteration: 11770, Total Loss: 0.01298839, Physics Loss: 0.00002473, BC Loss: 0.01296366\n",
      "Iteration: 11780, Total Loss: 0.01298733, Physics Loss: 0.00002434, BC Loss: 0.01296299\n",
      "Iteration: 11790, Total Loss: 0.01298550, Physics Loss: 0.00002387, BC Loss: 0.01296162\n",
      "Iteration: 11800, Total Loss: 0.01298235, Physics Loss: 0.00002126, BC Loss: 0.01296109\n",
      "Iteration: 11810, Total Loss: 0.01297997, Physics Loss: 0.00001781, BC Loss: 0.01296216\n",
      "Iteration: 11820, Total Loss: 0.01297872, Physics Loss: 0.00001593, BC Loss: 0.01296279\n",
      "Iteration: 11830, Total Loss: 0.01297657, Physics Loss: 0.00001370, BC Loss: 0.01296287\n",
      "Iteration: 11840, Total Loss: 0.01297367, Physics Loss: 0.00001246, BC Loss: 0.01296121\n",
      "Iteration: 11850, Total Loss: 0.01297162, Physics Loss: 0.00001235, BC Loss: 0.01295927\n",
      "Iteration: 11860, Total Loss: 0.01297019, Physics Loss: 0.00001406, BC Loss: 0.01295613\n",
      "Iteration: 11870, Total Loss: 0.01296815, Physics Loss: 0.00001646, BC Loss: 0.01295169\n",
      "Iteration: 11880, Total Loss: 0.01296644, Physics Loss: 0.00001719, BC Loss: 0.01294925\n",
      "Iteration: 11890, Total Loss: 0.01296594, Physics Loss: 0.00001810, BC Loss: 0.01294784\n",
      "Iteration: 11900, Total Loss: 0.01296521, Physics Loss: 0.00001932, BC Loss: 0.01294589\n",
      "Iteration: 11910, Total Loss: 0.01296278, Physics Loss: 0.00002197, BC Loss: 0.01294081\n",
      "Iteration: 11920, Total Loss: 0.01296106, Physics Loss: 0.00002305, BC Loss: 0.01293801\n",
      "Iteration: 11930, Total Loss: 0.01296010, Physics Loss: 0.00002399, BC Loss: 0.01293612\n",
      "Iteration: 11940, Total Loss: 0.01295840, Physics Loss: 0.00002384, BC Loss: 0.01293456\n",
      "Iteration: 11950, Total Loss: 0.01295410, Physics Loss: 0.00001962, BC Loss: 0.01293449\n",
      "Iteration: 11960, Total Loss: 0.01295079, Physics Loss: 0.00001566, BC Loss: 0.01293513\n",
      "Iteration: 11970, Total Loss: 0.01294839, Physics Loss: 0.00001895, BC Loss: 0.01292944\n",
      "Iteration: 11980, Total Loss: 0.01293838, Physics Loss: 0.00004976, BC Loss: 0.01288862\n",
      "Iteration: 11990, Total Loss: 0.01288299, Physics Loss: 0.00083107, BC Loss: 0.01205192\n",
      "Iteration: 12000, Total Loss: 0.01203385, Physics Loss: 0.00046174, BC Loss: 0.01157210\n",
      "Iteration: 12010, Total Loss: 950914688.00000000, Physics Loss: 950427904.00000000, BC Loss: 486777.31250000\n",
      "Iteration: 12020, Total Loss: 14043.44531250, Physics Loss: 13478.91210938, BC Loss: 564.53363037\n",
      "Iteration: 12030, Total Loss: 2.09265924, Physics Loss: 0.20506157, BC Loss: 1.88759756\n",
      "Iteration: 12040, Total Loss: 0.84330869, Physics Loss: 0.12403482, BC Loss: 0.71927387\n",
      "Iteration: 12050, Total Loss: 0.64457303, Physics Loss: 0.04439080, BC Loss: 0.60018224\n",
      "Iteration: 12060, Total Loss: 0.61920106, Physics Loss: 0.03604458, BC Loss: 0.58315647\n",
      "Iteration: 12070, Total Loss: 0.59433687, Physics Loss: 0.05838386, BC Loss: 0.53595299\n",
      "Iteration: 12080, Total Loss: 0.56313372, Physics Loss: 0.12412284, BC Loss: 0.43901086\n",
      "Iteration: 12090, Total Loss: 0.56244564, Physics Loss: 0.11805750, BC Loss: 0.44438812\n",
      "Iteration: 12100, Total Loss: 0.55196810, Physics Loss: 0.09725349, BC Loss: 0.45471460\n",
      "Iteration: 12110, Total Loss: 0.54550099, Physics Loss: 0.08682940, BC Loss: 0.45867160\n",
      "Iteration: 12120, Total Loss: 0.53327066, Physics Loss: 0.11058084, BC Loss: 0.42268983\n",
      "Iteration: 12130, Total Loss: 0.52404076, Physics Loss: 0.09313332, BC Loss: 0.43090743\n",
      "Iteration: 12140, Total Loss: 0.52192342, Physics Loss: 0.11196494, BC Loss: 0.40995848\n",
      "Iteration: 12150, Total Loss: 0.52104717, Physics Loss: 0.10826407, BC Loss: 0.41278312\n",
      "Iteration: 12160, Total Loss: 0.51995325, Physics Loss: 0.10509738, BC Loss: 0.41485590\n",
      "Iteration: 12170, Total Loss: 0.51842237, Physics Loss: 0.10584748, BC Loss: 0.41257486\n",
      "Iteration: 12180, Total Loss: 0.51761079, Physics Loss: 0.10516938, BC Loss: 0.41244143\n",
      "Iteration: 12190, Total Loss: 0.51429796, Physics Loss: 0.10275622, BC Loss: 0.41154176\n",
      "Iteration: 12200, Total Loss: 0.51280969, Physics Loss: 0.09805354, BC Loss: 0.41475618\n",
      "Iteration: 12210, Total Loss: 0.50928700, Physics Loss: 0.09693332, BC Loss: 0.41235366\n",
      "Iteration: 12220, Total Loss: 0.50554514, Physics Loss: 0.09473088, BC Loss: 0.41081429\n",
      "Iteration: 12230, Total Loss: 0.50109291, Physics Loss: 0.08994810, BC Loss: 0.41114479\n",
      "Iteration: 12240, Total Loss: 0.49438268, Physics Loss: 0.09998865, BC Loss: 0.39439404\n",
      "Iteration: 12250, Total Loss: 0.48353726, Physics Loss: 0.08920878, BC Loss: 0.39432847\n",
      "Iteration: 12260, Total Loss: 0.46801832, Physics Loss: 0.09975744, BC Loss: 0.36826089\n",
      "Iteration: 12270, Total Loss: 0.43572465, Physics Loss: 0.07576705, BC Loss: 0.35995761\n",
      "Iteration: 12280, Total Loss: 0.37159073, Physics Loss: 0.05514688, BC Loss: 0.31644386\n",
      "Iteration: 12290, Total Loss: 0.33869541, Physics Loss: 0.08446027, BC Loss: 0.25423515\n",
      "Iteration: 12300, Total Loss: 0.31797969, Physics Loss: 0.05603540, BC Loss: 0.26194429\n",
      "Iteration: 12310, Total Loss: 0.29699868, Physics Loss: 0.05110946, BC Loss: 0.24588922\n",
      "Iteration: 12320, Total Loss: 0.28424075, Physics Loss: 0.04621403, BC Loss: 0.23802672\n",
      "Iteration: 12330, Total Loss: 0.27299404, Physics Loss: 0.04123184, BC Loss: 0.23176222\n",
      "Iteration: 12340, Total Loss: 0.26590049, Physics Loss: 0.04071818, BC Loss: 0.22518231\n",
      "Iteration: 12350, Total Loss: 0.26149207, Physics Loss: 0.03985487, BC Loss: 0.22163719\n",
      "Iteration: 12360, Total Loss: 0.25656864, Physics Loss: 0.03927944, BC Loss: 0.21728921\n",
      "Iteration: 12370, Total Loss: 0.24987739, Physics Loss: 0.03581469, BC Loss: 0.21406271\n",
      "Iteration: 12380, Total Loss: 0.24711291, Physics Loss: 0.03417329, BC Loss: 0.21293962\n",
      "Iteration: 12390, Total Loss: 0.24069795, Physics Loss: 0.03089121, BC Loss: 0.20980674\n",
      "Iteration: 12400, Total Loss: 0.23689552, Physics Loss: 0.02583199, BC Loss: 0.21106353\n",
      "Iteration: 12410, Total Loss: 0.23287648, Physics Loss: 0.02818801, BC Loss: 0.20468846\n",
      "Iteration: 12420, Total Loss: 0.22959949, Physics Loss: 0.02679792, BC Loss: 0.20280157\n",
      "Iteration: 12430, Total Loss: 0.22876604, Physics Loss: 0.02577375, BC Loss: 0.20299229\n",
      "Iteration: 12440, Total Loss: 0.22732635, Physics Loss: 0.02578382, BC Loss: 0.20154253\n",
      "Iteration: 12450, Total Loss: 0.22497465, Physics Loss: 0.02180116, BC Loss: 0.20317349\n",
      "Iteration: 12460, Total Loss: 0.22421819, Physics Loss: 0.02227964, BC Loss: 0.20193855\n",
      "Iteration: 12470, Total Loss: 0.22200668, Physics Loss: 0.02341973, BC Loss: 0.19858696\n",
      "Iteration: 12480, Total Loss: 0.22051336, Physics Loss: 0.02315482, BC Loss: 0.19735853\n",
      "Iteration: 12490, Total Loss: 0.21897501, Physics Loss: 0.02361521, BC Loss: 0.19535980\n",
      "Iteration: 12500, Total Loss: 0.21755502, Physics Loss: 0.02315616, BC Loss: 0.19439887\n",
      "Iteration: 12510, Total Loss: 0.21476053, Physics Loss: 0.02624937, BC Loss: 0.18851116\n",
      "Iteration: 12520, Total Loss: 0.21067634, Physics Loss: 0.02788702, BC Loss: 0.18278933\n",
      "Iteration: 12530, Total Loss: 0.20842439, Physics Loss: 0.02597104, BC Loss: 0.18245335\n",
      "Iteration: 12540, Total Loss: 0.20652501, Physics Loss: 0.02529480, BC Loss: 0.18123022\n",
      "Iteration: 12550, Total Loss: 0.20425338, Physics Loss: 0.02552172, BC Loss: 0.17873165\n",
      "Iteration: 12560, Total Loss: 0.20153596, Physics Loss: 0.02669886, BC Loss: 0.17483710\n",
      "Iteration: 12570, Total Loss: 0.19722460, Physics Loss: 0.02641127, BC Loss: 0.17081332\n",
      "Iteration: 12580, Total Loss: 0.19329056, Physics Loss: 0.02827436, BC Loss: 0.16501620\n",
      "Iteration: 12590, Total Loss: 0.19004035, Physics Loss: 0.02861130, BC Loss: 0.16142905\n",
      "Iteration: 12600, Total Loss: 0.18743402, Physics Loss: 0.02793679, BC Loss: 0.15949723\n",
      "Iteration: 12610, Total Loss: 0.18538436, Physics Loss: 0.02723750, BC Loss: 0.15814686\n",
      "Iteration: 12620, Total Loss: 0.18337674, Physics Loss: 0.02782436, BC Loss: 0.15555239\n",
      "Iteration: 12630, Total Loss: 0.17943349, Physics Loss: 0.03065483, BC Loss: 0.14877866\n",
      "Iteration: 12640, Total Loss: 0.17729837, Physics Loss: 0.02947048, BC Loss: 0.14782789\n",
      "Iteration: 12650, Total Loss: 0.17343713, Physics Loss: 0.02935209, BC Loss: 0.14408505\n",
      "Iteration: 12660, Total Loss: 0.17118552, Physics Loss: 0.02975040, BC Loss: 0.14143513\n",
      "Iteration: 12670, Total Loss: 0.16812783, Physics Loss: 0.03125499, BC Loss: 0.13687284\n",
      "Iteration: 12680, Total Loss: 0.16501869, Physics Loss: 0.02919890, BC Loss: 0.13581979\n",
      "Iteration: 12690, Total Loss: 0.16079961, Physics Loss: 0.03201211, BC Loss: 0.12878750\n",
      "Iteration: 12700, Total Loss: 0.15811294, Physics Loss: 0.03044528, BC Loss: 0.12766767\n",
      "Iteration: 12710, Total Loss: 0.15571994, Physics Loss: 0.02651379, BC Loss: 0.12920615\n",
      "Iteration: 12720, Total Loss: 0.15089455, Physics Loss: 0.02639571, BC Loss: 0.12449884\n",
      "Iteration: 12730, Total Loss: 0.14785303, Physics Loss: 0.02840826, BC Loss: 0.11944477\n",
      "Iteration: 12740, Total Loss: 0.14467183, Physics Loss: 0.02936192, BC Loss: 0.11530991\n",
      "Iteration: 12750, Total Loss: 0.14319068, Physics Loss: 0.02949065, BC Loss: 0.11370003\n",
      "Iteration: 12760, Total Loss: 0.14137672, Physics Loss: 0.02868941, BC Loss: 0.11268731\n",
      "Iteration: 12770, Total Loss: 0.13991572, Physics Loss: 0.02724560, BC Loss: 0.11267012\n",
      "Iteration: 12780, Total Loss: 0.13852096, Physics Loss: 0.02481765, BC Loss: 0.11370331\n",
      "Iteration: 12790, Total Loss: 0.13729323, Physics Loss: 0.02683591, BC Loss: 0.11045732\n",
      "Iteration: 12800, Total Loss: 0.13568151, Physics Loss: 0.02644464, BC Loss: 0.10923687\n",
      "Iteration: 12810, Total Loss: 0.13455305, Physics Loss: 0.02495079, BC Loss: 0.10960226\n",
      "Iteration: 12820, Total Loss: 0.13270542, Physics Loss: 0.02282977, BC Loss: 0.10987564\n",
      "Iteration: 12830, Total Loss: 0.13145873, Physics Loss: 0.02223857, BC Loss: 0.10922016\n",
      "Iteration: 12840, Total Loss: 0.12999196, Physics Loss: 0.02182100, BC Loss: 0.10817096\n",
      "Iteration: 12850, Total Loss: 0.12834966, Physics Loss: 0.02138500, BC Loss: 0.10696466\n",
      "Iteration: 12860, Total Loss: 0.12721963, Physics Loss: 0.02140208, BC Loss: 0.10581756\n",
      "Iteration: 12870, Total Loss: 0.12640122, Physics Loss: 0.02236926, BC Loss: 0.10403195\n",
      "Iteration: 12880, Total Loss: 0.12538083, Physics Loss: 0.02251346, BC Loss: 0.10286737\n",
      "Iteration: 12890, Total Loss: 0.12434296, Physics Loss: 0.02177840, BC Loss: 0.10256456\n",
      "Iteration: 12900, Total Loss: 0.12302308, Physics Loss: 0.02010023, BC Loss: 0.10292285\n",
      "Iteration: 12910, Total Loss: 0.12246698, Physics Loss: 0.02005527, BC Loss: 0.10241171\n",
      "Iteration: 12920, Total Loss: 0.12141360, Physics Loss: 0.02071323, BC Loss: 0.10070037\n",
      "Iteration: 12930, Total Loss: 0.12012491, Physics Loss: 0.01888552, BC Loss: 0.10123939\n",
      "Iteration: 12940, Total Loss: 0.11953307, Physics Loss: 0.01817440, BC Loss: 0.10135867\n",
      "Iteration: 12950, Total Loss: 0.11936229, Physics Loss: 0.01846508, BC Loss: 0.10089721\n",
      "Iteration: 12960, Total Loss: 0.11919990, Physics Loss: 0.01781066, BC Loss: 0.10138924\n",
      "Iteration: 12970, Total Loss: 0.11859258, Physics Loss: 0.01759981, BC Loss: 0.10099278\n",
      "Iteration: 12980, Total Loss: 0.11837585, Physics Loss: 0.01811983, BC Loss: 0.10025602\n",
      "Iteration: 12990, Total Loss: 0.11802547, Physics Loss: 0.01783849, BC Loss: 0.10018699\n",
      "Iteration: 13000, Total Loss: 0.11775856, Physics Loss: 0.01768871, BC Loss: 0.10006985\n",
      "Iteration: 13010, Total Loss: 30.14928627, Physics Loss: 30.02529907, BC Loss: 0.12398799\n",
      "Iteration: 13020, Total Loss: 1.50040162, Physics Loss: 0.01644356, BC Loss: 1.48395801\n",
      "Iteration: 13030, Total Loss: 0.66538978, Physics Loss: 0.00644681, BC Loss: 0.65894300\n",
      "Iteration: 13040, Total Loss: 0.66347587, Physics Loss: 0.00236200, BC Loss: 0.66111386\n",
      "Iteration: 13050, Total Loss: 0.67178363, Physics Loss: 0.00579621, BC Loss: 0.66598743\n",
      "Iteration: 13060, Total Loss: 0.63094908, Physics Loss: 0.03612462, BC Loss: 0.59482443\n",
      "Iteration: 13070, Total Loss: 0.60534400, Physics Loss: 0.04680099, BC Loss: 0.55854303\n",
      "Iteration: 13080, Total Loss: 0.56847060, Physics Loss: 0.07425819, BC Loss: 0.49421239\n",
      "Iteration: 13090, Total Loss: 0.54412150, Physics Loss: 0.08693810, BC Loss: 0.45718339\n",
      "Iteration: 13100, Total Loss: 0.50596935, Physics Loss: 0.10158875, BC Loss: 0.40438062\n",
      "Iteration: 13110, Total Loss: 0.44462231, Physics Loss: 0.05065916, BC Loss: 0.39396316\n",
      "Iteration: 13120, Total Loss: 0.43266094, Physics Loss: 0.05693853, BC Loss: 0.37572241\n",
      "Iteration: 13130, Total Loss: 0.41555554, Physics Loss: 0.04089837, BC Loss: 0.37465715\n",
      "Iteration: 13140, Total Loss: 0.40257901, Physics Loss: 0.03215372, BC Loss: 0.37042528\n",
      "Iteration: 13150, Total Loss: 0.39732409, Physics Loss: 0.03259136, BC Loss: 0.36473274\n",
      "Iteration: 13160, Total Loss: 0.39200574, Physics Loss: 0.02100330, BC Loss: 0.37100244\n",
      "Iteration: 13170, Total Loss: 0.39104152, Physics Loss: 0.02105318, BC Loss: 0.36998832\n",
      "Iteration: 13180, Total Loss: 0.38965520, Physics Loss: 0.02494327, BC Loss: 0.36471194\n",
      "Iteration: 13190, Total Loss: 0.38788843, Physics Loss: 0.02125115, BC Loss: 0.36663729\n",
      "Iteration: 13200, Total Loss: 0.38628766, Physics Loss: 0.02356461, BC Loss: 0.36272305\n",
      "Iteration: 13210, Total Loss: 0.38468009, Physics Loss: 0.02222887, BC Loss: 0.36245123\n",
      "Iteration: 13220, Total Loss: 0.38408393, Physics Loss: 0.02248651, BC Loss: 0.36159742\n",
      "Iteration: 13230, Total Loss: 0.38369834, Physics Loss: 0.02164121, BC Loss: 0.36205715\n",
      "Iteration: 13240, Total Loss: 0.38242814, Physics Loss: 0.02110218, BC Loss: 0.36132595\n",
      "Iteration: 13250, Total Loss: 0.38150382, Physics Loss: 0.02484055, BC Loss: 0.35666329\n",
      "Iteration: 13260, Total Loss: 0.37966231, Physics Loss: 0.02743823, BC Loss: 0.35222408\n",
      "Iteration: 13270, Total Loss: 0.37757373, Physics Loss: 0.02690876, BC Loss: 0.35066497\n",
      "Iteration: 13280, Total Loss: 0.37546682, Physics Loss: 0.03254843, BC Loss: 0.34291840\n",
      "Iteration: 13290, Total Loss: 0.37176585, Physics Loss: 0.02566201, BC Loss: 0.34610385\n",
      "Iteration: 13300, Total Loss: 0.36963651, Physics Loss: 0.02266468, BC Loss: 0.34697184\n",
      "Iteration: 13310, Total Loss: 0.36422342, Physics Loss: 0.02816104, BC Loss: 0.33606237\n",
      "Iteration: 13320, Total Loss: 0.35808921, Physics Loss: 0.03527980, BC Loss: 0.32280940\n",
      "Iteration: 13330, Total Loss: 0.35069418, Physics Loss: 0.04330838, BC Loss: 0.30738580\n",
      "Iteration: 13340, Total Loss: 0.34414053, Physics Loss: 0.04297139, BC Loss: 0.30116913\n",
      "Iteration: 13350, Total Loss: 0.34112370, Physics Loss: 0.04147789, BC Loss: 0.29964581\n",
      "Iteration: 13360, Total Loss: 0.33895612, Physics Loss: 0.04608892, BC Loss: 0.29286718\n",
      "Iteration: 13370, Total Loss: 0.33442676, Physics Loss: 0.04947108, BC Loss: 0.28495568\n",
      "Iteration: 13380, Total Loss: 0.32493731, Physics Loss: 0.04415171, BC Loss: 0.28078559\n",
      "Iteration: 13390, Total Loss: 0.31855005, Physics Loss: 0.04646250, BC Loss: 0.27208754\n",
      "Iteration: 13400, Total Loss: 0.31180382, Physics Loss: 0.05570317, BC Loss: 0.25610065\n",
      "Iteration: 13410, Total Loss: 0.29686362, Physics Loss: 0.04347210, BC Loss: 0.25339150\n",
      "Iteration: 13420, Total Loss: 0.28741872, Physics Loss: 0.03984732, BC Loss: 0.24757141\n",
      "Iteration: 13430, Total Loss: 0.28212404, Physics Loss: 0.03774885, BC Loss: 0.24437520\n",
      "Iteration: 13440, Total Loss: 0.27869138, Physics Loss: 0.04183165, BC Loss: 0.23685974\n",
      "Iteration: 13450, Total Loss: 0.27287716, Physics Loss: 0.03550301, BC Loss: 0.23737416\n",
      "Iteration: 13460, Total Loss: 0.26914051, Physics Loss: 0.03409251, BC Loss: 0.23504801\n",
      "Iteration: 13470, Total Loss: 0.26714715, Physics Loss: 0.03687837, BC Loss: 0.23026878\n",
      "Iteration: 13480, Total Loss: 0.26030219, Physics Loss: 0.03475302, BC Loss: 0.22554916\n",
      "Iteration: 13490, Total Loss: 0.25716737, Physics Loss: 0.02930142, BC Loss: 0.22786595\n",
      "Iteration: 13500, Total Loss: 0.25479248, Physics Loss: 0.02907404, BC Loss: 0.22571844\n",
      "Iteration: 13510, Total Loss: 0.25301799, Physics Loss: 0.02660099, BC Loss: 0.22641699\n",
      "Iteration: 13520, Total Loss: 0.25014946, Physics Loss: 0.02670567, BC Loss: 0.22344378\n",
      "Iteration: 13530, Total Loss: 0.24835500, Physics Loss: 0.02730089, BC Loss: 0.22105411\n",
      "Iteration: 13540, Total Loss: 0.24693634, Physics Loss: 0.02646874, BC Loss: 0.22046760\n",
      "Iteration: 13550, Total Loss: 0.24551778, Physics Loss: 0.02647309, BC Loss: 0.21904469\n",
      "Iteration: 13560, Total Loss: 0.24458833, Physics Loss: 0.02663244, BC Loss: 0.21795589\n",
      "Iteration: 13570, Total Loss: 0.24353491, Physics Loss: 0.02537133, BC Loss: 0.21816358\n",
      "Iteration: 13580, Total Loss: 0.24263920, Physics Loss: 0.02551885, BC Loss: 0.21712035\n",
      "Iteration: 13590, Total Loss: 0.24158512, Physics Loss: 0.02460180, BC Loss: 0.21698332\n",
      "Iteration: 13600, Total Loss: 0.24097636, Physics Loss: 0.02627023, BC Loss: 0.21470612\n",
      "Iteration: 13610, Total Loss: 0.23964378, Physics Loss: 0.02279124, BC Loss: 0.21685255\n",
      "Iteration: 13620, Total Loss: 0.23857599, Physics Loss: 0.02287883, BC Loss: 0.21569717\n",
      "Iteration: 13630, Total Loss: 0.23756492, Physics Loss: 0.02218444, BC Loss: 0.21538049\n",
      "Iteration: 13640, Total Loss: 0.23394927, Physics Loss: 0.02108201, BC Loss: 0.21286726\n",
      "Iteration: 13650, Total Loss: 0.23243950, Physics Loss: 0.02266435, BC Loss: 0.20977515\n",
      "Iteration: 13660, Total Loss: 0.22938761, Physics Loss: 0.02398345, BC Loss: 0.20540416\n",
      "Iteration: 13670, Total Loss: 0.22140640, Physics Loss: 0.02125465, BC Loss: 0.20015174\n",
      "Iteration: 13680, Total Loss: 0.21775416, Physics Loss: 0.01910353, BC Loss: 0.19865063\n",
      "Iteration: 13690, Total Loss: 0.21574126, Physics Loss: 0.01982572, BC Loss: 0.19591554\n",
      "Iteration: 13700, Total Loss: 0.21376964, Physics Loss: 0.02226213, BC Loss: 0.19150752\n",
      "Iteration: 13710, Total Loss: 0.21154520, Physics Loss: 0.02024274, BC Loss: 0.19130245\n",
      "Iteration: 13720, Total Loss: 0.21124680, Physics Loss: 0.02026648, BC Loss: 0.19098033\n",
      "Iteration: 13730, Total Loss: 0.21045013, Physics Loss: 0.01914261, BC Loss: 0.19130751\n",
      "Iteration: 13740, Total Loss: 0.20929343, Physics Loss: 0.01798083, BC Loss: 0.19131260\n",
      "Iteration: 13750, Total Loss: 0.20857930, Physics Loss: 0.01702446, BC Loss: 0.19155484\n",
      "Iteration: 13760, Total Loss: 0.20807189, Physics Loss: 0.01713124, BC Loss: 0.19094065\n",
      "Iteration: 13770, Total Loss: 0.20741449, Physics Loss: 0.01690309, BC Loss: 0.19051141\n",
      "Iteration: 13780, Total Loss: 0.20622543, Physics Loss: 0.01494472, BC Loss: 0.19128071\n",
      "Iteration: 13790, Total Loss: 0.20429666, Physics Loss: 0.01526384, BC Loss: 0.18903282\n",
      "Iteration: 13800, Total Loss: 0.20314999, Physics Loss: 0.01612787, BC Loss: 0.18702212\n",
      "Iteration: 13810, Total Loss: 0.20231450, Physics Loss: 0.01635468, BC Loss: 0.18595982\n",
      "Iteration: 13820, Total Loss: 0.20137390, Physics Loss: 0.01706094, BC Loss: 0.18431295\n",
      "Iteration: 13830, Total Loss: 0.20082834, Physics Loss: 0.01730170, BC Loss: 0.18352664\n",
      "Iteration: 13840, Total Loss: 0.20031415, Physics Loss: 0.01779126, BC Loss: 0.18252289\n",
      "Iteration: 13850, Total Loss: 0.19955701, Physics Loss: 0.02015337, BC Loss: 0.17940363\n",
      "Iteration: 13860, Total Loss: 0.19927482, Physics Loss: 0.02115344, BC Loss: 0.17812139\n",
      "Iteration: 13870, Total Loss: 0.19885352, Physics Loss: 0.02165202, BC Loss: 0.17720151\n",
      "Iteration: 13880, Total Loss: 0.19808570, Physics Loss: 0.02167261, BC Loss: 0.17641309\n",
      "Iteration: 13890, Total Loss: 0.19753139, Physics Loss: 0.02114166, BC Loss: 0.17638972\n",
      "Iteration: 13900, Total Loss: 0.19695288, Physics Loss: 0.01935531, BC Loss: 0.17759757\n",
      "Iteration: 13910, Total Loss: 0.19652501, Physics Loss: 0.01863814, BC Loss: 0.17788687\n",
      "Iteration: 13920, Total Loss: 0.19579187, Physics Loss: 0.01911652, BC Loss: 0.17667535\n",
      "Iteration: 13930, Total Loss: 0.19482817, Physics Loss: 0.01921540, BC Loss: 0.17561276\n",
      "Iteration: 13940, Total Loss: 0.19383046, Physics Loss: 0.01697479, BC Loss: 0.17685567\n",
      "Iteration: 13950, Total Loss: 0.19329439, Physics Loss: 0.01719798, BC Loss: 0.17609641\n",
      "Iteration: 13960, Total Loss: 0.19293068, Physics Loss: 0.01636493, BC Loss: 0.17656575\n",
      "Iteration: 13970, Total Loss: 0.19236991, Physics Loss: 0.01664711, BC Loss: 0.17572281\n",
      "Iteration: 13980, Total Loss: 0.19134265, Physics Loss: 0.01649042, BC Loss: 0.17485224\n",
      "Iteration: 13990, Total Loss: 0.19075635, Physics Loss: 0.01586470, BC Loss: 0.17489165\n",
      "Iteration: 14000, Total Loss: 0.19051917, Physics Loss: 0.01609099, BC Loss: 0.17442818\n",
      "Iteration: 14010, Total Loss: 0.19772348, Physics Loss: 0.01665804, BC Loss: 0.18106544\n",
      "Iteration: 14020, Total Loss: 0.19543055, Physics Loss: 0.01851021, BC Loss: 0.17692034\n",
      "Iteration: 14030, Total Loss: 0.19264816, Physics Loss: 0.02133982, BC Loss: 0.17130834\n",
      "Iteration: 14040, Total Loss: 0.19037880, Physics Loss: 0.01907040, BC Loss: 0.17130840\n",
      "Iteration: 14050, Total Loss: 0.18977140, Physics Loss: 0.01955490, BC Loss: 0.17021650\n",
      "Iteration: 14060, Total Loss: 0.18912432, Physics Loss: 0.02006775, BC Loss: 0.16905656\n",
      "Iteration: 14070, Total Loss: 0.18854369, Physics Loss: 0.01918676, BC Loss: 0.16935693\n",
      "Iteration: 14080, Total Loss: 0.18782112, Physics Loss: 0.01863766, BC Loss: 0.16918345\n",
      "Iteration: 14090, Total Loss: 0.18763442, Physics Loss: 0.01839459, BC Loss: 0.16923983\n",
      "Iteration: 14100, Total Loss: 0.18692218, Physics Loss: 0.01778073, BC Loss: 0.16914144\n",
      "Iteration: 14110, Total Loss: 0.18546547, Physics Loss: 0.01748069, BC Loss: 0.16798478\n",
      "Iteration: 14120, Total Loss: 0.18421438, Physics Loss: 0.01763711, BC Loss: 0.16657728\n",
      "Iteration: 14130, Total Loss: 0.18340147, Physics Loss: 0.01775226, BC Loss: 0.16564921\n",
      "Iteration: 14140, Total Loss: 0.18275946, Physics Loss: 0.01818318, BC Loss: 0.16457628\n",
      "Iteration: 14150, Total Loss: 0.18206595, Physics Loss: 0.01825449, BC Loss: 0.16381146\n",
      "Iteration: 14160, Total Loss: 0.18165968, Physics Loss: 0.01884352, BC Loss: 0.16281617\n",
      "Iteration: 14170, Total Loss: 0.18135132, Physics Loss: 0.01938990, BC Loss: 0.16196142\n",
      "Iteration: 14180, Total Loss: 0.18121868, Physics Loss: 0.01896912, BC Loss: 0.16224957\n",
      "Iteration: 14190, Total Loss: 0.18113132, Physics Loss: 0.01908123, BC Loss: 0.16205008\n",
      "Iteration: 14200, Total Loss: 0.18103211, Physics Loss: 0.01934803, BC Loss: 0.16168408\n",
      "Iteration: 14210, Total Loss: 0.18079817, Physics Loss: 0.01974596, BC Loss: 0.16105221\n",
      "Iteration: 14220, Total Loss: 0.18061082, Physics Loss: 0.01970772, BC Loss: 0.16090310\n",
      "Iteration: 14230, Total Loss: 0.18032852, Physics Loss: 0.01924761, BC Loss: 0.16108090\n",
      "Iteration: 14240, Total Loss: 0.18007663, Physics Loss: 0.01896857, BC Loss: 0.16110806\n",
      "Iteration: 14250, Total Loss: 0.17994581, Physics Loss: 0.01927226, BC Loss: 0.16067356\n",
      "Iteration: 14260, Total Loss: 0.17978002, Physics Loss: 0.01948342, BC Loss: 0.16029660\n",
      "Iteration: 14270, Total Loss: 0.17958194, Physics Loss: 0.01966792, BC Loss: 0.15991402\n",
      "Iteration: 14280, Total Loss: 0.17940885, Physics Loss: 0.01976213, BC Loss: 0.15964672\n",
      "Iteration: 14290, Total Loss: 0.17918387, Physics Loss: 0.01958638, BC Loss: 0.15959749\n",
      "Iteration: 14300, Total Loss: 0.17901400, Physics Loss: 0.01932567, BC Loss: 0.15968834\n",
      "Iteration: 14310, Total Loss: 0.17894177, Physics Loss: 0.01896848, BC Loss: 0.15997329\n",
      "Iteration: 14320, Total Loss: 0.17882431, Physics Loss: 0.01890046, BC Loss: 0.15992385\n",
      "Iteration: 14330, Total Loss: 0.17853697, Physics Loss: 0.01857705, BC Loss: 0.15995991\n",
      "Iteration: 14340, Total Loss: 0.17821112, Physics Loss: 0.01798347, BC Loss: 0.16022766\n",
      "Iteration: 14350, Total Loss: 0.17799211, Physics Loss: 0.01828099, BC Loss: 0.15971112\n",
      "Iteration: 14360, Total Loss: 0.17781004, Physics Loss: 0.01871466, BC Loss: 0.15909538\n",
      "Iteration: 14370, Total Loss: 0.17774589, Physics Loss: 0.01873373, BC Loss: 0.15901217\n",
      "Iteration: 14380, Total Loss: 0.17764302, Physics Loss: 0.01884629, BC Loss: 0.15879673\n",
      "Iteration: 14390, Total Loss: 0.17747289, Physics Loss: 0.01872408, BC Loss: 0.15874881\n",
      "Iteration: 14400, Total Loss: 0.17732021, Physics Loss: 0.01855242, BC Loss: 0.15876779\n",
      "Iteration: 14410, Total Loss: 0.17723821, Physics Loss: 0.01854684, BC Loss: 0.15869138\n",
      "Iteration: 14420, Total Loss: 0.17709625, Physics Loss: 0.01836398, BC Loss: 0.15873227\n",
      "Iteration: 14430, Total Loss: 0.17698702, Physics Loss: 0.01847192, BC Loss: 0.15851510\n",
      "Iteration: 14440, Total Loss: 0.17693046, Physics Loss: 0.01826522, BC Loss: 0.15866524\n",
      "Iteration: 14450, Total Loss: 0.17685920, Physics Loss: 0.01847708, BC Loss: 0.15838212\n",
      "Iteration: 14460, Total Loss: 0.17679098, Physics Loss: 0.01896280, BC Loss: 0.15782818\n",
      "Iteration: 14470, Total Loss: 0.17663896, Physics Loss: 0.01849990, BC Loss: 0.15813906\n",
      "Iteration: 14480, Total Loss: 0.17650016, Physics Loss: 0.01837071, BC Loss: 0.15812944\n",
      "Iteration: 14490, Total Loss: 0.17638516, Physics Loss: 0.01918041, BC Loss: 0.15720475\n",
      "Iteration: 14500, Total Loss: 0.17629230, Physics Loss: 0.01917062, BC Loss: 0.15712167\n",
      "Iteration: 14510, Total Loss: 0.17615579, Physics Loss: 0.01896690, BC Loss: 0.15718889\n",
      "Iteration: 14520, Total Loss: 0.17605308, Physics Loss: 0.01904573, BC Loss: 0.15700735\n",
      "Iteration: 14530, Total Loss: 0.17600131, Physics Loss: 0.01871767, BC Loss: 0.15728365\n",
      "Iteration: 14540, Total Loss: 0.17592521, Physics Loss: 0.01847925, BC Loss: 0.15744595\n",
      "Iteration: 14550, Total Loss: 0.17573895, Physics Loss: 0.01826762, BC Loss: 0.15747133\n",
      "Iteration: 14560, Total Loss: 0.17543592, Physics Loss: 0.01809727, BC Loss: 0.15733865\n",
      "Iteration: 14570, Total Loss: 0.17534404, Physics Loss: 0.01802842, BC Loss: 0.15731561\n",
      "Iteration: 14580, Total Loss: 0.17526373, Physics Loss: 0.01786888, BC Loss: 0.15739486\n",
      "Iteration: 14590, Total Loss: 0.17523059, Physics Loss: 0.01780978, BC Loss: 0.15742081\n",
      "Iteration: 14600, Total Loss: 0.17518708, Physics Loss: 0.01798311, BC Loss: 0.15720397\n",
      "Iteration: 14610, Total Loss: 0.17510375, Physics Loss: 0.01780267, BC Loss: 0.15730108\n",
      "Iteration: 14620, Total Loss: 0.17503059, Physics Loss: 0.01750906, BC Loss: 0.15752153\n",
      "Iteration: 14630, Total Loss: 0.17496143, Physics Loss: 0.01744714, BC Loss: 0.15751429\n",
      "Iteration: 14640, Total Loss: 0.17482285, Physics Loss: 0.01745485, BC Loss: 0.15736800\n",
      "Iteration: 14650, Total Loss: 0.17472568, Physics Loss: 0.01742994, BC Loss: 0.15729573\n",
      "Iteration: 14660, Total Loss: 0.17467281, Physics Loss: 0.01738370, BC Loss: 0.15728912\n",
      "Iteration: 14670, Total Loss: 0.17455460, Physics Loss: 0.01723005, BC Loss: 0.15732455\n",
      "Iteration: 14680, Total Loss: 0.17443880, Physics Loss: 0.01692142, BC Loss: 0.15751739\n",
      "Iteration: 14690, Total Loss: 0.17436011, Physics Loss: 0.01698970, BC Loss: 0.15737042\n",
      "Iteration: 14700, Total Loss: 0.17423820, Physics Loss: 0.01696791, BC Loss: 0.15727030\n",
      "Iteration: 14710, Total Loss: 0.17411058, Physics Loss: 0.01746384, BC Loss: 0.15664674\n",
      "Iteration: 14720, Total Loss: 0.17398490, Physics Loss: 0.01723184, BC Loss: 0.15675306\n",
      "Iteration: 14730, Total Loss: 0.17381838, Physics Loss: 0.01657409, BC Loss: 0.15724429\n",
      "Iteration: 14740, Total Loss: 0.17344189, Physics Loss: 0.01685906, BC Loss: 0.15658283\n",
      "Iteration: 14750, Total Loss: 0.17317474, Physics Loss: 0.01708511, BC Loss: 0.15608963\n",
      "Iteration: 14760, Total Loss: 0.17292988, Physics Loss: 0.01718065, BC Loss: 0.15574923\n",
      "Iteration: 14770, Total Loss: 0.17268056, Physics Loss: 0.01763626, BC Loss: 0.15504430\n",
      "Iteration: 14780, Total Loss: 0.17231138, Physics Loss: 0.01874737, BC Loss: 0.15356401\n",
      "Iteration: 14790, Total Loss: 0.17190872, Physics Loss: 0.01937456, BC Loss: 0.15253416\n",
      "Iteration: 14800, Total Loss: 0.17169541, Physics Loss: 0.01898557, BC Loss: 0.15270984\n",
      "Iteration: 14810, Total Loss: 0.17145979, Physics Loss: 0.01859190, BC Loss: 0.15286790\n",
      "Iteration: 14820, Total Loss: 0.17134984, Physics Loss: 0.01897107, BC Loss: 0.15237877\n",
      "Iteration: 14830, Total Loss: 0.17114455, Physics Loss: 0.01880208, BC Loss: 0.15234247\n",
      "Iteration: 14840, Total Loss: 0.17095301, Physics Loss: 0.01795708, BC Loss: 0.15299591\n",
      "Iteration: 14850, Total Loss: 0.17082246, Physics Loss: 0.01719217, BC Loss: 0.15363029\n",
      "Iteration: 14860, Total Loss: 0.17076914, Physics Loss: 0.01710944, BC Loss: 0.15365970\n",
      "Iteration: 14870, Total Loss: 0.17066304, Physics Loss: 0.01690922, BC Loss: 0.15375382\n",
      "Iteration: 14880, Total Loss: 0.17063132, Physics Loss: 0.01685005, BC Loss: 0.15378127\n",
      "Iteration: 14890, Total Loss: 0.17057341, Physics Loss: 0.01693872, BC Loss: 0.15363468\n",
      "Iteration: 14900, Total Loss: 0.17049144, Physics Loss: 0.01728170, BC Loss: 0.15320975\n",
      "Iteration: 14910, Total Loss: 0.17032540, Physics Loss: 0.01704231, BC Loss: 0.15328309\n",
      "Iteration: 14920, Total Loss: 0.17022446, Physics Loss: 0.01636002, BC Loss: 0.15386444\n",
      "Iteration: 14930, Total Loss: 0.17013873, Physics Loss: 0.01631545, BC Loss: 0.15382329\n",
      "Iteration: 14940, Total Loss: 0.17006029, Physics Loss: 0.01668692, BC Loss: 0.15337338\n",
      "Iteration: 14950, Total Loss: 0.17000620, Physics Loss: 0.01673103, BC Loss: 0.15327516\n",
      "Iteration: 14960, Total Loss: 0.16982406, Physics Loss: 0.01649069, BC Loss: 0.15333337\n",
      "Iteration: 14970, Total Loss: 0.16950960, Physics Loss: 0.01664384, BC Loss: 0.15286577\n",
      "Iteration: 14980, Total Loss: 0.16938639, Physics Loss: 0.01669985, BC Loss: 0.15268654\n",
      "Iteration: 14990, Total Loss: 0.16931528, Physics Loss: 0.01670948, BC Loss: 0.15260580\n",
      "Iteration: 15000, Total Loss: 0.16914995, Physics Loss: 0.01680185, BC Loss: 0.15234810\n",
      "Iteration: 15010, Total Loss: 0.17431171, Physics Loss: 0.02044980, BC Loss: 0.15386191\n",
      "Iteration: 15020, Total Loss: 0.17264672, Physics Loss: 0.01962775, BC Loss: 0.15301897\n",
      "Iteration: 15030, Total Loss: 0.17191508, Physics Loss: 0.01914230, BC Loss: 0.15277278\n",
      "Iteration: 15040, Total Loss: 0.17147015, Physics Loss: 0.01774921, BC Loss: 0.15372095\n",
      "Iteration: 15050, Total Loss: 0.17108873, Physics Loss: 0.01807023, BC Loss: 0.15301849\n",
      "Iteration: 15060, Total Loss: 0.17060533, Physics Loss: 0.01926833, BC Loss: 0.15133700\n",
      "Iteration: 15070, Total Loss: 0.16997781, Physics Loss: 0.01918533, BC Loss: 0.15079248\n",
      "Iteration: 15080, Total Loss: 0.16946281, Physics Loss: 0.01951489, BC Loss: 0.14994793\n",
      "Iteration: 15090, Total Loss: 0.16936219, Physics Loss: 0.01950967, BC Loss: 0.14985251\n",
      "Iteration: 15100, Total Loss: 0.16924667, Physics Loss: 0.01931869, BC Loss: 0.14992797\n",
      "Iteration: 15110, Total Loss: 0.16907448, Physics Loss: 0.01889671, BC Loss: 0.15017778\n",
      "Iteration: 15120, Total Loss: 0.16859519, Physics Loss: 0.01814781, BC Loss: 0.15044738\n",
      "Iteration: 15130, Total Loss: 0.16846527, Physics Loss: 0.01774897, BC Loss: 0.15071630\n",
      "Iteration: 15140, Total Loss: 0.16828185, Physics Loss: 0.01714230, BC Loss: 0.15113956\n",
      "Iteration: 15150, Total Loss: 0.16807812, Physics Loss: 0.01730627, BC Loss: 0.15077186\n",
      "Iteration: 15160, Total Loss: 0.16796550, Physics Loss: 0.01776993, BC Loss: 0.15019558\n",
      "Iteration: 15170, Total Loss: 0.16784315, Physics Loss: 0.01814212, BC Loss: 0.14970103\n",
      "Iteration: 15180, Total Loss: 0.16778591, Physics Loss: 0.01819269, BC Loss: 0.14959322\n",
      "Iteration: 15190, Total Loss: 0.16764191, Physics Loss: 0.01780623, BC Loss: 0.14983568\n",
      "Iteration: 15200, Total Loss: 0.16747932, Physics Loss: 0.01751935, BC Loss: 0.14995998\n",
      "Iteration: 15210, Total Loss: 0.16737299, Physics Loss: 0.01694278, BC Loss: 0.15043020\n",
      "Iteration: 15220, Total Loss: 0.16728707, Physics Loss: 0.01682495, BC Loss: 0.15046212\n",
      "Iteration: 15230, Total Loss: 0.16718763, Physics Loss: 0.01711555, BC Loss: 0.15007208\n",
      "Iteration: 15240, Total Loss: 0.16701777, Physics Loss: 0.01748567, BC Loss: 0.14953211\n",
      "Iteration: 15250, Total Loss: 0.16685778, Physics Loss: 0.01687902, BC Loss: 0.14997876\n",
      "Iteration: 15260, Total Loss: 0.16675532, Physics Loss: 0.01646158, BC Loss: 0.15029374\n",
      "Iteration: 15270, Total Loss: 0.16671875, Physics Loss: 0.01644776, BC Loss: 0.15027100\n",
      "Iteration: 15280, Total Loss: 0.16668126, Physics Loss: 0.01630552, BC Loss: 0.15037574\n",
      "Iteration: 15290, Total Loss: 0.16666044, Physics Loss: 0.01629996, BC Loss: 0.15036048\n",
      "Iteration: 15300, Total Loss: 0.16662373, Physics Loss: 0.01640997, BC Loss: 0.15021375\n",
      "Iteration: 15310, Total Loss: 0.16651610, Physics Loss: 0.01676482, BC Loss: 0.14975128\n",
      "Iteration: 15320, Total Loss: 0.16641660, Physics Loss: 0.01688058, BC Loss: 0.14953601\n",
      "Iteration: 15330, Total Loss: 0.16636290, Physics Loss: 0.01661845, BC Loss: 0.14974445\n",
      "Iteration: 15340, Total Loss: 0.16630641, Physics Loss: 0.01634315, BC Loss: 0.14996326\n",
      "Iteration: 15350, Total Loss: 0.16627371, Physics Loss: 0.01633605, BC Loss: 0.14993766\n",
      "Iteration: 15360, Total Loss: 0.16625115, Physics Loss: 0.01650712, BC Loss: 0.14974403\n",
      "Iteration: 15370, Total Loss: 0.16619737, Physics Loss: 0.01647703, BC Loss: 0.14972034\n",
      "Iteration: 15380, Total Loss: 0.16612360, Physics Loss: 0.01652389, BC Loss: 0.14959970\n",
      "Iteration: 15390, Total Loss: 0.16603188, Physics Loss: 0.01682222, BC Loss: 0.14920966\n",
      "Iteration: 15400, Total Loss: 0.16593042, Physics Loss: 0.01687902, BC Loss: 0.14905140\n",
      "Iteration: 15410, Total Loss: 0.16584259, Physics Loss: 0.01674608, BC Loss: 0.14909652\n",
      "Iteration: 15420, Total Loss: 0.16577362, Physics Loss: 0.01696666, BC Loss: 0.14880696\n",
      "Iteration: 15430, Total Loss: 0.16575705, Physics Loss: 0.01710038, BC Loss: 0.14865667\n",
      "Iteration: 15440, Total Loss: 0.16566432, Physics Loss: 0.01709453, BC Loss: 0.14856979\n",
      "Iteration: 15450, Total Loss: 0.16557714, Physics Loss: 0.01659725, BC Loss: 0.14897989\n",
      "Iteration: 15460, Total Loss: 0.16551425, Physics Loss: 0.01641656, BC Loss: 0.14909768\n",
      "Iteration: 15470, Total Loss: 0.16544625, Physics Loss: 0.01669508, BC Loss: 0.14875117\n",
      "Iteration: 15480, Total Loss: 0.16541766, Physics Loss: 0.01678558, BC Loss: 0.14863208\n",
      "Iteration: 15490, Total Loss: 0.16538972, Physics Loss: 0.01663410, BC Loss: 0.14875562\n",
      "Iteration: 15500, Total Loss: 0.16531000, Physics Loss: 0.01655033, BC Loss: 0.14875966\n",
      "Iteration: 15510, Total Loss: 0.16524103, Physics Loss: 0.01679087, BC Loss: 0.14845017\n",
      "Iteration: 15520, Total Loss: 0.16518518, Physics Loss: 0.01671789, BC Loss: 0.14846729\n",
      "Iteration: 15530, Total Loss: 0.16513886, Physics Loss: 0.01650926, BC Loss: 0.14862959\n",
      "Iteration: 15540, Total Loss: 0.16507035, Physics Loss: 0.01673856, BC Loss: 0.14833179\n",
      "Iteration: 15550, Total Loss: 0.16502841, Physics Loss: 0.01709627, BC Loss: 0.14793214\n",
      "Iteration: 15560, Total Loss: 0.16498724, Physics Loss: 0.01724589, BC Loss: 0.14774135\n",
      "Iteration: 15570, Total Loss: 0.16493781, Physics Loss: 0.01691801, BC Loss: 0.14801979\n",
      "Iteration: 15580, Total Loss: 0.16486843, Physics Loss: 0.01673291, BC Loss: 0.14813551\n",
      "Iteration: 15590, Total Loss: 0.16477385, Physics Loss: 0.01691232, BC Loss: 0.14786154\n",
      "Iteration: 15600, Total Loss: 0.16467749, Physics Loss: 0.01710226, BC Loss: 0.14757523\n",
      "Iteration: 15610, Total Loss: 0.16457283, Physics Loss: 0.01693618, BC Loss: 0.14763665\n",
      "Iteration: 15620, Total Loss: 0.16451241, Physics Loss: 0.01669862, BC Loss: 0.14781378\n",
      "Iteration: 15630, Total Loss: 0.16443232, Physics Loss: 0.01631569, BC Loss: 0.14811662\n",
      "Iteration: 15640, Total Loss: 0.16433194, Physics Loss: 0.01588991, BC Loss: 0.14844203\n",
      "Iteration: 15650, Total Loss: 0.16422561, Physics Loss: 0.01580650, BC Loss: 0.14841910\n",
      "Iteration: 15660, Total Loss: 0.16417873, Physics Loss: 0.01593132, BC Loss: 0.14824742\n",
      "Iteration: 15670, Total Loss: 0.16415210, Physics Loss: 0.01573275, BC Loss: 0.14841935\n",
      "Iteration: 15680, Total Loss: 0.16412887, Physics Loss: 0.01556575, BC Loss: 0.14856312\n",
      "Iteration: 15690, Total Loss: 0.16409519, Physics Loss: 0.01573164, BC Loss: 0.14836356\n",
      "Iteration: 15700, Total Loss: 0.16405281, Physics Loss: 0.01597922, BC Loss: 0.14807358\n",
      "Iteration: 15710, Total Loss: 0.16395083, Physics Loss: 0.01604095, BC Loss: 0.14790988\n",
      "Iteration: 15720, Total Loss: 0.16385128, Physics Loss: 0.01600151, BC Loss: 0.14784977\n",
      "Iteration: 15730, Total Loss: 0.16374253, Physics Loss: 0.01612012, BC Loss: 0.14762241\n",
      "Iteration: 15740, Total Loss: 0.16356006, Physics Loss: 0.01652060, BC Loss: 0.14703946\n",
      "Iteration: 15750, Total Loss: 0.16347265, Physics Loss: 0.01663988, BC Loss: 0.14683276\n",
      "Iteration: 15760, Total Loss: 0.16337001, Physics Loss: 0.01673793, BC Loss: 0.14663208\n",
      "Iteration: 15770, Total Loss: 0.16315517, Physics Loss: 0.01688845, BC Loss: 0.14626673\n",
      "Iteration: 15780, Total Loss: 0.16301391, Physics Loss: 0.01662431, BC Loss: 0.14638960\n",
      "Iteration: 15790, Total Loss: 0.16290912, Physics Loss: 0.01647156, BC Loss: 0.14643756\n",
      "Iteration: 15800, Total Loss: 0.16282257, Physics Loss: 0.01652182, BC Loss: 0.14630076\n",
      "Iteration: 15810, Total Loss: 0.16272160, Physics Loss: 0.01696708, BC Loss: 0.14575452\n",
      "Iteration: 15820, Total Loss: 0.16267163, Physics Loss: 0.01697972, BC Loss: 0.14569190\n",
      "Iteration: 15830, Total Loss: 0.16257314, Physics Loss: 0.01659835, BC Loss: 0.14597479\n",
      "Iteration: 15840, Total Loss: 0.16246198, Physics Loss: 0.01626398, BC Loss: 0.14619800\n",
      "Iteration: 15850, Total Loss: 0.16226840, Physics Loss: 0.01611046, BC Loss: 0.14615795\n",
      "Iteration: 15860, Total Loss: 0.16200747, Physics Loss: 0.01624145, BC Loss: 0.14576602\n",
      "Iteration: 15870, Total Loss: 0.16189043, Physics Loss: 0.01638485, BC Loss: 0.14550558\n",
      "Iteration: 15880, Total Loss: 0.16173951, Physics Loss: 0.01647989, BC Loss: 0.14525962\n",
      "Iteration: 15890, Total Loss: 0.16162366, Physics Loss: 0.01631001, BC Loss: 0.14531365\n",
      "Iteration: 15900, Total Loss: 0.16152479, Physics Loss: 0.01587244, BC Loss: 0.14565235\n",
      "Iteration: 15910, Total Loss: 0.16145268, Physics Loss: 0.01613989, BC Loss: 0.14531279\n",
      "Iteration: 15920, Total Loss: 0.16131610, Physics Loss: 0.01649693, BC Loss: 0.14481917\n",
      "Iteration: 15930, Total Loss: 0.16116598, Physics Loss: 0.01647841, BC Loss: 0.14468756\n",
      "Iteration: 15940, Total Loss: 0.16103019, Physics Loss: 0.01646064, BC Loss: 0.14456955\n",
      "Iteration: 15950, Total Loss: 0.16089575, Physics Loss: 0.01615312, BC Loss: 0.14474264\n",
      "Iteration: 15960, Total Loss: 0.16068739, Physics Loss: 0.01586615, BC Loss: 0.14482124\n",
      "Iteration: 15970, Total Loss: 0.16051434, Physics Loss: 0.01607694, BC Loss: 0.14443740\n",
      "Iteration: 15980, Total Loss: 0.16032125, Physics Loss: 0.01623954, BC Loss: 0.14408171\n",
      "Iteration: 15990, Total Loss: 0.16000965, Physics Loss: 0.01649418, BC Loss: 0.14351547\n",
      "Iteration: 16000, Total Loss: 0.15972777, Physics Loss: 0.01702912, BC Loss: 0.14269865\n",
      "Iteration: 16010, Total Loss: 0.16578647, Physics Loss: 0.01866731, BC Loss: 0.14711916\n",
      "Iteration: 16020, Total Loss: 0.16493265, Physics Loss: 0.01740977, BC Loss: 0.14752288\n",
      "Iteration: 16030, Total Loss: 0.16450642, Physics Loss: 0.01779409, BC Loss: 0.14671233\n",
      "Iteration: 16040, Total Loss: 0.16409935, Physics Loss: 0.01851739, BC Loss: 0.14558196\n",
      "Iteration: 16050, Total Loss: 0.16378029, Physics Loss: 0.01817455, BC Loss: 0.14560574\n",
      "Iteration: 16060, Total Loss: 0.16350797, Physics Loss: 0.01788849, BC Loss: 0.14561948\n",
      "Iteration: 16070, Total Loss: 0.16323315, Physics Loss: 0.01776293, BC Loss: 0.14547022\n",
      "Iteration: 16080, Total Loss: 0.16301097, Physics Loss: 0.01739746, BC Loss: 0.14561351\n",
      "Iteration: 16090, Total Loss: 0.16280621, Physics Loss: 0.01727619, BC Loss: 0.14553003\n",
      "Iteration: 16100, Total Loss: 0.16236296, Physics Loss: 0.01776781, BC Loss: 0.14459515\n",
      "Iteration: 16110, Total Loss: 0.16187862, Physics Loss: 0.01756209, BC Loss: 0.14431652\n",
      "Iteration: 16120, Total Loss: 0.16163231, Physics Loss: 0.01694651, BC Loss: 0.14468580\n",
      "Iteration: 16130, Total Loss: 0.16142362, Physics Loss: 0.01660571, BC Loss: 0.14481792\n",
      "Iteration: 16140, Total Loss: 0.16124667, Physics Loss: 0.01633315, BC Loss: 0.14491352\n",
      "Iteration: 16150, Total Loss: 0.16103265, Physics Loss: 0.01587841, BC Loss: 0.14515424\n",
      "Iteration: 16160, Total Loss: 0.16088577, Physics Loss: 0.01556464, BC Loss: 0.14532113\n",
      "Iteration: 16170, Total Loss: 0.16066189, Physics Loss: 0.01502758, BC Loss: 0.14563431\n",
      "Iteration: 16180, Total Loss: 0.16057701, Physics Loss: 0.01500317, BC Loss: 0.14557385\n",
      "Iteration: 16190, Total Loss: 0.16052385, Physics Loss: 0.01510372, BC Loss: 0.14542012\n",
      "Iteration: 16200, Total Loss: 0.16045819, Physics Loss: 0.01511241, BC Loss: 0.14534578\n",
      "Iteration: 16210, Total Loss: 0.16040219, Physics Loss: 0.01500841, BC Loss: 0.14539377\n",
      "Iteration: 16220, Total Loss: 0.16030885, Physics Loss: 0.01473701, BC Loss: 0.14557184\n",
      "Iteration: 16230, Total Loss: 0.15996712, Physics Loss: 0.01500500, BC Loss: 0.14496212\n",
      "Iteration: 16240, Total Loss: 0.15956806, Physics Loss: 0.01542473, BC Loss: 0.14414333\n",
      "Iteration: 16250, Total Loss: 0.15934043, Physics Loss: 0.01537044, BC Loss: 0.14396998\n",
      "Iteration: 16260, Total Loss: 0.15911907, Physics Loss: 0.01584760, BC Loss: 0.14327146\n",
      "Iteration: 16270, Total Loss: 0.15888628, Physics Loss: 0.01665016, BC Loss: 0.14223611\n",
      "Iteration: 16280, Total Loss: 0.15869738, Physics Loss: 0.01700525, BC Loss: 0.14169213\n",
      "Iteration: 16290, Total Loss: 0.15846899, Physics Loss: 0.01671857, BC Loss: 0.14175043\n",
      "Iteration: 16300, Total Loss: 0.15817617, Physics Loss: 0.01607553, BC Loss: 0.14210063\n",
      "Iteration: 16310, Total Loss: 0.15799071, Physics Loss: 0.01581847, BC Loss: 0.14217223\n",
      "Iteration: 16320, Total Loss: 0.15783261, Physics Loss: 0.01574845, BC Loss: 0.14208415\n",
      "Iteration: 16330, Total Loss: 0.15761375, Physics Loss: 0.01583376, BC Loss: 0.14177999\n",
      "Iteration: 16340, Total Loss: 0.15747923, Physics Loss: 0.01553216, BC Loss: 0.14194708\n",
      "Iteration: 16350, Total Loss: 0.15741695, Physics Loss: 0.01531753, BC Loss: 0.14209943\n",
      "Iteration: 16360, Total Loss: 0.15733133, Physics Loss: 0.01533370, BC Loss: 0.14199764\n",
      "Iteration: 16370, Total Loss: 0.15718395, Physics Loss: 0.01511142, BC Loss: 0.14207253\n",
      "Iteration: 16380, Total Loss: 0.15709874, Physics Loss: 0.01505312, BC Loss: 0.14204562\n",
      "Iteration: 16390, Total Loss: 0.15703827, Physics Loss: 0.01521517, BC Loss: 0.14182310\n",
      "Iteration: 16400, Total Loss: 0.15693477, Physics Loss: 0.01486812, BC Loss: 0.14206666\n",
      "Iteration: 16410, Total Loss: 0.15680736, Physics Loss: 0.01446676, BC Loss: 0.14234060\n",
      "Iteration: 16420, Total Loss: 0.15670800, Physics Loss: 0.01449089, BC Loss: 0.14221711\n",
      "Iteration: 16430, Total Loss: 0.15664831, Physics Loss: 0.01450099, BC Loss: 0.14214732\n",
      "Iteration: 16440, Total Loss: 0.15660208, Physics Loss: 0.01454534, BC Loss: 0.14205675\n",
      "Iteration: 16450, Total Loss: 0.15656595, Physics Loss: 0.01441059, BC Loss: 0.14215536\n",
      "Iteration: 16460, Total Loss: 0.15651241, Physics Loss: 0.01421128, BC Loss: 0.14230113\n",
      "Iteration: 16470, Total Loss: 0.15638335, Physics Loss: 0.01405998, BC Loss: 0.14232337\n",
      "Iteration: 16480, Total Loss: 0.15619299, Physics Loss: 0.01446616, BC Loss: 0.14172682\n",
      "Iteration: 16490, Total Loss: 0.15607303, Physics Loss: 0.01441410, BC Loss: 0.14165893\n",
      "Iteration: 16500, Total Loss: 0.15594926, Physics Loss: 0.01381585, BC Loss: 0.14213341\n",
      "Iteration: 16510, Total Loss: 0.15586169, Physics Loss: 0.01391110, BC Loss: 0.14195059\n",
      "Iteration: 16520, Total Loss: 0.15577626, Physics Loss: 0.01435132, BC Loss: 0.14142494\n",
      "Iteration: 16530, Total Loss: 0.15566039, Physics Loss: 0.01451644, BC Loss: 0.14114395\n",
      "Iteration: 16540, Total Loss: 0.15554307, Physics Loss: 0.01461779, BC Loss: 0.14092529\n",
      "Iteration: 16550, Total Loss: 0.15542176, Physics Loss: 0.01509013, BC Loss: 0.14033163\n",
      "Iteration: 16560, Total Loss: 0.15521501, Physics Loss: 0.01529222, BC Loss: 0.13992280\n",
      "Iteration: 16570, Total Loss: 0.15509473, Physics Loss: 0.01499097, BC Loss: 0.14010376\n",
      "Iteration: 16580, Total Loss: 0.15493698, Physics Loss: 0.01513955, BC Loss: 0.13979743\n",
      "Iteration: 16590, Total Loss: 0.15485831, Physics Loss: 0.01512284, BC Loss: 0.13973546\n",
      "Iteration: 16600, Total Loss: 0.15477085, Physics Loss: 0.01474810, BC Loss: 0.14002275\n",
      "Iteration: 16610, Total Loss: 0.15455936, Physics Loss: 0.01435278, BC Loss: 0.14020658\n",
      "Iteration: 16620, Total Loss: 0.15437734, Physics Loss: 0.01419458, BC Loss: 0.14018276\n",
      "Iteration: 16630, Total Loss: 0.15423359, Physics Loss: 0.01464761, BC Loss: 0.13958597\n",
      "Iteration: 16640, Total Loss: 0.15414318, Physics Loss: 0.01495389, BC Loss: 0.13918930\n",
      "Iteration: 16650, Total Loss: 0.15406004, Physics Loss: 0.01471892, BC Loss: 0.13934112\n",
      "Iteration: 16660, Total Loss: 0.15400325, Physics Loss: 0.01459097, BC Loss: 0.13941228\n",
      "Iteration: 16670, Total Loss: 0.15386067, Physics Loss: 0.01445051, BC Loss: 0.13941017\n",
      "Iteration: 16680, Total Loss: 0.15370125, Physics Loss: 0.01397198, BC Loss: 0.13972926\n",
      "Iteration: 16690, Total Loss: 0.15353884, Physics Loss: 0.01377430, BC Loss: 0.13976453\n",
      "Iteration: 16700, Total Loss: 0.15341735, Physics Loss: 0.01395425, BC Loss: 0.13946310\n",
      "Iteration: 16710, Total Loss: 0.15336791, Physics Loss: 0.01406160, BC Loss: 0.13930631\n",
      "Iteration: 16720, Total Loss: 0.15318897, Physics Loss: 0.01395899, BC Loss: 0.13922998\n",
      "Iteration: 16730, Total Loss: 0.15272537, Physics Loss: 0.01423313, BC Loss: 0.13849224\n",
      "Iteration: 16740, Total Loss: 0.15251757, Physics Loss: 0.01456064, BC Loss: 0.13795693\n",
      "Iteration: 16750, Total Loss: 0.15232539, Physics Loss: 0.01500956, BC Loss: 0.13731584\n",
      "Iteration: 16760, Total Loss: 0.15224996, Physics Loss: 0.01505699, BC Loss: 0.13719296\n",
      "Iteration: 16770, Total Loss: 0.15214920, Physics Loss: 0.01474777, BC Loss: 0.13740143\n",
      "Iteration: 16780, Total Loss: 0.15199754, Physics Loss: 0.01485625, BC Loss: 0.13714129\n",
      "Iteration: 16790, Total Loss: 0.15185177, Physics Loss: 0.01517805, BC Loss: 0.13667373\n",
      "Iteration: 16800, Total Loss: 0.15177967, Physics Loss: 0.01526479, BC Loss: 0.13651487\n",
      "Iteration: 16810, Total Loss: 0.15169895, Physics Loss: 0.01546750, BC Loss: 0.13623145\n",
      "Iteration: 16820, Total Loss: 0.15159512, Physics Loss: 0.01543521, BC Loss: 0.13615991\n",
      "Iteration: 16830, Total Loss: 0.15154067, Physics Loss: 0.01507922, BC Loss: 0.13646144\n",
      "Iteration: 16840, Total Loss: 0.15147430, Physics Loss: 0.01490928, BC Loss: 0.13656501\n",
      "Iteration: 16850, Total Loss: 0.15136062, Physics Loss: 0.01506389, BC Loss: 0.13629673\n",
      "Iteration: 16860, Total Loss: 0.15126368, Physics Loss: 0.01540670, BC Loss: 0.13585699\n",
      "Iteration: 16870, Total Loss: 0.15115245, Physics Loss: 0.01522324, BC Loss: 0.13592921\n",
      "Iteration: 16880, Total Loss: 0.15101182, Physics Loss: 0.01508712, BC Loss: 0.13592470\n",
      "Iteration: 16890, Total Loss: 0.15098336, Physics Loss: 0.01502577, BC Loss: 0.13595760\n",
      "Iteration: 16900, Total Loss: 0.15094841, Physics Loss: 0.01491502, BC Loss: 0.13603339\n",
      "Iteration: 16910, Total Loss: 0.15090652, Physics Loss: 0.01476542, BC Loss: 0.13614109\n",
      "Iteration: 16920, Total Loss: 0.15083149, Physics Loss: 0.01440214, BC Loss: 0.13642935\n",
      "Iteration: 16930, Total Loss: 0.15078428, Physics Loss: 0.01423178, BC Loss: 0.13655251\n",
      "Iteration: 16940, Total Loss: 0.15072145, Physics Loss: 0.01419435, BC Loss: 0.13652709\n",
      "Iteration: 16950, Total Loss: 0.15066324, Physics Loss: 0.01422943, BC Loss: 0.13643381\n",
      "Iteration: 16960, Total Loss: 0.15055224, Physics Loss: 0.01424501, BC Loss: 0.13630722\n",
      "Iteration: 16970, Total Loss: 0.15043159, Physics Loss: 0.01419411, BC Loss: 0.13623747\n",
      "Iteration: 16980, Total Loss: 0.15034257, Physics Loss: 0.01423874, BC Loss: 0.13610382\n",
      "Iteration: 16990, Total Loss: 0.15019189, Physics Loss: 0.01470987, BC Loss: 0.13548203\n",
      "Iteration: 17000, Total Loss: 0.15012310, Physics Loss: 0.01454090, BC Loss: 0.13558221\n",
      "Iteration: 17010, Total Loss: 0.16043016, Physics Loss: 0.02017041, BC Loss: 0.14025976\n",
      "Iteration: 17020, Total Loss: 0.15765135, Physics Loss: 0.01640989, BC Loss: 0.14124146\n",
      "Iteration: 17030, Total Loss: 0.15713763, Physics Loss: 0.01558443, BC Loss: 0.14155321\n",
      "Iteration: 17040, Total Loss: 0.15682250, Physics Loss: 0.01558199, BC Loss: 0.14124051\n",
      "Iteration: 17050, Total Loss: 0.15650791, Physics Loss: 0.01529375, BC Loss: 0.14121416\n",
      "Iteration: 17060, Total Loss: 0.15613092, Physics Loss: 0.01556861, BC Loss: 0.14056231\n",
      "Iteration: 17070, Total Loss: 0.15582807, Physics Loss: 0.01645103, BC Loss: 0.13937704\n",
      "Iteration: 17080, Total Loss: 0.15568864, Physics Loss: 0.01658086, BC Loss: 0.13910778\n",
      "Iteration: 17090, Total Loss: 0.15551683, Physics Loss: 0.01599631, BC Loss: 0.13952053\n",
      "Iteration: 17100, Total Loss: 0.15543267, Physics Loss: 0.01627546, BC Loss: 0.13915722\n",
      "Iteration: 17110, Total Loss: 0.15534729, Physics Loss: 0.01670287, BC Loss: 0.13864441\n",
      "Iteration: 17120, Total Loss: 0.15505126, Physics Loss: 0.01593438, BC Loss: 0.13911688\n",
      "Iteration: 17130, Total Loss: 0.15475713, Physics Loss: 0.01575488, BC Loss: 0.13900225\n",
      "Iteration: 17140, Total Loss: 0.15455447, Physics Loss: 0.01593615, BC Loss: 0.13861832\n",
      "Iteration: 17150, Total Loss: 0.15444598, Physics Loss: 0.01551595, BC Loss: 0.13893002\n",
      "Iteration: 17160, Total Loss: 0.15428668, Physics Loss: 0.01536121, BC Loss: 0.13892548\n",
      "Iteration: 17170, Total Loss: 0.15412590, Physics Loss: 0.01572591, BC Loss: 0.13839999\n",
      "Iteration: 17180, Total Loss: 0.15395398, Physics Loss: 0.01585813, BC Loss: 0.13809586\n",
      "Iteration: 17190, Total Loss: 0.15383986, Physics Loss: 0.01583048, BC Loss: 0.13800938\n",
      "Iteration: 17200, Total Loss: 0.15368654, Physics Loss: 0.01593692, BC Loss: 0.13774961\n",
      "Iteration: 17210, Total Loss: 0.15344019, Physics Loss: 0.01608038, BC Loss: 0.13735981\n",
      "Iteration: 17220, Total Loss: 0.15319553, Physics Loss: 0.01595015, BC Loss: 0.13724539\n",
      "Iteration: 17230, Total Loss: 0.15306152, Physics Loss: 0.01579861, BC Loss: 0.13726291\n",
      "Iteration: 17240, Total Loss: 0.15290004, Physics Loss: 0.01571247, BC Loss: 0.13718757\n",
      "Iteration: 17250, Total Loss: 0.15274881, Physics Loss: 0.01580806, BC Loss: 0.13694075\n",
      "Iteration: 17260, Total Loss: 0.15256318, Physics Loss: 0.01562398, BC Loss: 0.13693920\n",
      "Iteration: 17270, Total Loss: 0.15240270, Physics Loss: 0.01492110, BC Loss: 0.13748160\n",
      "Iteration: 17280, Total Loss: 0.15232086, Physics Loss: 0.01488081, BC Loss: 0.13744006\n",
      "Iteration: 17290, Total Loss: 0.15220021, Physics Loss: 0.01508645, BC Loss: 0.13711375\n",
      "Iteration: 17300, Total Loss: 0.15208577, Physics Loss: 0.01506364, BC Loss: 0.13702212\n",
      "Iteration: 17310, Total Loss: 0.15191367, Physics Loss: 0.01503666, BC Loss: 0.13687700\n",
      "Iteration: 17320, Total Loss: 0.15179616, Physics Loss: 0.01494501, BC Loss: 0.13685116\n",
      "Iteration: 17330, Total Loss: 0.15166131, Physics Loss: 0.01498532, BC Loss: 0.13667600\n",
      "Iteration: 17340, Total Loss: 0.15154056, Physics Loss: 0.01520253, BC Loss: 0.13633803\n",
      "Iteration: 17350, Total Loss: 0.15130550, Physics Loss: 0.01519948, BC Loss: 0.13610601\n",
      "Iteration: 17360, Total Loss: 0.15110926, Physics Loss: 0.01486465, BC Loss: 0.13624461\n",
      "Iteration: 17370, Total Loss: 0.15095513, Physics Loss: 0.01485561, BC Loss: 0.13609952\n",
      "Iteration: 17380, Total Loss: 0.15073591, Physics Loss: 0.01509534, BC Loss: 0.13564058\n",
      "Iteration: 17390, Total Loss: 0.15060732, Physics Loss: 0.01486501, BC Loss: 0.13574231\n",
      "Iteration: 17400, Total Loss: 0.15054283, Physics Loss: 0.01480361, BC Loss: 0.13573921\n",
      "Iteration: 17410, Total Loss: 0.15048891, Physics Loss: 0.01495127, BC Loss: 0.13553764\n",
      "Iteration: 17420, Total Loss: 0.15040658, Physics Loss: 0.01500581, BC Loss: 0.13540077\n",
      "Iteration: 17430, Total Loss: 0.15021391, Physics Loss: 0.01439074, BC Loss: 0.13582318\n",
      "Iteration: 17440, Total Loss: 0.15004660, Physics Loss: 0.01391848, BC Loss: 0.13612813\n",
      "Iteration: 17450, Total Loss: 0.14998639, Physics Loss: 0.01379772, BC Loss: 0.13618866\n",
      "Iteration: 17460, Total Loss: 0.14995159, Physics Loss: 0.01378288, BC Loss: 0.13616872\n",
      "Iteration: 17470, Total Loss: 0.14991105, Physics Loss: 0.01375025, BC Loss: 0.13616081\n",
      "Iteration: 17480, Total Loss: 0.14983742, Physics Loss: 0.01371264, BC Loss: 0.13612477\n",
      "Iteration: 17490, Total Loss: 0.14974777, Physics Loss: 0.01397468, BC Loss: 0.13577309\n",
      "Iteration: 17500, Total Loss: 0.14967144, Physics Loss: 0.01406234, BC Loss: 0.13560909\n",
      "Iteration: 17510, Total Loss: 0.14954092, Physics Loss: 0.01384434, BC Loss: 0.13569658\n",
      "Iteration: 17520, Total Loss: 0.14947009, Physics Loss: 0.01375039, BC Loss: 0.13571970\n",
      "Iteration: 17530, Total Loss: 0.14941165, Physics Loss: 0.01407797, BC Loss: 0.13533369\n",
      "Iteration: 17540, Total Loss: 0.14937711, Physics Loss: 0.01418338, BC Loss: 0.13519372\n",
      "Iteration: 17550, Total Loss: 0.14934829, Physics Loss: 0.01413559, BC Loss: 0.13521270\n",
      "Iteration: 17560, Total Loss: 0.14931497, Physics Loss: 0.01401220, BC Loss: 0.13530278\n",
      "Iteration: 17570, Total Loss: 0.14926890, Physics Loss: 0.01393101, BC Loss: 0.13533789\n",
      "Iteration: 17580, Total Loss: 0.14921606, Physics Loss: 0.01422590, BC Loss: 0.13499016\n",
      "Iteration: 17590, Total Loss: 0.14904429, Physics Loss: 0.01494234, BC Loss: 0.13410196\n",
      "Iteration: 17600, Total Loss: 0.14880443, Physics Loss: 0.01483635, BC Loss: 0.13396809\n",
      "Iteration: 17610, Total Loss: 0.14864114, Physics Loss: 0.01458103, BC Loss: 0.13406011\n",
      "Iteration: 17620, Total Loss: 0.14847678, Physics Loss: 0.01492891, BC Loss: 0.13354786\n",
      "Iteration: 17630, Total Loss: 0.14835258, Physics Loss: 0.01531110, BC Loss: 0.13304147\n",
      "Iteration: 17640, Total Loss: 0.14822936, Physics Loss: 0.01525968, BC Loss: 0.13296968\n",
      "Iteration: 17650, Total Loss: 0.14808589, Physics Loss: 0.01550738, BC Loss: 0.13257851\n",
      "Iteration: 17660, Total Loss: 0.14797644, Physics Loss: 0.01551081, BC Loss: 0.13246563\n",
      "Iteration: 17670, Total Loss: 0.14788790, Physics Loss: 0.01507478, BC Loss: 0.13281313\n",
      "Iteration: 17680, Total Loss: 0.14781168, Physics Loss: 0.01487193, BC Loss: 0.13293976\n",
      "Iteration: 17690, Total Loss: 0.14768702, Physics Loss: 0.01500094, BC Loss: 0.13268608\n",
      "Iteration: 17700, Total Loss: 0.14757374, Physics Loss: 0.01519935, BC Loss: 0.13237439\n",
      "Iteration: 17710, Total Loss: 0.14751182, Physics Loss: 0.01527920, BC Loss: 0.13223262\n",
      "Iteration: 17720, Total Loss: 0.14730589, Physics Loss: 0.01498695, BC Loss: 0.13231894\n",
      "Iteration: 17730, Total Loss: 0.14707780, Physics Loss: 0.01428598, BC Loss: 0.13279182\n",
      "Iteration: 17740, Total Loss: 0.14695072, Physics Loss: 0.01414060, BC Loss: 0.13281013\n",
      "Iteration: 17750, Total Loss: 0.14684111, Physics Loss: 0.01405820, BC Loss: 0.13278291\n",
      "Iteration: 17760, Total Loss: 0.14675221, Physics Loss: 0.01417041, BC Loss: 0.13258180\n",
      "Iteration: 17770, Total Loss: 0.14664988, Physics Loss: 0.01450584, BC Loss: 0.13214403\n",
      "Iteration: 17780, Total Loss: 0.14646818, Physics Loss: 0.01449411, BC Loss: 0.13197407\n",
      "Iteration: 17790, Total Loss: 0.14632130, Physics Loss: 0.01403035, BC Loss: 0.13229094\n",
      "Iteration: 17800, Total Loss: 0.14618254, Physics Loss: 0.01410730, BC Loss: 0.13207524\n",
      "Iteration: 17810, Total Loss: 0.14606558, Physics Loss: 0.01421693, BC Loss: 0.13184865\n",
      "Iteration: 17820, Total Loss: 0.14599587, Physics Loss: 0.01401074, BC Loss: 0.13198513\n",
      "Iteration: 17830, Total Loss: 0.14588627, Physics Loss: 0.01412995, BC Loss: 0.13175632\n",
      "Iteration: 17840, Total Loss: 0.14578669, Physics Loss: 0.01443537, BC Loss: 0.13135132\n",
      "Iteration: 17850, Total Loss: 0.14570463, Physics Loss: 0.01464981, BC Loss: 0.13105482\n",
      "Iteration: 17860, Total Loss: 0.14560114, Physics Loss: 0.01455202, BC Loss: 0.13104913\n",
      "Iteration: 17870, Total Loss: 0.14544983, Physics Loss: 0.01402436, BC Loss: 0.13142547\n",
      "Iteration: 17880, Total Loss: 0.14530420, Physics Loss: 0.01433494, BC Loss: 0.13096926\n",
      "Iteration: 17890, Total Loss: 0.14524645, Physics Loss: 0.01468371, BC Loss: 0.13056274\n",
      "Iteration: 17900, Total Loss: 0.14512324, Physics Loss: 0.01452908, BC Loss: 0.13059416\n",
      "Iteration: 17910, Total Loss: 0.14502916, Physics Loss: 0.01406259, BC Loss: 0.13096656\n",
      "Iteration: 17920, Total Loss: 0.14489600, Physics Loss: 0.01435787, BC Loss: 0.13053814\n",
      "Iteration: 17930, Total Loss: 0.14473718, Physics Loss: 0.01493812, BC Loss: 0.12979905\n",
      "Iteration: 17940, Total Loss: 0.14464265, Physics Loss: 0.01510358, BC Loss: 0.12953907\n",
      "Iteration: 17950, Total Loss: 0.14456053, Physics Loss: 0.01536225, BC Loss: 0.12919828\n",
      "Iteration: 17960, Total Loss: 0.14448959, Physics Loss: 0.01534916, BC Loss: 0.12914044\n",
      "Iteration: 17970, Total Loss: 0.14437042, Physics Loss: 0.01492439, BC Loss: 0.12944603\n",
      "Iteration: 17980, Total Loss: 0.14417495, Physics Loss: 0.01479001, BC Loss: 0.12938493\n",
      "Iteration: 17990, Total Loss: 0.14404802, Physics Loss: 0.01495540, BC Loss: 0.12909262\n",
      "Iteration: 18000, Total Loss: 0.14384231, Physics Loss: 0.01521373, BC Loss: 0.12862858\n",
      "Iteration: 18010, Total Loss: 0.13949500, Physics Loss: 0.01760211, BC Loss: 0.12189290\n",
      "Iteration: 18020, Total Loss: 0.13473478, Physics Loss: 0.01750043, BC Loss: 0.11723435\n",
      "Iteration: 18030, Total Loss: 0.13307111, Physics Loss: 0.01626478, BC Loss: 0.11680634\n",
      "Iteration: 18040, Total Loss: 0.13131088, Physics Loss: 0.01630336, BC Loss: 0.11500751\n",
      "Iteration: 18050, Total Loss: 0.13055335, Physics Loss: 0.01516264, BC Loss: 0.11539071\n",
      "Iteration: 18060, Total Loss: 0.13019364, Physics Loss: 0.01498294, BC Loss: 0.11521070\n",
      "Iteration: 18070, Total Loss: 0.12986180, Physics Loss: 0.01565165, BC Loss: 0.11421016\n",
      "Iteration: 18080, Total Loss: 0.12927693, Physics Loss: 0.01516344, BC Loss: 0.11411349\n",
      "Iteration: 18090, Total Loss: 0.12874100, Physics Loss: 0.01512367, BC Loss: 0.11361732\n",
      "Iteration: 18100, Total Loss: 0.12851739, Physics Loss: 0.01505965, BC Loss: 0.11345774\n",
      "Iteration: 18110, Total Loss: 0.12825559, Physics Loss: 0.01481594, BC Loss: 0.11343966\n",
      "Iteration: 18120, Total Loss: 0.12787144, Physics Loss: 0.01457019, BC Loss: 0.11330125\n",
      "Iteration: 18130, Total Loss: 0.12759781, Physics Loss: 0.01401091, BC Loss: 0.11358690\n",
      "Iteration: 18140, Total Loss: 0.12726098, Physics Loss: 0.01398062, BC Loss: 0.11328036\n",
      "Iteration: 18150, Total Loss: 0.12684622, Physics Loss: 0.01404359, BC Loss: 0.11280263\n",
      "Iteration: 18160, Total Loss: 0.12658364, Physics Loss: 0.01414125, BC Loss: 0.11244237\n",
      "Iteration: 18170, Total Loss: 0.12617615, Physics Loss: 0.01435939, BC Loss: 0.11181676\n",
      "Iteration: 18180, Total Loss: 0.12569869, Physics Loss: 0.01356422, BC Loss: 0.11213447\n",
      "Iteration: 18190, Total Loss: 0.12547223, Physics Loss: 0.01385465, BC Loss: 0.11161758\n",
      "Iteration: 18200, Total Loss: 0.12525669, Physics Loss: 0.01420271, BC Loss: 0.11105397\n",
      "Iteration: 18210, Total Loss: 0.12504244, Physics Loss: 0.01349181, BC Loss: 0.11155062\n",
      "Iteration: 18220, Total Loss: 0.12470676, Physics Loss: 0.01324204, BC Loss: 0.11146472\n",
      "Iteration: 18230, Total Loss: 0.12450628, Physics Loss: 0.01412734, BC Loss: 0.11037894\n",
      "Iteration: 18240, Total Loss: 0.12431385, Physics Loss: 0.01420898, BC Loss: 0.11010486\n",
      "Iteration: 18250, Total Loss: 0.12413965, Physics Loss: 0.01383863, BC Loss: 0.11030102\n",
      "Iteration: 18260, Total Loss: 0.12394715, Physics Loss: 0.01362196, BC Loss: 0.11032519\n",
      "Iteration: 18270, Total Loss: 0.12357592, Physics Loss: 0.01423123, BC Loss: 0.10934469\n",
      "Iteration: 18280, Total Loss: 0.12326911, Physics Loss: 0.01461126, BC Loss: 0.10865785\n",
      "Iteration: 18290, Total Loss: 0.12314424, Physics Loss: 0.01472588, BC Loss: 0.10841836\n",
      "Iteration: 18300, Total Loss: 0.12295912, Physics Loss: 0.01472222, BC Loss: 0.10823691\n",
      "Iteration: 18310, Total Loss: 0.12244356, Physics Loss: 0.01415111, BC Loss: 0.10829246\n",
      "Iteration: 18320, Total Loss: 0.12220977, Physics Loss: 0.01412270, BC Loss: 0.10808706\n",
      "Iteration: 18330, Total Loss: 0.12196580, Physics Loss: 0.01468640, BC Loss: 0.10727939\n",
      "Iteration: 18340, Total Loss: 0.12179291, Physics Loss: 0.01486088, BC Loss: 0.10693203\n",
      "Iteration: 18350, Total Loss: 0.12145983, Physics Loss: 0.01435341, BC Loss: 0.10710642\n",
      "Iteration: 18360, Total Loss: 0.12100644, Physics Loss: 0.01410115, BC Loss: 0.10690530\n",
      "Iteration: 18370, Total Loss: 0.12069007, Physics Loss: 0.01428711, BC Loss: 0.10640296\n",
      "Iteration: 18380, Total Loss: 0.12032795, Physics Loss: 0.01425288, BC Loss: 0.10607507\n",
      "Iteration: 18390, Total Loss: 0.12001584, Physics Loss: 0.01401056, BC Loss: 0.10600529\n",
      "Iteration: 18400, Total Loss: 0.11972536, Physics Loss: 0.01376382, BC Loss: 0.10596155\n",
      "Iteration: 18410, Total Loss: 0.11960455, Physics Loss: 0.01330814, BC Loss: 0.10629641\n",
      "Iteration: 18420, Total Loss: 0.11954702, Physics Loss: 0.01320085, BC Loss: 0.10634618\n",
      "Iteration: 18430, Total Loss: 0.11949268, Physics Loss: 0.01332567, BC Loss: 0.10616701\n",
      "Iteration: 18440, Total Loss: 0.11940808, Physics Loss: 0.01321669, BC Loss: 0.10619139\n",
      "Iteration: 18450, Total Loss: 0.11924499, Physics Loss: 0.01281775, BC Loss: 0.10642724\n",
      "Iteration: 18460, Total Loss: 0.11900234, Physics Loss: 0.01259415, BC Loss: 0.10640819\n",
      "Iteration: 18470, Total Loss: 0.11864252, Physics Loss: 0.01263444, BC Loss: 0.10600808\n",
      "Iteration: 18480, Total Loss: 0.11850077, Physics Loss: 0.01270854, BC Loss: 0.10579222\n",
      "Iteration: 18490, Total Loss: 0.11823382, Physics Loss: 0.01272061, BC Loss: 0.10551322\n",
      "Iteration: 18500, Total Loss: 0.11817598, Physics Loss: 0.01260128, BC Loss: 0.10557470\n",
      "Iteration: 18510, Total Loss: 0.11810333, Physics Loss: 0.01244894, BC Loss: 0.10565439\n",
      "Iteration: 18520, Total Loss: 0.11794436, Physics Loss: 0.01204756, BC Loss: 0.10589680\n",
      "Iteration: 18530, Total Loss: 0.11765405, Physics Loss: 0.01163832, BC Loss: 0.10601573\n",
      "Iteration: 18540, Total Loss: 0.11756191, Physics Loss: 0.01165241, BC Loss: 0.10590950\n",
      "Iteration: 18550, Total Loss: 0.11745988, Physics Loss: 0.01173358, BC Loss: 0.10572630\n",
      "Iteration: 18560, Total Loss: 0.11738036, Physics Loss: 0.01185644, BC Loss: 0.10552391\n",
      "Iteration: 18570, Total Loss: 0.11732574, Physics Loss: 0.01169175, BC Loss: 0.10563399\n",
      "Iteration: 18580, Total Loss: 0.11724781, Physics Loss: 0.01153756, BC Loss: 0.10571025\n",
      "Iteration: 18590, Total Loss: 0.11689465, Physics Loss: 0.01142162, BC Loss: 0.10547303\n",
      "Iteration: 18600, Total Loss: 0.11674640, Physics Loss: 0.01141236, BC Loss: 0.10533403\n",
      "Iteration: 18610, Total Loss: 0.11665733, Physics Loss: 0.01129399, BC Loss: 0.10536334\n",
      "Iteration: 18620, Total Loss: 0.11638747, Physics Loss: 0.01108278, BC Loss: 0.10530469\n",
      "Iteration: 18630, Total Loss: 0.11627987, Physics Loss: 0.01115256, BC Loss: 0.10512730\n",
      "Iteration: 18640, Total Loss: 0.11620694, Physics Loss: 0.01100310, BC Loss: 0.10520384\n",
      "Iteration: 18650, Total Loss: 0.11617941, Physics Loss: 0.01085344, BC Loss: 0.10532597\n",
      "Iteration: 18660, Total Loss: 0.11609562, Physics Loss: 0.01067716, BC Loss: 0.10541846\n",
      "Iteration: 18670, Total Loss: 0.11587624, Physics Loss: 0.01114134, BC Loss: 0.10473490\n",
      "Iteration: 18680, Total Loss: 0.11556422, Physics Loss: 0.01167799, BC Loss: 0.10388622\n",
      "Iteration: 18690, Total Loss: 0.11534110, Physics Loss: 0.01099594, BC Loss: 0.10434516\n",
      "Iteration: 18700, Total Loss: 0.11513206, Physics Loss: 0.01128113, BC Loss: 0.10385093\n",
      "Iteration: 18710, Total Loss: 0.11497897, Physics Loss: 0.01145716, BC Loss: 0.10352181\n",
      "Iteration: 18720, Total Loss: 0.11483511, Physics Loss: 0.01119521, BC Loss: 0.10363990\n",
      "Iteration: 18730, Total Loss: 0.11469832, Physics Loss: 0.01109353, BC Loss: 0.10360479\n",
      "Iteration: 18740, Total Loss: 0.11460055, Physics Loss: 0.01124999, BC Loss: 0.10335056\n",
      "Iteration: 18750, Total Loss: 0.11450913, Physics Loss: 0.01129990, BC Loss: 0.10320923\n",
      "Iteration: 18760, Total Loss: 0.11422564, Physics Loss: 0.01099745, BC Loss: 0.10322819\n",
      "Iteration: 18770, Total Loss: 0.11406782, Physics Loss: 0.01096502, BC Loss: 0.10310280\n",
      "Iteration: 18780, Total Loss: 0.11398398, Physics Loss: 0.01110687, BC Loss: 0.10287711\n",
      "Iteration: 18790, Total Loss: 0.11392894, Physics Loss: 0.01116117, BC Loss: 0.10276777\n",
      "Iteration: 18800, Total Loss: 0.11386516, Physics Loss: 0.01109938, BC Loss: 0.10276578\n",
      "Iteration: 18810, Total Loss: 0.11378632, Physics Loss: 0.01101484, BC Loss: 0.10277148\n",
      "Iteration: 18820, Total Loss: 0.11368687, Physics Loss: 0.01104335, BC Loss: 0.10264352\n",
      "Iteration: 18830, Total Loss: 0.11362184, Physics Loss: 0.01106216, BC Loss: 0.10255969\n",
      "Iteration: 18840, Total Loss: 0.11355908, Physics Loss: 0.01121195, BC Loss: 0.10234714\n",
      "Iteration: 18850, Total Loss: 0.11346841, Physics Loss: 0.01132900, BC Loss: 0.10213941\n",
      "Iteration: 18860, Total Loss: 0.11339824, Physics Loss: 0.01137767, BC Loss: 0.10202056\n",
      "Iteration: 18870, Total Loss: 0.11331346, Physics Loss: 0.01133338, BC Loss: 0.10198008\n",
      "Iteration: 18880, Total Loss: 0.11322504, Physics Loss: 0.01120842, BC Loss: 0.10201663\n",
      "Iteration: 18890, Total Loss: 0.11307540, Physics Loss: 0.01092742, BC Loss: 0.10214798\n",
      "Iteration: 18900, Total Loss: 0.11298762, Physics Loss: 0.01090807, BC Loss: 0.10207954\n",
      "Iteration: 18910, Total Loss: 0.11288892, Physics Loss: 0.01120149, BC Loss: 0.10168743\n",
      "Iteration: 18920, Total Loss: 0.11273267, Physics Loss: 0.01152581, BC Loss: 0.10120686\n",
      "Iteration: 18930, Total Loss: 0.11262639, Physics Loss: 0.01145374, BC Loss: 0.10117266\n",
      "Iteration: 18940, Total Loss: 0.11249294, Physics Loss: 0.01147910, BC Loss: 0.10101384\n",
      "Iteration: 18950, Total Loss: 0.11242014, Physics Loss: 0.01141042, BC Loss: 0.10100973\n",
      "Iteration: 18960, Total Loss: 0.11236359, Physics Loss: 0.01139544, BC Loss: 0.10096815\n",
      "Iteration: 18970, Total Loss: 0.11225895, Physics Loss: 0.01142745, BC Loss: 0.10083150\n",
      "Iteration: 18980, Total Loss: 0.11207982, Physics Loss: 0.01112672, BC Loss: 0.10095310\n",
      "Iteration: 18990, Total Loss: 0.11198342, Physics Loss: 0.01112266, BC Loss: 0.10086076\n",
      "Iteration: 19000, Total Loss: 0.11193183, Physics Loss: 0.01117444, BC Loss: 0.10075739\n",
      "Iteration: 19010, Total Loss: 0.12530842, Physics Loss: 0.01310964, BC Loss: 0.11219878\n",
      "Iteration: 19020, Total Loss: 0.12423999, Physics Loss: 0.01248618, BC Loss: 0.11175381\n",
      "Iteration: 19030, Total Loss: 0.12405922, Physics Loss: 0.01255555, BC Loss: 0.11150368\n",
      "Iteration: 19040, Total Loss: 0.12397976, Physics Loss: 0.01224018, BC Loss: 0.11173958\n",
      "Iteration: 19050, Total Loss: 0.12384173, Physics Loss: 0.01193377, BC Loss: 0.11190796\n",
      "Iteration: 19060, Total Loss: 0.12358903, Physics Loss: 0.01216095, BC Loss: 0.11142807\n",
      "Iteration: 19070, Total Loss: 0.12334597, Physics Loss: 0.01208286, BC Loss: 0.11126311\n",
      "Iteration: 19080, Total Loss: 0.12322658, Physics Loss: 0.01184807, BC Loss: 0.11137851\n",
      "Iteration: 19090, Total Loss: 0.12298422, Physics Loss: 0.01169969, BC Loss: 0.11128452\n",
      "Iteration: 19100, Total Loss: 0.12273923, Physics Loss: 0.01180655, BC Loss: 0.11093268\n",
      "Iteration: 19110, Total Loss: 0.12260339, Physics Loss: 0.01166028, BC Loss: 0.11094311\n",
      "Iteration: 19120, Total Loss: 0.12238331, Physics Loss: 0.01171323, BC Loss: 0.11067009\n",
      "Iteration: 19130, Total Loss: 0.12228736, Physics Loss: 0.01200294, BC Loss: 0.11028442\n",
      "Iteration: 19140, Total Loss: 0.12216279, Physics Loss: 0.01225415, BC Loss: 0.10990863\n",
      "Iteration: 19150, Total Loss: 0.12205020, Physics Loss: 0.01202305, BC Loss: 0.11002714\n",
      "Iteration: 19160, Total Loss: 0.12179609, Physics Loss: 0.01197150, BC Loss: 0.10982459\n",
      "Iteration: 19170, Total Loss: 0.12162232, Physics Loss: 0.01224888, BC Loss: 0.10937344\n",
      "Iteration: 19180, Total Loss: 0.12155071, Physics Loss: 0.01223853, BC Loss: 0.10931218\n",
      "Iteration: 19190, Total Loss: 0.12145323, Physics Loss: 0.01211228, BC Loss: 0.10934095\n",
      "Iteration: 19200, Total Loss: 0.12136585, Physics Loss: 0.01204032, BC Loss: 0.10932553\n",
      "Iteration: 19210, Total Loss: 0.12128489, Physics Loss: 0.01206807, BC Loss: 0.10921681\n",
      "Iteration: 19220, Total Loss: 0.12117129, Physics Loss: 0.01222397, BC Loss: 0.10894732\n",
      "Iteration: 19230, Total Loss: 0.12100707, Physics Loss: 0.01221607, BC Loss: 0.10879099\n",
      "Iteration: 19240, Total Loss: 0.12089755, Physics Loss: 0.01223109, BC Loss: 0.10866645\n",
      "Iteration: 19250, Total Loss: 0.12077010, Physics Loss: 0.01228829, BC Loss: 0.10848181\n",
      "Iteration: 19260, Total Loss: 0.12057357, Physics Loss: 0.01239411, BC Loss: 0.10817946\n",
      "Iteration: 19270, Total Loss: 0.12034501, Physics Loss: 0.01225821, BC Loss: 0.10808680\n",
      "Iteration: 19280, Total Loss: 0.12009820, Physics Loss: 0.01197115, BC Loss: 0.10812706\n",
      "Iteration: 19290, Total Loss: 0.11994372, Physics Loss: 0.01171846, BC Loss: 0.10822526\n",
      "Iteration: 19300, Total Loss: 0.11983600, Physics Loss: 0.01177458, BC Loss: 0.10806142\n",
      "Iteration: 19310, Total Loss: 0.11977024, Physics Loss: 0.01185413, BC Loss: 0.10791612\n",
      "Iteration: 19320, Total Loss: 0.11970331, Physics Loss: 0.01189366, BC Loss: 0.10780964\n",
      "Iteration: 19330, Total Loss: 0.11960931, Physics Loss: 0.01205166, BC Loss: 0.10755765\n",
      "Iteration: 19340, Total Loss: 0.11953273, Physics Loss: 0.01216653, BC Loss: 0.10736620\n",
      "Iteration: 19350, Total Loss: 0.11946879, Physics Loss: 0.01216159, BC Loss: 0.10730720\n",
      "Iteration: 19360, Total Loss: 0.11938089, Physics Loss: 0.01221208, BC Loss: 0.10716881\n",
      "Iteration: 19370, Total Loss: 0.11926076, Physics Loss: 0.01186311, BC Loss: 0.10739765\n",
      "Iteration: 19380, Total Loss: 0.11917624, Physics Loss: 0.01181730, BC Loss: 0.10735894\n",
      "Iteration: 19390, Total Loss: 0.11909749, Physics Loss: 0.01186270, BC Loss: 0.10723478\n",
      "Iteration: 19400, Total Loss: 0.11902202, Physics Loss: 0.01168760, BC Loss: 0.10733441\n",
      "Iteration: 19410, Total Loss: 0.11890156, Physics Loss: 0.01149487, BC Loss: 0.10740669\n",
      "Iteration: 19420, Total Loss: 0.11874110, Physics Loss: 0.01148003, BC Loss: 0.10726106\n",
      "Iteration: 19430, Total Loss: 0.11860210, Physics Loss: 0.01155173, BC Loss: 0.10705037\n",
      "Iteration: 19440, Total Loss: 0.11850957, Physics Loss: 0.01129341, BC Loss: 0.10721616\n",
      "Iteration: 19450, Total Loss: 0.11844586, Physics Loss: 0.01106724, BC Loss: 0.10737862\n",
      "Iteration: 19460, Total Loss: 0.11841146, Physics Loss: 0.01107716, BC Loss: 0.10733430\n",
      "Iteration: 19470, Total Loss: 0.11836691, Physics Loss: 0.01108730, BC Loss: 0.10727961\n",
      "Iteration: 19480, Total Loss: 0.11829069, Physics Loss: 0.01117998, BC Loss: 0.10711071\n",
      "Iteration: 19490, Total Loss: 0.11822493, Physics Loss: 0.01122897, BC Loss: 0.10699596\n",
      "Iteration: 19500, Total Loss: 0.11817389, Physics Loss: 0.01117504, BC Loss: 0.10699885\n",
      "Iteration: 19510, Total Loss: 0.11813290, Physics Loss: 0.01114452, BC Loss: 0.10698839\n",
      "Iteration: 19520, Total Loss: 0.11811376, Physics Loss: 0.01116388, BC Loss: 0.10694988\n",
      "Iteration: 19530, Total Loss: 0.11805990, Physics Loss: 0.01130287, BC Loss: 0.10675704\n",
      "Iteration: 19540, Total Loss: 0.11796864, Physics Loss: 0.01133317, BC Loss: 0.10663547\n",
      "Iteration: 19550, Total Loss: 0.11792083, Physics Loss: 0.01130704, BC Loss: 0.10661379\n",
      "Iteration: 19560, Total Loss: 0.11786336, Physics Loss: 0.01139255, BC Loss: 0.10647081\n",
      "Iteration: 19570, Total Loss: 0.11779370, Physics Loss: 0.01147761, BC Loss: 0.10631609\n",
      "Iteration: 19580, Total Loss: 0.11770532, Physics Loss: 0.01134345, BC Loss: 0.10636187\n",
      "Iteration: 19590, Total Loss: 0.11759755, Physics Loss: 0.01149296, BC Loss: 0.10610460\n",
      "Iteration: 19600, Total Loss: 0.11752743, Physics Loss: 0.01169474, BC Loss: 0.10583268\n",
      "Iteration: 19610, Total Loss: 0.11747061, Physics Loss: 0.01168008, BC Loss: 0.10579053\n",
      "Iteration: 19620, Total Loss: 0.11736007, Physics Loss: 0.01130331, BC Loss: 0.10605676\n",
      "Iteration: 19630, Total Loss: 0.11724333, Physics Loss: 0.01118818, BC Loss: 0.10605516\n",
      "Iteration: 19640, Total Loss: 0.11711003, Physics Loss: 0.01116558, BC Loss: 0.10594445\n",
      "Iteration: 19650, Total Loss: 0.11693788, Physics Loss: 0.01126055, BC Loss: 0.10567734\n",
      "Iteration: 19660, Total Loss: 0.11689426, Physics Loss: 0.01127329, BC Loss: 0.10562097\n",
      "Iteration: 19670, Total Loss: 0.11681027, Physics Loss: 0.01109064, BC Loss: 0.10571963\n",
      "Iteration: 19680, Total Loss: 0.11670112, Physics Loss: 0.01081096, BC Loss: 0.10589015\n",
      "Iteration: 19690, Total Loss: 0.11659318, Physics Loss: 0.01080778, BC Loss: 0.10578540\n",
      "Iteration: 19700, Total Loss: 0.11651748, Physics Loss: 0.01088187, BC Loss: 0.10563561\n",
      "Iteration: 19710, Total Loss: 0.11638734, Physics Loss: 0.01093969, BC Loss: 0.10544765\n",
      "Iteration: 19720, Total Loss: 0.11626918, Physics Loss: 0.01096812, BC Loss: 0.10530106\n",
      "Iteration: 19730, Total Loss: 0.11621276, Physics Loss: 0.01072394, BC Loss: 0.10548882\n",
      "Iteration: 19740, Total Loss: 0.11618362, Physics Loss: 0.01060613, BC Loss: 0.10557749\n",
      "Iteration: 19750, Total Loss: 0.11615378, Physics Loss: 0.01059554, BC Loss: 0.10555823\n",
      "Iteration: 19760, Total Loss: 0.11611195, Physics Loss: 0.01070857, BC Loss: 0.10540338\n",
      "Iteration: 19770, Total Loss: 0.11607736, Physics Loss: 0.01073316, BC Loss: 0.10534419\n",
      "Iteration: 19780, Total Loss: 0.11599115, Physics Loss: 0.01051285, BC Loss: 0.10547829\n",
      "Iteration: 19790, Total Loss: 0.11594163, Physics Loss: 0.01032239, BC Loss: 0.10561924\n",
      "Iteration: 19800, Total Loss: 0.11588572, Physics Loss: 0.01016052, BC Loss: 0.10572521\n",
      "Iteration: 19810, Total Loss: 0.11583205, Physics Loss: 0.01014093, BC Loss: 0.10569113\n",
      "Iteration: 19820, Total Loss: 0.11576034, Physics Loss: 0.01024625, BC Loss: 0.10551409\n",
      "Iteration: 19830, Total Loss: 0.11568712, Physics Loss: 0.01024282, BC Loss: 0.10544429\n",
      "Iteration: 19840, Total Loss: 0.11560721, Physics Loss: 0.01037451, BC Loss: 0.10523270\n",
      "Iteration: 19850, Total Loss: 0.11555079, Physics Loss: 0.01043437, BC Loss: 0.10511641\n",
      "Iteration: 19860, Total Loss: 0.11549032, Physics Loss: 0.01047524, BC Loss: 0.10501508\n",
      "Iteration: 19870, Total Loss: 0.11541258, Physics Loss: 0.01055659, BC Loss: 0.10485598\n",
      "Iteration: 19880, Total Loss: 0.11535729, Physics Loss: 0.01037172, BC Loss: 0.10498556\n",
      "Iteration: 19890, Total Loss: 0.11532219, Physics Loss: 0.01013755, BC Loss: 0.10518464\n",
      "Iteration: 19900, Total Loss: 0.11527271, Physics Loss: 0.01005051, BC Loss: 0.10522220\n",
      "Iteration: 19910, Total Loss: 0.11521484, Physics Loss: 0.01012364, BC Loss: 0.10509120\n",
      "Iteration: 19920, Total Loss: 0.11512594, Physics Loss: 0.01016936, BC Loss: 0.10495658\n",
      "Iteration: 19930, Total Loss: 0.11506968, Physics Loss: 0.00985258, BC Loss: 0.10521710\n",
      "Iteration: 19940, Total Loss: 0.11501663, Physics Loss: 0.00980153, BC Loss: 0.10521510\n",
      "Iteration: 19950, Total Loss: 0.11496908, Physics Loss: 0.00990991, BC Loss: 0.10505918\n",
      "Iteration: 19960, Total Loss: 0.11491134, Physics Loss: 0.00993889, BC Loss: 0.10497245\n",
      "Iteration: 19970, Total Loss: 0.11486384, Physics Loss: 0.01000365, BC Loss: 0.10486019\n",
      "Iteration: 19980, Total Loss: 0.11474623, Physics Loss: 0.00992173, BC Loss: 0.10482449\n",
      "Iteration: 19990, Total Loss: 0.11472704, Physics Loss: 0.00986399, BC Loss: 0.10486306\n",
      "Iteration: 20000, Total Loss: 0.11470681, Physics Loss: 0.00978108, BC Loss: 0.10492573\n",
      "Iteration: 20010, Total Loss: 0.13229823, Physics Loss: 0.01916366, BC Loss: 0.11313457\n",
      "Iteration: 20020, Total Loss: 0.12843013, Physics Loss: 0.01471755, BC Loss: 0.11371259\n",
      "Iteration: 20030, Total Loss: 0.12792565, Physics Loss: 0.01369862, BC Loss: 0.11422703\n",
      "Iteration: 20040, Total Loss: 0.12770905, Physics Loss: 0.01347041, BC Loss: 0.11423863\n",
      "Iteration: 20050, Total Loss: 0.12743647, Physics Loss: 0.01310415, BC Loss: 0.11433233\n",
      "Iteration: 20060, Total Loss: 0.12721851, Physics Loss: 0.01308933, BC Loss: 0.11412919\n",
      "Iteration: 20070, Total Loss: 0.12700750, Physics Loss: 0.01320788, BC Loss: 0.11379962\n",
      "Iteration: 20080, Total Loss: 0.12673967, Physics Loss: 0.01284048, BC Loss: 0.11389919\n",
      "Iteration: 20090, Total Loss: 0.12655224, Physics Loss: 0.01259332, BC Loss: 0.11395893\n",
      "Iteration: 20100, Total Loss: 0.12629850, Physics Loss: 0.01213583, BC Loss: 0.11416267\n",
      "Iteration: 20110, Total Loss: 0.12612620, Physics Loss: 0.01207522, BC Loss: 0.11405098\n",
      "Iteration: 20120, Total Loss: 0.12598678, Physics Loss: 0.01205534, BC Loss: 0.11393145\n",
      "Iteration: 20130, Total Loss: 0.12584937, Physics Loss: 0.01194364, BC Loss: 0.11390573\n",
      "Iteration: 20140, Total Loss: 0.12572671, Physics Loss: 0.01174732, BC Loss: 0.11397940\n",
      "Iteration: 20150, Total Loss: 0.12556994, Physics Loss: 0.01153892, BC Loss: 0.11403102\n",
      "Iteration: 20160, Total Loss: 0.12547794, Physics Loss: 0.01139161, BC Loss: 0.11408633\n",
      "Iteration: 20170, Total Loss: 0.12537527, Physics Loss: 0.01134376, BC Loss: 0.11403151\n",
      "Iteration: 20180, Total Loss: 0.12523425, Physics Loss: 0.01122543, BC Loss: 0.11400881\n",
      "Iteration: 20190, Total Loss: 0.12514122, Physics Loss: 0.01125284, BC Loss: 0.11388838\n",
      "Iteration: 20200, Total Loss: 0.12507144, Physics Loss: 0.01129624, BC Loss: 0.11377520\n",
      "Iteration: 20210, Total Loss: 0.12500587, Physics Loss: 0.01123393, BC Loss: 0.11377195\n",
      "Iteration: 20220, Total Loss: 0.12494673, Physics Loss: 0.01104964, BC Loss: 0.11389709\n",
      "Iteration: 20230, Total Loss: 0.12489450, Physics Loss: 0.01086781, BC Loss: 0.11402670\n",
      "Iteration: 20240, Total Loss: 0.12484138, Physics Loss: 0.01083439, BC Loss: 0.11400699\n",
      "Iteration: 20250, Total Loss: 0.12478510, Physics Loss: 0.01079614, BC Loss: 0.11398896\n",
      "Iteration: 20260, Total Loss: 0.12469097, Physics Loss: 0.01064493, BC Loss: 0.11404604\n",
      "Iteration: 20270, Total Loss: 0.12454445, Physics Loss: 0.01047730, BC Loss: 0.11406715\n",
      "Iteration: 20280, Total Loss: 0.12446422, Physics Loss: 0.01034361, BC Loss: 0.11412062\n",
      "Iteration: 20290, Total Loss: 0.12434942, Physics Loss: 0.01011014, BC Loss: 0.11423928\n",
      "Iteration: 20300, Total Loss: 0.12419202, Physics Loss: 0.01001162, BC Loss: 0.11418041\n",
      "Iteration: 20310, Total Loss: 0.12407109, Physics Loss: 0.01008978, BC Loss: 0.11398131\n",
      "Iteration: 20320, Total Loss: 0.12393454, Physics Loss: 0.01006277, BC Loss: 0.11387176\n",
      "Iteration: 20330, Total Loss: 0.12385456, Physics Loss: 0.00987273, BC Loss: 0.11398184\n",
      "Iteration: 20340, Total Loss: 0.12369189, Physics Loss: 0.00977799, BC Loss: 0.11391391\n",
      "Iteration: 20350, Total Loss: 0.12360991, Physics Loss: 0.00967928, BC Loss: 0.11393063\n",
      "Iteration: 20360, Total Loss: 0.12356459, Physics Loss: 0.00962889, BC Loss: 0.11393569\n",
      "Iteration: 20370, Total Loss: 0.12349445, Physics Loss: 0.00948345, BC Loss: 0.11401100\n",
      "Iteration: 20380, Total Loss: 0.12342469, Physics Loss: 0.00921068, BC Loss: 0.11421402\n",
      "Iteration: 20390, Total Loss: 0.12338518, Physics Loss: 0.00917318, BC Loss: 0.11421201\n",
      "Iteration: 20400, Total Loss: 0.12335150, Physics Loss: 0.00921057, BC Loss: 0.11414093\n",
      "Iteration: 20410, Total Loss: 0.12328240, Physics Loss: 0.00916485, BC Loss: 0.11411755\n",
      "Iteration: 20420, Total Loss: 0.12311609, Physics Loss: 0.00911140, BC Loss: 0.11400469\n",
      "Iteration: 20430, Total Loss: 0.12300864, Physics Loss: 0.00916398, BC Loss: 0.11384466\n",
      "Iteration: 20440, Total Loss: 0.12297319, Physics Loss: 0.00918914, BC Loss: 0.11378404\n",
      "Iteration: 20450, Total Loss: 0.12292122, Physics Loss: 0.00909440, BC Loss: 0.11382683\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 226\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m开始训练PINN模型...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     \u001b[43mpinn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 减少epoch数以加快训练\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m训练完成!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[11], line 203\u001b[0m, in \u001b[0;36mPINN_NavierStokes.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclosure_wrapper\u001b[39m():\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure(x_col, y_col, t_col, x_bc, y_bc, t_bc)\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_wrapper\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/optim/lbfgs.py:444\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_directional_evaluate(closure, x, t, d)\n\u001b[0;32m--> 444\u001b[0m     loss, flat_grad, t, ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[43m_strong_wolfe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtd\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[1;32m    448\u001b[0m opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/optim/lbfgs.py:92\u001b[0m, in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     90\u001b[0m g_prev \u001b[38;5;241m=\u001b[39m g_new\u001b[38;5;241m.\u001b[39mclone(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n\u001b[1;32m     91\u001b[0m gtd_prev \u001b[38;5;241m=\u001b[39m gtd_new\n\u001b[0;32m---> 92\u001b[0m f_new, g_new \u001b[38;5;241m=\u001b[39m \u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m ls_func_evals \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     94\u001b[0m gtd_new \u001b[38;5;241m=\u001b[39m g_new\u001b[38;5;241m.\u001b[39mdot(d)\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/optim/lbfgs.py:442\u001b[0m, in \u001b[0;36mLBFGS.step.<locals>.obj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[0;32m--> 442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_directional_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/optim/lbfgs.py:296\u001b[0m, in \u001b[0;36mLBFGS._directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_directional_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure, x, t, d):\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[0;32m--> 296\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    297\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_param(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 201\u001b[0m, in \u001b[0;36mPINN_NavierStokes.train.<locals>.closure_wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclosure_wrapper\u001b[39m():\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_bc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_bc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_bc\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 153\u001b[0m, in \u001b[0;36mPINN_NavierStokes.closure\u001b[0;34m(self, x_col, y_col, t_col, x_bc, y_bc, t_bc)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# 总损失\u001b[39;00m\n\u001b[1;32m    151\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m physics_loss \u001b[38;5;241m+\u001b[39m bc_loss\n\u001b[0;32m--> 153\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# 物理参数\n",
    "nu = 0.01  # 运动粘度\n",
    "rho = 1.0  # 密度\n",
    "inlet_velocity = 1.0  # 入口速度\n",
    "\n",
    "class PINN_NavierStokes():\n",
    "    def __init__(self, layers, cylinder_center=(0.5, 0.2), cylinder_radius=0.05):\n",
    "        # 网络结构\n",
    "        self.layers = layers\n",
    "        self.cylinder_center = cylinder_center\n",
    "        self.cylinder_radius = cylinder_radius\n",
    "        \n",
    "        # 初始化网络\n",
    "        self.network()\n",
    "        \n",
    "        # 优化器\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.net.parameters(), \n",
    "            lr=0.1, \n",
    "            max_iter=1000, \n",
    "            max_eval=1000,\n",
    "            history_size=50, \n",
    "            tolerance_grad=1e-07, \n",
    "            tolerance_change=1e-09,\n",
    "            line_search_fn=\"strong_wolfe\"\n",
    "        )\n",
    "        \n",
    "        self.mse = nn.MSELoss()\n",
    "        self.iter = 0\n",
    "\n",
    "    def network(self):\n",
    "        \"\"\"构建神经网络\"\"\"\n",
    "        layers_list = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            layer = nn.Linear(self.layers[i], self.layers[i+1])\n",
    "            # 初始化权重\n",
    "            if i < len(self.layers) - 2:\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "            else:\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "            layers_list.append(layer)\n",
    "            if i < len(self.layers) - 2:  # 最后一层不需要激活函数\n",
    "                layers_list.append(nn.Tanh())\n",
    "        \n",
    "        self.net = nn.Sequential(*layers_list)\n",
    "\n",
    "    def governing_equations(self, x, y, t):\n",
    "        \"\"\"计算Navier-Stokes方程的残差\"\"\"\n",
    "        # 确保输入张量需要梯度\n",
    "        x = x.requires_grad_(True)\n",
    "        y = y.requires_grad_(True)\n",
    "        t = t.requires_grad_(True)\n",
    "        \n",
    "        # 网络输出: [u, v, p] (速度分量和压力)\n",
    "        input_tensor = torch.cat([x, y, t], dim=1)\n",
    "        output = self.net(input_tensor)\n",
    "        u, v, p = output[:, 0:1], output[:, 1:2], output[:, 2:3]\n",
    "        \n",
    "        # 计算速度的偏导数\n",
    "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "        u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y), create_graph=True)[0]\n",
    "        \n",
    "        v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "        v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "        v_t = torch.autograd.grad(v, t, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "        v_xx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(v_x), create_graph=True)[0]\n",
    "        v_yy = torch.autograd.grad(v_y, y, grad_outputs=torch.ones_like(v_y), create_graph=True)[0]\n",
    "        \n",
    "        # 压力梯度\n",
    "        p_x = torch.autograd.grad(p, x, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "        p_y = torch.autograd.grad(p, y, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "        \n",
    "        # Navier-Stokes方程残差\n",
    "        f = u_t + u * u_x + v * u_y + (1.0/rho) * p_x - nu * (u_xx + u_yy)\n",
    "        g = v_t + u * v_x + v * v_y + (1.0/rho) * p_y - nu * (v_xx + v_yy)\n",
    "        \n",
    "        # 连续性方程残差 (质量守恒)\n",
    "        continuity = u_x + v_y\n",
    "        \n",
    "        return u, v, p, f, g, continuity\n",
    "\n",
    "    def boundary_conditions(self, x, y, t):\n",
    "        \"\"\"计算边界条件损失\"\"\"\n",
    "        # 确保输入张量需要梯度\n",
    "        x = x.requires_grad_(True)\n",
    "        y = y.requires_grad_(True)\n",
    "        t = t.requires_grad_(True)\n",
    "        \n",
    "        # 网络输出\n",
    "        input_tensor = torch.cat([x, y, t], dim=1)\n",
    "        output = self.net(input_tensor)\n",
    "        u, v, p = output[:, 0:1], output[:, 1:2], output[:, 2:3]\n",
    "        \n",
    "        # 计算到圆柱中心的距离\n",
    "        dist_to_cylinder = torch.sqrt((x - self.cylinder_center[0])**2 + (y - self.cylinder_center[1])**2)\n",
    "        cylinder_mask = dist_to_cylinder <= self.cylinder_radius + 0.005\n",
    "        \n",
    "        # 入口边界 (x=0)\n",
    "        inlet_mask = torch.abs(x) < 0.005\n",
    "        \n",
    "        # 出口边界 (x=1) - 自然边界条件，压力梯度为0\n",
    "        outlet_mask = torch.abs(x - 1.0) < 0.005\n",
    "        \n",
    "        # 上下边界 (y=0, y=0.4) - 无滑移条件\n",
    "        wall_mask = (torch.abs(y) < 0.005) | (torch.abs(y - 0.4) < 0.005)\n",
    "        \n",
    "        # 计算边界损失\n",
    "        bc_loss = 0.0\n",
    "        \n",
    "        # 圆柱边界: 无滑移条件\n",
    "        if cylinder_mask.any():\n",
    "            u_cyl = u[cylinder_mask]\n",
    "            v_cyl = v[cylinder_mask]\n",
    "            bc_loss += torch.mean(u_cyl**2) + torch.mean(v_cyl**2)\n",
    "        \n",
    "        # 入口边界: 指定入口速度\n",
    "        if inlet_mask.any():\n",
    "            u_inlet = u[inlet_mask]\n",
    "            v_inlet = v[inlet_mask]\n",
    "            bc_loss += torch.mean((u_inlet - inlet_velocity)**2) + torch.mean(v_inlet**2)\n",
    "        \n",
    "        # 壁面边界: 无滑移条件\n",
    "        if wall_mask.any():\n",
    "            u_wall = u[wall_mask]\n",
    "            v_wall = v[wall_mask]\n",
    "            bc_loss += torch.mean(u_wall**2) + torch.mean(v_wall**2)\n",
    "            \n",
    "        return bc_loss\n",
    "\n",
    "    def closure(self, x_col, y_col, t_col, x_bc, y_bc, t_bc):\n",
    "        \"\"\"损失函数\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # 物理方程损失 (残差)\n",
    "        _, _, _, f, g, continuity = self.governing_equations(x_col, y_col, t_col)\n",
    "        physics_loss = torch.mean(f**2) + torch.mean(g**2) + torch.mean(continuity**2)\n",
    "        \n",
    "        # 边界条件损失\n",
    "        bc_loss = self.boundary_conditions(x_bc, y_bc, t_bc)\n",
    "        \n",
    "        # 总损失\n",
    "        total_loss = physics_loss + bc_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        \n",
    "        self.iter += 1\n",
    "        if self.iter % 10 == 0:\n",
    "            print(f'Iteration: {self.iter}, Total Loss: {total_loss.item():.8f}, '\n",
    "                  f'Physics Loss: {physics_loss.item():.8f}, BC Loss: {bc_loss.item():.8f}')\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    def train(self, epochs=100):\n",
    "        \"\"\"训练函数\"\"\"\n",
    "        self.net.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # 生成训练点\n",
    "            # 物理域内的点 (用于物理方程约束)\n",
    "            n_col = 500  # 配置点数量\n",
    "            x_col = torch.rand(n_col, 1) * 1.0  # x: 0 to 1\n",
    "            y_col = torch.rand(n_col, 1) * 0.4  # y: 0 to 0.4\n",
    "            t_col = torch.rand(n_col, 1) * 0.5  # t: 0 to 0.5\n",
    "            \n",
    "            # 边界点 (用于边界条件约束)\n",
    "            n_bc = 400\n",
    "            # 圆柱边界采样\n",
    "            theta = torch.rand(n_bc//4, 1) * 2 * np.pi\n",
    "            x_cyl = self.cylinder_center[0] + self.cylinder_radius * torch.cos(theta)\n",
    "            y_cyl = self.cylinder_center[1] + self.cylinder_radius * torch.sin(theta)\n",
    "            \n",
    "            # 入口边界 (x=0)\n",
    "            y_inlet = torch.rand(n_bc//4, 1) * 0.4\n",
    "            x_inlet = torch.zeros_like(y_inlet)\n",
    "            \n",
    "            # 出口边界 (x=1)\n",
    "            y_outlet = torch.rand(n_bc//4, 1) * 0.4\n",
    "            x_outlet = torch.ones_like(y_outlet)\n",
    "            \n",
    "            # 上下边界\n",
    "            x_wall = torch.rand(n_bc//2, 1)\n",
    "            y_wall_top = torch.ones(n_bc//4, 1) * 0.4\n",
    "            y_wall_bottom = torch.zeros(n_bc//4, 1)\n",
    "            \n",
    "            # 合并边界点\n",
    "            x_bc = torch.cat([x_cyl, x_inlet, x_outlet, x_wall[:n_bc//4], x_wall[n_bc//4:]], dim=0)\n",
    "            y_bc = torch.cat([y_cyl, y_inlet, y_outlet, y_wall_top, y_wall_bottom], dim=0)\n",
    "            t_bc = torch.rand_like(x_bc) * 0.5\n",
    "            \n",
    "            # 调用优化器\n",
    "            def closure_wrapper():\n",
    "                return self.closure(x_col, y_col, t_col, x_bc, y_bc, t_bc)\n",
    "            \n",
    "            self.optimizer.step(closure_wrapper)\n",
    "\n",
    "    def predict(self, x, y, t):\n",
    "        \"\"\"预测速度场和压力场\"\"\"\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            x_tensor = torch.tensor(x, dtype=torch.float32).reshape(-1, 1)\n",
    "            y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "            t_tensor = torch.tensor(t, dtype=torch.float32).reshape(-1, 1)\n",
    "            \n",
    "            input_tensor = torch.cat([x_tensor, y_tensor, t_tensor], dim=1)\n",
    "            output = self.net(input_tensor)\n",
    "            u, v, p = output[:, 0:1], output[:, 1:2], output[:, 2:3]\n",
    "            \n",
    "        return u.numpy(), v.numpy(), p.numpy()\n",
    "\n",
    "# 创建PINN模型\n",
    "layers = [3, 50, 50, 50, 50, 50, 3]  # [输入: x,y,t -> 输出: u, v, p]\n",
    "pinn = PINN_NavierStokes(layers)\n",
    "\n",
    "# 训练模型\n",
    "print(\"开始训练PINN模型...\")\n",
    "try:\n",
    "    pinn.train(epochs=50)  # 减少epoch数以加快训练\n",
    "    print(\"训练完成!\")\n",
    "except Exception as e:\n",
    "    print(f\"训练过程中出现错误: {e}\")\n",
    "\n",
    "# 创建网格用于可视化\n",
    "x_range = np.linspace(0, 1, 100)\n",
    "y_range = np.linspace(0, 0.4, 50)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "x_flat = X.flatten()\n",
    "y_flat = Y.flatten()\n",
    "\n",
    "# 静态可视化 - 速度流线图\n",
    "def plot_flow_field():\n",
    "    fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 计算t=0.25时刻的场\n",
    "    t_static = np.ones_like(x_flat) * 0.25\n",
    "    try:\n",
    "        u_static, v_static, p_static = pinn.predict(x_flat, y_flat, t_static)\n",
    "        \n",
    "        U_static = u_static.reshape(X.shape)\n",
    "        V_static = v_static.reshape(Y.shape)\n",
    "        P_static = p_static.reshape(X.shape)\n",
    "        \n",
    "        # 左图：压力场\n",
    "        contour1 = ax1.contourf(X, Y, P_static, levels=50, cmap='viridis')\n",
    "        ax1.streamplot(X, Y, U_static.reshape(X.shape), V_static.reshape(Y.shape), \n",
    "                      color='white', density=1.5, linewidth=0.8, arrowsize=1)\n",
    "        cylinder1 = plt.Circle(pinn.cylinder_center, pinn.cylinder_radius, color='red', zorder=5)\n",
    "        ax1.add_patch(cylinder1)\n",
    "        ax1.set_xlabel('x')\n",
    "        ax1.set_ylabel('y')\n",
    "        ax1.set_title('Pressure Field with Velocity Streamlines')\n",
    "        plt.colorbar(contour1, ax=ax1)\n",
    "        \n",
    "        # 右图：速度幅值\n",
    "        speed = np.sqrt(U_static**2 + V_static**2)\n",
    "        contour2 = ax2.contourf(X, Y, speed, levels=50, cmap='plasma')\n",
    "        ax2.streamplot(X, Y, U_static, V_static, color='white', density=1.5, linewidth=0.8, arrowsize=1)\n",
    "        cylinder2 = plt.Circle(pinn.cylinder_center, pinn.cylinder_radius, color='red', zorder=5)\n",
    "        ax2.add_patch(cylinder2)\n",
    "        ax2.set_xlabel('x')\n",
    "        ax2.set_ylabel('y')\n",
    "        ax2.set_title('Velocity Magnitude with Streamlines')\n",
    "        plt.colorbar(contour2, ax=ax2)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"可视化过程中出现错误: {e}\")\n",
    "        print(\"这可能是因为模型训练不充分或网络输出不稳定\")\n",
    "\n",
    "plot_flow_field()\n",
    "\n",
    "# 如果训练成功，可以尝试动画\n",
    "try:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    def animate(t_frame):\n",
    "        ax.clear()\n",
    "        \n",
    "        # 计算当前时间步的速度场和压力场\n",
    "        t_flat = np.ones_like(x_flat) * t_frame * 0.05  # 时间从0到2.5变化\n",
    "        \n",
    "        u_pred, v_pred, p_pred = pinn.predict(x_flat, y_flat, t_flat)\n",
    "        \n",
    "        # 重塑为网格形式\n",
    "        U = u_pred.reshape(X.shape)\n",
    "        V = v_pred.reshape(Y.shape)\n",
    "        P = p_pred.reshape(X.shape)\n",
    "        \n",
    "        # 绘制压力场\n",
    "        contour = ax.contourf(X, Y, P, levels=50, cmap='viridis')\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('y')\n",
    "        ax.set_title(f'Pressure Field at t={t_frame*0.05:.2f}')\n",
    "        \n",
    "        # 绘制圆柱\n",
    "        cylinder = plt.Circle(pinn.cylinder_center, pinn.cylinder_radius, color='red', zorder=5)\n",
    "        ax.add_patch(cylinder)\n",
    "        \n",
    "        # 可选：绘制速度矢量\n",
    "        skip = 5  # 每5个点绘制一个矢量\n",
    "        ax.quiver(X[::skip, ::skip], Y[::skip, ::skip], \n",
    "                  U[::skip, ::skip], V[::skip, ::skip], \n",
    "                  alpha=0.5, scale=5, color='white', width=0.002)\n",
    "        \n",
    "        if t_frame == 0:  # 只在第一帧添加颜色条\n",
    "            plt.colorbar(contour, ax=ax)\n",
    "    \n",
    "    # 创建动画\n",
    "    ani = animation.FuncAnimation(fig, animate, frames=30, interval=300, repeat=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"动画创建失败: {e}\")\n",
    "    print(\"可能是由于模型训练不充分导致的数值不稳定\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bfe478d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda:0\n",
      "开始训练PINN模型...\n",
      "Iteration: 10, Total Loss: 0.52304798, Physics Loss: 0.02019024, BC Loss: 0.50285774\n",
      "Iteration: 20, Total Loss: 0.36451483, Physics Loss: 0.03372309, BC Loss: 0.33079174\n",
      "Iteration: 30, Total Loss: 0.31367686, Physics Loss: 0.01806405, BC Loss: 0.29561281\n",
      "Iteration: 40, Total Loss: 0.29163140, Physics Loss: 0.02233054, BC Loss: 0.26930088\n",
      "Iteration: 50, Total Loss: 0.27845600, Physics Loss: 0.01527265, BC Loss: 0.26318336\n",
      "Iteration: 60, Total Loss: 0.26715177, Physics Loss: 0.01815578, BC Loss: 0.24899599\n",
      "Iteration: 70, Total Loss: 0.26024067, Physics Loss: 0.01577677, BC Loss: 0.24446389\n",
      "Iteration: 80, Total Loss: 0.25624654, Physics Loss: 0.01079700, BC Loss: 0.24544954\n",
      "Iteration: 90, Total Loss: 0.25445306, Physics Loss: 0.00972525, BC Loss: 0.24472781\n",
      "Iteration: 100, Total Loss: 0.25079358, Physics Loss: 0.01301021, BC Loss: 0.23778337\n",
      "Iteration: 110, Total Loss: 0.24709977, Physics Loss: 0.01122035, BC Loss: 0.23587942\n",
      "Iteration: 120, Total Loss: 0.24654210, Physics Loss: 0.01151434, BC Loss: 0.23502776\n",
      "Iteration: 130, Total Loss: 0.24577019, Physics Loss: 0.01302597, BC Loss: 0.23274422\n",
      "Iteration: 140, Total Loss: 0.24321505, Physics Loss: 0.01484927, BC Loss: 0.22836578\n",
      "Iteration: 150, Total Loss: 0.23907235, Physics Loss: 0.01326422, BC Loss: 0.22580813\n",
      "Iteration: 160, Total Loss: 0.23349212, Physics Loss: 0.01405226, BC Loss: 0.21943986\n",
      "Iteration: 170, Total Loss: 0.22774045, Physics Loss: 0.01696132, BC Loss: 0.21077913\n",
      "Iteration: 180, Total Loss: 0.22443326, Physics Loss: 0.02140899, BC Loss: 0.20302427\n",
      "Iteration: 190, Total Loss: 0.21871552, Physics Loss: 0.01939367, BC Loss: 0.19932185\n",
      "Iteration: 200, Total Loss: 0.21091136, Physics Loss: 0.02320398, BC Loss: 0.18770738\n",
      "Iteration: 210, Total Loss: 0.20120391, Physics Loss: 0.02621194, BC Loss: 0.17499197\n",
      "Iteration: 220, Total Loss: 0.19528809, Physics Loss: 0.02361280, BC Loss: 0.17167529\n",
      "Iteration: 230, Total Loss: 0.18775401, Physics Loss: 0.02119906, BC Loss: 0.16655494\n",
      "Iteration: 240, Total Loss: 0.18397185, Physics Loss: 0.02060142, BC Loss: 0.16337043\n",
      "Iteration: 250, Total Loss: 0.17924881, Physics Loss: 0.01859245, BC Loss: 0.16065636\n",
      "Iteration: 260, Total Loss: 0.17574728, Physics Loss: 0.01779253, BC Loss: 0.15795475\n",
      "Iteration: 270, Total Loss: 0.17260760, Physics Loss: 0.01833031, BC Loss: 0.15427729\n",
      "Iteration: 280, Total Loss: 0.16839434, Physics Loss: 0.01824005, BC Loss: 0.15015429\n",
      "Iteration: 290, Total Loss: 0.16627115, Physics Loss: 0.01595304, BC Loss: 0.15031810\n",
      "Iteration: 300, Total Loss: 0.16517247, Physics Loss: 0.01564661, BC Loss: 0.14952587\n",
      "Iteration: 310, Total Loss: 0.16251436, Physics Loss: 0.01393942, BC Loss: 0.14857495\n",
      "Iteration: 320, Total Loss: 0.16123390, Physics Loss: 0.01408613, BC Loss: 0.14714777\n",
      "Iteration: 330, Total Loss: 0.16000932, Physics Loss: 0.01379345, BC Loss: 0.14621587\n",
      "Iteration: 340, Total Loss: 0.15881553, Physics Loss: 0.01439094, BC Loss: 0.14442459\n",
      "Iteration: 350, Total Loss: 0.15718395, Physics Loss: 0.01538998, BC Loss: 0.14179397\n",
      "Iteration: 360, Total Loss: 0.15418199, Physics Loss: 0.01426240, BC Loss: 0.13991958\n",
      "Iteration: 370, Total Loss: 0.15359978, Physics Loss: 0.01319729, BC Loss: 0.14040250\n",
      "Iteration: 380, Total Loss: 0.15110263, Physics Loss: 0.01357081, BC Loss: 0.13753182\n",
      "Iteration: 390, Total Loss: 0.14902239, Physics Loss: 0.01263543, BC Loss: 0.13638696\n",
      "Iteration: 400, Total Loss: 0.14738661, Physics Loss: 0.01189553, BC Loss: 0.13549107\n",
      "Iteration: 410, Total Loss: 0.14620259, Physics Loss: 0.01211134, BC Loss: 0.13409126\n",
      "Iteration: 420, Total Loss: 0.14497942, Physics Loss: 0.01226985, BC Loss: 0.13270956\n",
      "Iteration: 430, Total Loss: 0.14338008, Physics Loss: 0.01224482, BC Loss: 0.13113526\n",
      "Iteration: 440, Total Loss: 0.14126199, Physics Loss: 0.01183888, BC Loss: 0.12942311\n",
      "Iteration: 450, Total Loss: 0.13981055, Physics Loss: 0.01201315, BC Loss: 0.12779739\n",
      "Iteration: 460, Total Loss: 0.13852639, Physics Loss: 0.01028825, BC Loss: 0.12823814\n",
      "Iteration: 470, Total Loss: 0.13743685, Physics Loss: 0.01069946, BC Loss: 0.12673739\n",
      "Iteration: 480, Total Loss: 0.13550901, Physics Loss: 0.01159687, BC Loss: 0.12391215\n",
      "Iteration: 490, Total Loss: 0.13452446, Physics Loss: 0.01136268, BC Loss: 0.12316178\n",
      "Iteration: 500, Total Loss: 0.13276112, Physics Loss: 0.01275868, BC Loss: 0.12000244\n",
      "Iteration: 510, Total Loss: 0.13114223, Physics Loss: 0.01232945, BC Loss: 0.11881278\n",
      "Iteration: 520, Total Loss: 0.13019313, Physics Loss: 0.01222724, BC Loss: 0.11796589\n",
      "Iteration: 530, Total Loss: 0.12913682, Physics Loss: 0.01175579, BC Loss: 0.11738102\n",
      "Iteration: 540, Total Loss: 0.12843040, Physics Loss: 0.01189349, BC Loss: 0.11653690\n",
      "Iteration: 550, Total Loss: 0.12758604, Physics Loss: 0.01135422, BC Loss: 0.11623182\n",
      "Iteration: 560, Total Loss: 0.12664533, Physics Loss: 0.01113608, BC Loss: 0.11550925\n",
      "Iteration: 570, Total Loss: 0.12550858, Physics Loss: 0.01230883, BC Loss: 0.11319975\n",
      "Iteration: 580, Total Loss: 0.12479656, Physics Loss: 0.01233643, BC Loss: 0.11246014\n",
      "Iteration: 590, Total Loss: 0.12341419, Physics Loss: 0.01130056, BC Loss: 0.11211362\n",
      "Iteration: 600, Total Loss: 0.12190250, Physics Loss: 0.01112839, BC Loss: 0.11077411\n",
      "Iteration: 610, Total Loss: 0.12132204, Physics Loss: 0.01099875, BC Loss: 0.11032328\n",
      "Iteration: 620, Total Loss: 0.12040985, Physics Loss: 0.01061899, BC Loss: 0.10979086\n",
      "Iteration: 630, Total Loss: 0.11981919, Physics Loss: 0.01098540, BC Loss: 0.10883379\n",
      "Iteration: 640, Total Loss: 0.11885503, Physics Loss: 0.01040095, BC Loss: 0.10845408\n",
      "Iteration: 650, Total Loss: 0.11839534, Physics Loss: 0.01022810, BC Loss: 0.10816724\n",
      "Iteration: 660, Total Loss: 0.11776911, Physics Loss: 0.01079737, BC Loss: 0.10697174\n",
      "Iteration: 670, Total Loss: 0.11666125, Physics Loss: 0.01028797, BC Loss: 0.10637328\n",
      "Iteration: 680, Total Loss: 0.11574242, Physics Loss: 0.01043384, BC Loss: 0.10530858\n",
      "Iteration: 690, Total Loss: 0.11457697, Physics Loss: 0.00994254, BC Loss: 0.10463443\n",
      "Iteration: 700, Total Loss: 0.11401263, Physics Loss: 0.00947809, BC Loss: 0.10453454\n",
      "Iteration: 710, Total Loss: 0.11359835, Physics Loss: 0.00916425, BC Loss: 0.10443410\n",
      "Iteration: 720, Total Loss: 0.11301634, Physics Loss: 0.00929816, BC Loss: 0.10371818\n",
      "Iteration: 730, Total Loss: 0.11246520, Physics Loss: 0.00887024, BC Loss: 0.10359496\n",
      "Iteration: 740, Total Loss: 0.11173350, Physics Loss: 0.00885938, BC Loss: 0.10287412\n",
      "Iteration: 750, Total Loss: 0.11124977, Physics Loss: 0.00897924, BC Loss: 0.10227052\n",
      "Iteration: 760, Total Loss: 0.11092358, Physics Loss: 0.00890408, BC Loss: 0.10201950\n",
      "Iteration: 770, Total Loss: 0.11021472, Physics Loss: 0.00851490, BC Loss: 0.10169981\n",
      "Iteration: 780, Total Loss: 0.10994162, Physics Loss: 0.00843091, BC Loss: 0.10151071\n",
      "Iteration: 790, Total Loss: 0.10928804, Physics Loss: 0.00865284, BC Loss: 0.10063519\n",
      "Iteration: 800, Total Loss: 0.10893326, Physics Loss: 0.00825601, BC Loss: 0.10067725\n",
      "Iteration: 810, Total Loss: 0.10867662, Physics Loss: 0.00834411, BC Loss: 0.10033251\n",
      "Iteration: 820, Total Loss: 0.10833486, Physics Loss: 0.00834787, BC Loss: 0.09998699\n",
      "Iteration: 830, Total Loss: 0.10801873, Physics Loss: 0.00845549, BC Loss: 0.09956325\n",
      "Iteration: 840, Total Loss: 0.10763276, Physics Loss: 0.00782665, BC Loss: 0.09980612\n",
      "Iteration: 850, Total Loss: 0.10736236, Physics Loss: 0.00804604, BC Loss: 0.09931632\n",
      "Iteration: 860, Total Loss: 0.10719682, Physics Loss: 0.00809779, BC Loss: 0.09909903\n",
      "Iteration: 870, Total Loss: 0.10689582, Physics Loss: 0.00810067, BC Loss: 0.09879515\n",
      "Iteration: 880, Total Loss: 0.10654159, Physics Loss: 0.00813950, BC Loss: 0.09840209\n",
      "Iteration: 890, Total Loss: 0.10620712, Physics Loss: 0.00845787, BC Loss: 0.09774925\n",
      "Iteration: 900, Total Loss: 0.10596641, Physics Loss: 0.00855599, BC Loss: 0.09741042\n",
      "Iteration: 910, Total Loss: 0.10564531, Physics Loss: 0.00880787, BC Loss: 0.09683745\n",
      "Iteration: 920, Total Loss: 0.10542673, Physics Loss: 0.00873677, BC Loss: 0.09668996\n",
      "Iteration: 930, Total Loss: 0.10500397, Physics Loss: 0.00910449, BC Loss: 0.09589948\n",
      "Iteration: 940, Total Loss: 0.10452173, Physics Loss: 0.00902959, BC Loss: 0.09549214\n",
      "Iteration: 950, Total Loss: 0.10398450, Physics Loss: 0.00898292, BC Loss: 0.09500158\n",
      "Iteration: 960, Total Loss: 0.10369760, Physics Loss: 0.00875332, BC Loss: 0.09494428\n",
      "Iteration: 970, Total Loss: 0.10328645, Physics Loss: 0.00834956, BC Loss: 0.09493688\n",
      "Iteration: 980, Total Loss: 0.10294377, Physics Loss: 0.00821092, BC Loss: 0.09473285\n",
      "Iteration: 990, Total Loss: 0.10250974, Physics Loss: 0.00831757, BC Loss: 0.09419216\n",
      "Iteration: 1000, Total Loss: 0.10214697, Physics Loss: 0.00823206, BC Loss: 0.09391491\n",
      "Iteration: 1010, Total Loss: 0.11334156, Physics Loss: 0.01624566, BC Loss: 0.09709591\n",
      "Iteration: 1020, Total Loss: 0.11013242, Physics Loss: 0.01136330, BC Loss: 0.09876912\n",
      "Iteration: 1030, Total Loss: 0.10912991, Physics Loss: 0.00998466, BC Loss: 0.09914524\n",
      "Iteration: 1040, Total Loss: 0.10823606, Physics Loss: 0.00977971, BC Loss: 0.09845635\n",
      "Iteration: 1050, Total Loss: 0.10723080, Physics Loss: 0.00968279, BC Loss: 0.09754801\n",
      "Iteration: 1060, Total Loss: 0.10635219, Physics Loss: 0.00967865, BC Loss: 0.09667353\n",
      "Iteration: 1070, Total Loss: 0.10556478, Physics Loss: 0.00936258, BC Loss: 0.09620220\n",
      "Iteration: 1080, Total Loss: 0.10510380, Physics Loss: 0.00909665, BC Loss: 0.09600715\n",
      "Iteration: 1090, Total Loss: 0.10452469, Physics Loss: 0.00863386, BC Loss: 0.09589083\n",
      "Iteration: 1100, Total Loss: 0.10410655, Physics Loss: 0.00888043, BC Loss: 0.09522612\n",
      "Iteration: 1110, Total Loss: 0.10345383, Physics Loss: 0.00888366, BC Loss: 0.09457017\n",
      "Iteration: 1120, Total Loss: 0.10289827, Physics Loss: 0.00889087, BC Loss: 0.09400740\n",
      "Iteration: 1130, Total Loss: 0.10260956, Physics Loss: 0.00865082, BC Loss: 0.09395874\n",
      "Iteration: 1140, Total Loss: 0.10211358, Physics Loss: 0.00869566, BC Loss: 0.09341792\n",
      "Iteration: 1150, Total Loss: 0.10161094, Physics Loss: 0.00887799, BC Loss: 0.09273296\n",
      "Iteration: 1160, Total Loss: 0.10143198, Physics Loss: 0.00866623, BC Loss: 0.09276575\n",
      "Iteration: 1170, Total Loss: 0.10119235, Physics Loss: 0.00866763, BC Loss: 0.09252471\n",
      "Iteration: 1180, Total Loss: 0.10063611, Physics Loss: 0.00801337, BC Loss: 0.09262274\n",
      "Iteration: 1190, Total Loss: 0.10031821, Physics Loss: 0.00817159, BC Loss: 0.09214662\n",
      "Iteration: 1200, Total Loss: 0.09990744, Physics Loss: 0.00826372, BC Loss: 0.09164372\n",
      "Iteration: 1210, Total Loss: 0.09958505, Physics Loss: 0.00839161, BC Loss: 0.09119344\n",
      "Iteration: 1220, Total Loss: 0.09919737, Physics Loss: 0.00841086, BC Loss: 0.09078650\n",
      "Iteration: 1230, Total Loss: 0.09888687, Physics Loss: 0.00811273, BC Loss: 0.09077414\n",
      "Iteration: 1240, Total Loss: 0.09860703, Physics Loss: 0.00802030, BC Loss: 0.09058673\n",
      "Iteration: 1250, Total Loss: 0.09828763, Physics Loss: 0.00783020, BC Loss: 0.09045742\n",
      "Iteration: 1260, Total Loss: 0.09781951, Physics Loss: 0.00793057, BC Loss: 0.08988895\n",
      "Iteration: 1270, Total Loss: 0.09729935, Physics Loss: 0.00770726, BC Loss: 0.08959210\n",
      "Iteration: 1280, Total Loss: 0.09709036, Physics Loss: 0.00765784, BC Loss: 0.08943252\n",
      "Iteration: 1290, Total Loss: 0.09686538, Physics Loss: 0.00778284, BC Loss: 0.08908254\n",
      "Iteration: 1300, Total Loss: 0.09650770, Physics Loss: 0.00788270, BC Loss: 0.08862500\n",
      "Iteration: 1310, Total Loss: 0.09601007, Physics Loss: 0.00779662, BC Loss: 0.08821346\n",
      "Iteration: 1320, Total Loss: 0.09575774, Physics Loss: 0.00809067, BC Loss: 0.08766707\n",
      "Iteration: 1330, Total Loss: 0.09543597, Physics Loss: 0.00849693, BC Loss: 0.08693904\n",
      "Iteration: 1340, Total Loss: 0.09517788, Physics Loss: 0.00823045, BC Loss: 0.08694743\n",
      "Iteration: 1350, Total Loss: 0.09500924, Physics Loss: 0.00839210, BC Loss: 0.08661714\n",
      "Iteration: 1360, Total Loss: 0.09473245, Physics Loss: 0.00799544, BC Loss: 0.08673701\n",
      "Iteration: 1370, Total Loss: 0.09443357, Physics Loss: 0.00798210, BC Loss: 0.08645147\n",
      "Iteration: 1380, Total Loss: 0.09409794, Physics Loss: 0.00786577, BC Loss: 0.08623217\n",
      "Iteration: 1390, Total Loss: 0.09363981, Physics Loss: 0.00761540, BC Loss: 0.08602441\n",
      "Iteration: 1400, Total Loss: 0.09342802, Physics Loss: 0.00741581, BC Loss: 0.08601220\n",
      "Iteration: 1410, Total Loss: 0.09312271, Physics Loss: 0.00765433, BC Loss: 0.08546838\n",
      "Iteration: 1420, Total Loss: 0.09286810, Physics Loss: 0.00756134, BC Loss: 0.08530676\n",
      "Iteration: 1430, Total Loss: 0.09251940, Physics Loss: 0.00737107, BC Loss: 0.08514833\n",
      "Iteration: 1440, Total Loss: 0.09225673, Physics Loss: 0.00734403, BC Loss: 0.08491270\n",
      "Iteration: 1450, Total Loss: 0.09187794, Physics Loss: 0.00734102, BC Loss: 0.08453692\n",
      "Iteration: 1460, Total Loss: 0.09162531, Physics Loss: 0.00775670, BC Loss: 0.08386861\n",
      "Iteration: 1470, Total Loss: 0.09132759, Physics Loss: 0.00767597, BC Loss: 0.08365162\n",
      "Iteration: 1480, Total Loss: 0.09102078, Physics Loss: 0.00761117, BC Loss: 0.08340961\n",
      "Iteration: 1490, Total Loss: 0.09075876, Physics Loss: 0.00756797, BC Loss: 0.08319079\n",
      "Iteration: 1500, Total Loss: 0.09050415, Physics Loss: 0.00771607, BC Loss: 0.08278809\n",
      "Iteration: 1510, Total Loss: 0.09000792, Physics Loss: 0.00755267, BC Loss: 0.08245525\n",
      "Iteration: 1520, Total Loss: 0.08979329, Physics Loss: 0.00742447, BC Loss: 0.08236882\n",
      "Iteration: 1530, Total Loss: 0.08959847, Physics Loss: 0.00715952, BC Loss: 0.08243895\n",
      "Iteration: 1540, Total Loss: 0.08938733, Physics Loss: 0.00722214, BC Loss: 0.08216519\n",
      "Iteration: 1550, Total Loss: 0.08920299, Physics Loss: 0.00709271, BC Loss: 0.08211028\n",
      "Iteration: 1560, Total Loss: 0.08904499, Physics Loss: 0.00704619, BC Loss: 0.08199880\n",
      "Iteration: 1570, Total Loss: 0.08893181, Physics Loss: 0.00722450, BC Loss: 0.08170731\n",
      "Iteration: 1580, Total Loss: 0.08873145, Physics Loss: 0.00718572, BC Loss: 0.08154573\n",
      "Iteration: 1590, Total Loss: 0.08850545, Physics Loss: 0.00697293, BC Loss: 0.08153252\n",
      "Iteration: 1600, Total Loss: 0.08833475, Physics Loss: 0.00687413, BC Loss: 0.08146062\n",
      "Iteration: 1610, Total Loss: 0.08816388, Physics Loss: 0.00691380, BC Loss: 0.08125009\n",
      "Iteration: 1620, Total Loss: 0.08802542, Physics Loss: 0.00693445, BC Loss: 0.08109097\n",
      "Iteration: 1630, Total Loss: 0.08785347, Physics Loss: 0.00696277, BC Loss: 0.08089069\n",
      "Iteration: 1640, Total Loss: 0.08772941, Physics Loss: 0.00685902, BC Loss: 0.08087039\n",
      "Iteration: 1650, Total Loss: 0.08750417, Physics Loss: 0.00676655, BC Loss: 0.08073762\n",
      "Iteration: 1660, Total Loss: 0.08725916, Physics Loss: 0.00728705, BC Loss: 0.07997210\n",
      "Iteration: 1670, Total Loss: 0.08707607, Physics Loss: 0.00706364, BC Loss: 0.08001243\n",
      "Iteration: 1680, Total Loss: 0.08685960, Physics Loss: 0.00694419, BC Loss: 0.07991540\n",
      "Iteration: 1690, Total Loss: 0.08665977, Physics Loss: 0.00704385, BC Loss: 0.07961592\n",
      "Iteration: 1700, Total Loss: 0.08654384, Physics Loss: 0.00703661, BC Loss: 0.07950724\n",
      "Iteration: 1710, Total Loss: 0.08639225, Physics Loss: 0.00688172, BC Loss: 0.07951052\n",
      "Iteration: 1720, Total Loss: 0.08618616, Physics Loss: 0.00680499, BC Loss: 0.07938117\n",
      "Iteration: 1730, Total Loss: 0.08600114, Physics Loss: 0.00690854, BC Loss: 0.07909261\n",
      "Iteration: 1740, Total Loss: 0.08585650, Physics Loss: 0.00686334, BC Loss: 0.07899316\n",
      "Iteration: 1750, Total Loss: 0.08569648, Physics Loss: 0.00688141, BC Loss: 0.07881507\n",
      "Iteration: 1760, Total Loss: 0.08550408, Physics Loss: 0.00679240, BC Loss: 0.07871167\n",
      "Iteration: 1770, Total Loss: 0.08526335, Physics Loss: 0.00658394, BC Loss: 0.07867941\n",
      "Iteration: 1780, Total Loss: 0.08506046, Physics Loss: 0.00666093, BC Loss: 0.07839953\n",
      "Iteration: 1790, Total Loss: 0.08495069, Physics Loss: 0.00676834, BC Loss: 0.07818234\n",
      "Iteration: 1800, Total Loss: 0.08476921, Physics Loss: 0.00673661, BC Loss: 0.07803261\n",
      "Iteration: 1810, Total Loss: 0.08467457, Physics Loss: 0.00668181, BC Loss: 0.07799276\n",
      "Iteration: 1820, Total Loss: 0.08451437, Physics Loss: 0.00684854, BC Loss: 0.07766584\n",
      "Iteration: 1830, Total Loss: 0.08442208, Physics Loss: 0.00671893, BC Loss: 0.07770316\n",
      "Iteration: 1840, Total Loss: 0.08422441, Physics Loss: 0.00655513, BC Loss: 0.07766929\n",
      "Iteration: 1850, Total Loss: 0.08409472, Physics Loss: 0.00656819, BC Loss: 0.07752652\n",
      "Iteration: 1860, Total Loss: 0.08400321, Physics Loss: 0.00660088, BC Loss: 0.07740233\n",
      "Iteration: 1870, Total Loss: 0.08388583, Physics Loss: 0.00659243, BC Loss: 0.07729340\n",
      "Iteration: 1880, Total Loss: 0.08376583, Physics Loss: 0.00641987, BC Loss: 0.07734596\n",
      "Iteration: 1890, Total Loss: 0.08362884, Physics Loss: 0.00636123, BC Loss: 0.07726761\n",
      "Iteration: 1900, Total Loss: 0.08349229, Physics Loss: 0.00633117, BC Loss: 0.07716112\n",
      "Iteration: 1910, Total Loss: 0.08327629, Physics Loss: 0.00624538, BC Loss: 0.07703090\n",
      "Iteration: 1920, Total Loss: 0.08310397, Physics Loss: 0.00642772, BC Loss: 0.07667625\n",
      "Iteration: 1930, Total Loss: 0.08292121, Physics Loss: 0.00656830, BC Loss: 0.07635291\n",
      "Iteration: 1940, Total Loss: 0.08272043, Physics Loss: 0.00656235, BC Loss: 0.07615808\n",
      "Iteration: 1950, Total Loss: 0.08259778, Physics Loss: 0.00673873, BC Loss: 0.07585905\n",
      "Iteration: 1960, Total Loss: 0.08244203, Physics Loss: 0.00675923, BC Loss: 0.07568280\n",
      "Iteration: 1970, Total Loss: 0.08229454, Physics Loss: 0.00700136, BC Loss: 0.07529318\n",
      "Iteration: 1980, Total Loss: 0.08213443, Physics Loss: 0.00683245, BC Loss: 0.07530198\n",
      "Iteration: 1990, Total Loss: 0.08199298, Physics Loss: 0.00677061, BC Loss: 0.07522237\n",
      "Iteration: 2000, Total Loss: 0.08187857, Physics Loss: 0.00692293, BC Loss: 0.07495564\n",
      "Iteration: 2010, Total Loss: 0.10265400, Physics Loss: 0.02930569, BC Loss: 0.07334831\n",
      "Iteration: 2020, Total Loss: 0.09352142, Physics Loss: 0.01835000, BC Loss: 0.07517143\n",
      "Iteration: 2030, Total Loss: 0.08933827, Physics Loss: 0.01484521, BC Loss: 0.07449307\n",
      "Iteration: 2040, Total Loss: 0.08609641, Physics Loss: 0.01055048, BC Loss: 0.07554593\n",
      "Iteration: 2050, Total Loss: 0.08483668, Physics Loss: 0.00983200, BC Loss: 0.07500468\n",
      "Iteration: 2060, Total Loss: 0.08404724, Physics Loss: 0.01031303, BC Loss: 0.07373421\n",
      "Iteration: 2070, Total Loss: 0.08272967, Physics Loss: 0.00981327, BC Loss: 0.07291641\n",
      "Iteration: 2080, Total Loss: 0.08111963, Physics Loss: 0.00978528, BC Loss: 0.07133435\n",
      "Iteration: 2090, Total Loss: 0.08035817, Physics Loss: 0.00961577, BC Loss: 0.07074240\n",
      "Iteration: 2100, Total Loss: 0.07960908, Physics Loss: 0.00945380, BC Loss: 0.07015528\n",
      "Iteration: 2110, Total Loss: 0.07916624, Physics Loss: 0.00903625, BC Loss: 0.07012999\n",
      "Iteration: 2120, Total Loss: 0.07870780, Physics Loss: 0.00865825, BC Loss: 0.07004955\n",
      "Iteration: 2130, Total Loss: 0.07805870, Physics Loss: 0.00831199, BC Loss: 0.06974671\n",
      "Iteration: 2140, Total Loss: 0.07731959, Physics Loss: 0.00810958, BC Loss: 0.06921001\n",
      "Iteration: 2150, Total Loss: 0.07676624, Physics Loss: 0.00796889, BC Loss: 0.06879735\n",
      "Iteration: 2160, Total Loss: 0.07628563, Physics Loss: 0.00798848, BC Loss: 0.06829716\n",
      "Iteration: 2170, Total Loss: 0.07586998, Physics Loss: 0.00745228, BC Loss: 0.06841771\n",
      "Iteration: 2180, Total Loss: 0.07566163, Physics Loss: 0.00754691, BC Loss: 0.06811473\n",
      "Iteration: 2190, Total Loss: 0.07521886, Physics Loss: 0.00743519, BC Loss: 0.06778368\n",
      "Iteration: 2200, Total Loss: 0.07490136, Physics Loss: 0.00751043, BC Loss: 0.06739093\n",
      "Iteration: 2210, Total Loss: 0.07458816, Physics Loss: 0.00774043, BC Loss: 0.06684773\n",
      "Iteration: 2220, Total Loss: 0.07425041, Physics Loss: 0.00745999, BC Loss: 0.06679042\n",
      "Iteration: 2230, Total Loss: 0.07395347, Physics Loss: 0.00744158, BC Loss: 0.06651189\n",
      "Iteration: 2240, Total Loss: 0.07367314, Physics Loss: 0.00723942, BC Loss: 0.06643372\n",
      "Iteration: 2250, Total Loss: 0.07326571, Physics Loss: 0.00706042, BC Loss: 0.06620529\n",
      "Iteration: 2260, Total Loss: 0.07305720, Physics Loss: 0.00675025, BC Loss: 0.06630695\n",
      "Iteration: 2270, Total Loss: 0.07260985, Physics Loss: 0.00638120, BC Loss: 0.06622865\n",
      "Iteration: 2280, Total Loss: 0.07230891, Physics Loss: 0.00635616, BC Loss: 0.06595275\n",
      "Iteration: 2290, Total Loss: 0.07187328, Physics Loss: 0.00597515, BC Loss: 0.06589814\n",
      "Iteration: 2300, Total Loss: 0.07174459, Physics Loss: 0.00599698, BC Loss: 0.06574761\n",
      "Iteration: 2310, Total Loss: 0.07156762, Physics Loss: 0.00585374, BC Loss: 0.06571388\n",
      "Iteration: 2320, Total Loss: 0.07138339, Physics Loss: 0.00598153, BC Loss: 0.06540185\n",
      "Iteration: 2330, Total Loss: 0.07112781, Physics Loss: 0.00593153, BC Loss: 0.06519628\n",
      "Iteration: 2340, Total Loss: 0.07085813, Physics Loss: 0.00596112, BC Loss: 0.06489701\n",
      "Iteration: 2350, Total Loss: 0.07050481, Physics Loss: 0.00603896, BC Loss: 0.06446584\n",
      "Iteration: 2360, Total Loss: 0.07009944, Physics Loss: 0.00604796, BC Loss: 0.06405147\n",
      "Iteration: 2370, Total Loss: 0.06969985, Physics Loss: 0.00620588, BC Loss: 0.06349397\n",
      "Iteration: 2380, Total Loss: 0.06942545, Physics Loss: 0.00608379, BC Loss: 0.06334166\n",
      "Iteration: 2390, Total Loss: 0.06905078, Physics Loss: 0.00607468, BC Loss: 0.06297611\n",
      "Iteration: 2400, Total Loss: 0.06874416, Physics Loss: 0.00616917, BC Loss: 0.06257499\n",
      "Iteration: 2410, Total Loss: 0.06856705, Physics Loss: 0.00613967, BC Loss: 0.06242738\n",
      "Iteration: 2420, Total Loss: 0.06828138, Physics Loss: 0.00618099, BC Loss: 0.06210040\n",
      "Iteration: 2430, Total Loss: 0.06784769, Physics Loss: 0.00589588, BC Loss: 0.06195182\n",
      "Iteration: 2440, Total Loss: 0.06746254, Physics Loss: 0.00556626, BC Loss: 0.06189629\n",
      "Iteration: 2450, Total Loss: 0.06732400, Physics Loss: 0.00563443, BC Loss: 0.06168956\n",
      "Iteration: 2460, Total Loss: 0.06697308, Physics Loss: 0.00544336, BC Loss: 0.06152972\n",
      "Iteration: 2470, Total Loss: 0.06665632, Physics Loss: 0.00553904, BC Loss: 0.06111728\n",
      "Iteration: 2480, Total Loss: 0.06640486, Physics Loss: 0.00560196, BC Loss: 0.06080289\n",
      "Iteration: 2490, Total Loss: 0.06622192, Physics Loss: 0.00555746, BC Loss: 0.06066446\n",
      "Iteration: 2500, Total Loss: 0.06595729, Physics Loss: 0.00557172, BC Loss: 0.06038558\n",
      "Iteration: 2510, Total Loss: 0.06563051, Physics Loss: 0.00539648, BC Loss: 0.06023403\n",
      "Iteration: 2520, Total Loss: 0.06537918, Physics Loss: 0.00533016, BC Loss: 0.06004902\n",
      "Iteration: 2530, Total Loss: 0.06516980, Physics Loss: 0.00556719, BC Loss: 0.05960261\n",
      "Iteration: 2540, Total Loss: 0.06488866, Physics Loss: 0.00544502, BC Loss: 0.05944364\n",
      "Iteration: 2550, Total Loss: 0.06472065, Physics Loss: 0.00539476, BC Loss: 0.05932590\n",
      "Iteration: 2560, Total Loss: 0.06450481, Physics Loss: 0.00536920, BC Loss: 0.05913561\n",
      "Iteration: 2570, Total Loss: 0.06418573, Physics Loss: 0.00515896, BC Loss: 0.05902678\n",
      "Iteration: 2580, Total Loss: 0.06392518, Physics Loss: 0.00531153, BC Loss: 0.05861365\n",
      "Iteration: 2590, Total Loss: 0.06361594, Physics Loss: 0.00536993, BC Loss: 0.05824601\n",
      "Iteration: 2600, Total Loss: 0.06330396, Physics Loss: 0.00513610, BC Loss: 0.05816786\n",
      "Iteration: 2610, Total Loss: 0.06305856, Physics Loss: 0.00496048, BC Loss: 0.05809809\n",
      "Iteration: 2620, Total Loss: 0.06275132, Physics Loss: 0.00503988, BC Loss: 0.05771143\n",
      "Iteration: 2630, Total Loss: 0.06234576, Physics Loss: 0.00515826, BC Loss: 0.05718750\n",
      "Iteration: 2640, Total Loss: 0.06203899, Physics Loss: 0.00510038, BC Loss: 0.05693861\n",
      "Iteration: 2650, Total Loss: 0.06177285, Physics Loss: 0.00525924, BC Loss: 0.05651360\n",
      "Iteration: 2660, Total Loss: 0.06167437, Physics Loss: 0.00516396, BC Loss: 0.05651041\n",
      "Iteration: 2670, Total Loss: 0.06151404, Physics Loss: 0.00507414, BC Loss: 0.05643990\n",
      "Iteration: 2680, Total Loss: 0.06128659, Physics Loss: 0.00501251, BC Loss: 0.05627409\n",
      "Iteration: 2690, Total Loss: 0.06100991, Physics Loss: 0.00494103, BC Loss: 0.05606888\n",
      "Iteration: 2700, Total Loss: 0.06075539, Physics Loss: 0.00485546, BC Loss: 0.05589993\n",
      "Iteration: 2710, Total Loss: 0.06060924, Physics Loss: 0.00491587, BC Loss: 0.05569337\n",
      "Iteration: 2720, Total Loss: 0.06045928, Physics Loss: 0.00494010, BC Loss: 0.05551919\n",
      "Iteration: 2730, Total Loss: 0.06026570, Physics Loss: 0.00471623, BC Loss: 0.05554947\n",
      "Iteration: 2740, Total Loss: 0.06007911, Physics Loss: 0.00468354, BC Loss: 0.05539558\n",
      "Iteration: 2750, Total Loss: 0.05992829, Physics Loss: 0.00492177, BC Loss: 0.05500653\n",
      "Iteration: 2760, Total Loss: 0.05969548, Physics Loss: 0.00492420, BC Loss: 0.05477128\n",
      "Iteration: 2770, Total Loss: 0.05953979, Physics Loss: 0.00500239, BC Loss: 0.05453741\n",
      "Iteration: 2780, Total Loss: 0.05939183, Physics Loss: 0.00507747, BC Loss: 0.05431435\n",
      "Iteration: 2790, Total Loss: 0.05912200, Physics Loss: 0.00491515, BC Loss: 0.05420685\n",
      "Iteration: 2800, Total Loss: 0.05885260, Physics Loss: 0.00472038, BC Loss: 0.05413222\n",
      "Iteration: 2810, Total Loss: 0.05851156, Physics Loss: 0.00454775, BC Loss: 0.05396382\n",
      "Iteration: 2820, Total Loss: 0.05802487, Physics Loss: 0.00463778, BC Loss: 0.05338709\n",
      "Iteration: 2830, Total Loss: 0.05769803, Physics Loss: 0.00454098, BC Loss: 0.05315704\n",
      "Iteration: 2840, Total Loss: 0.05744512, Physics Loss: 0.00440097, BC Loss: 0.05304414\n",
      "Iteration: 2850, Total Loss: 0.05705443, Physics Loss: 0.00431135, BC Loss: 0.05274308\n",
      "Iteration: 2860, Total Loss: 0.05685891, Physics Loss: 0.00430394, BC Loss: 0.05255497\n",
      "Iteration: 2870, Total Loss: 0.05655450, Physics Loss: 0.00441471, BC Loss: 0.05213979\n",
      "Iteration: 2880, Total Loss: 0.05628680, Physics Loss: 0.00434049, BC Loss: 0.05194630\n",
      "Iteration: 2890, Total Loss: 0.05601765, Physics Loss: 0.00430907, BC Loss: 0.05170858\n",
      "Iteration: 2900, Total Loss: 0.05570187, Physics Loss: 0.00433786, BC Loss: 0.05136401\n",
      "Iteration: 2910, Total Loss: 0.05549758, Physics Loss: 0.00429769, BC Loss: 0.05119989\n",
      "Iteration: 2920, Total Loss: 0.05526880, Physics Loss: 0.00439783, BC Loss: 0.05087097\n",
      "Iteration: 2930, Total Loss: 0.05492418, Physics Loss: 0.00438649, BC Loss: 0.05053768\n",
      "Iteration: 2940, Total Loss: 0.05462784, Physics Loss: 0.00453952, BC Loss: 0.05008832\n",
      "Iteration: 2950, Total Loss: 0.05437235, Physics Loss: 0.00448751, BC Loss: 0.04988484\n",
      "Iteration: 2960, Total Loss: 0.05417785, Physics Loss: 0.00472091, BC Loss: 0.04945694\n",
      "Iteration: 2970, Total Loss: 0.05399011, Physics Loss: 0.00468570, BC Loss: 0.04930441\n",
      "Iteration: 2980, Total Loss: 0.05370972, Physics Loss: 0.00468124, BC Loss: 0.04902848\n",
      "Iteration: 2990, Total Loss: 0.05332705, Physics Loss: 0.00461086, BC Loss: 0.04871619\n",
      "Iteration: 3000, Total Loss: 0.05293996, Physics Loss: 0.00434302, BC Loss: 0.04859693\n",
      "Iteration: 3010, Total Loss: 2.95668030, Physics Loss: 2.37873745, BC Loss: 0.57794297\n",
      "Iteration: 3020, Total Loss: 0.90759212, Physics Loss: 0.45246655, BC Loss: 0.45512557\n",
      "Iteration: 3030, Total Loss: 0.34302759, Physics Loss: 0.11354083, BC Loss: 0.22948676\n",
      "Iteration: 3040, Total Loss: 0.21749903, Physics Loss: 0.06979334, BC Loss: 0.14770569\n",
      "Iteration: 3050, Total Loss: 0.18023610, Physics Loss: 0.03968647, BC Loss: 0.14054963\n",
      "Iteration: 3060, Total Loss: 0.16422024, Physics Loss: 0.02748116, BC Loss: 0.13673908\n",
      "Iteration: 3070, Total Loss: 0.15208058, Physics Loss: 0.01794046, BC Loss: 0.13414012\n",
      "Iteration: 3080, Total Loss: 0.14415795, Physics Loss: 0.02088422, BC Loss: 0.12327372\n",
      "Iteration: 3090, Total Loss: 0.13785633, Physics Loss: 0.01961650, BC Loss: 0.11823984\n",
      "Iteration: 3100, Total Loss: 0.13313721, Physics Loss: 0.01840519, BC Loss: 0.11473203\n",
      "Iteration: 3110, Total Loss: 0.13069989, Physics Loss: 0.01740656, BC Loss: 0.11329333\n",
      "Iteration: 3120, Total Loss: 0.12869412, Physics Loss: 0.01780402, BC Loss: 0.11089009\n",
      "Iteration: 3130, Total Loss: 0.12602976, Physics Loss: 0.01647142, BC Loss: 0.10955834\n",
      "Iteration: 3140, Total Loss: 0.12481163, Physics Loss: 0.01566359, BC Loss: 0.10914805\n",
      "Iteration: 3150, Total Loss: 0.12302239, Physics Loss: 0.01281093, BC Loss: 0.11021146\n",
      "Iteration: 3160, Total Loss: 0.12242116, Physics Loss: 0.01255867, BC Loss: 0.10986249\n",
      "Iteration: 3170, Total Loss: 0.12145631, Physics Loss: 0.01166067, BC Loss: 0.10979564\n",
      "Iteration: 3180, Total Loss: 0.12024049, Physics Loss: 0.01094968, BC Loss: 0.10929081\n",
      "Iteration: 3190, Total Loss: 0.11827747, Physics Loss: 0.01011125, BC Loss: 0.10816622\n",
      "Iteration: 3200, Total Loss: 0.11752693, Physics Loss: 0.00915767, BC Loss: 0.10836926\n",
      "Iteration: 3210, Total Loss: 0.11662509, Physics Loss: 0.00895065, BC Loss: 0.10767445\n",
      "Iteration: 3220, Total Loss: 0.11625508, Physics Loss: 0.00882129, BC Loss: 0.10743380\n",
      "Iteration: 3230, Total Loss: 0.11535621, Physics Loss: 0.00868316, BC Loss: 0.10667305\n",
      "Iteration: 3240, Total Loss: 0.11459640, Physics Loss: 0.00817850, BC Loss: 0.10641790\n",
      "Iteration: 3250, Total Loss: 0.11433022, Physics Loss: 0.00803758, BC Loss: 0.10629264\n",
      "Iteration: 3260, Total Loss: 0.11375714, Physics Loss: 0.00840513, BC Loss: 0.10535201\n",
      "Iteration: 3270, Total Loss: 0.11342926, Physics Loss: 0.00869335, BC Loss: 0.10473592\n",
      "Iteration: 3280, Total Loss: 0.11300189, Physics Loss: 0.00862391, BC Loss: 0.10437798\n",
      "Iteration: 3290, Total Loss: 0.11249756, Physics Loss: 0.00889622, BC Loss: 0.10360134\n",
      "Iteration: 3300, Total Loss: 0.11205876, Physics Loss: 0.00869298, BC Loss: 0.10336578\n",
      "Iteration: 3310, Total Loss: 0.11165166, Physics Loss: 0.00878505, BC Loss: 0.10286661\n",
      "Iteration: 3320, Total Loss: 0.11135672, Physics Loss: 0.00858429, BC Loss: 0.10277243\n",
      "Iteration: 3330, Total Loss: 0.11094190, Physics Loss: 0.00886484, BC Loss: 0.10207707\n",
      "Iteration: 3340, Total Loss: 0.11066447, Physics Loss: 0.00892340, BC Loss: 0.10174108\n",
      "Iteration: 3350, Total Loss: 0.11029084, Physics Loss: 0.00869809, BC Loss: 0.10159274\n",
      "Iteration: 3360, Total Loss: 0.11000914, Physics Loss: 0.00854646, BC Loss: 0.10146268\n",
      "Iteration: 3370, Total Loss: 0.10962212, Physics Loss: 0.00844848, BC Loss: 0.10117364\n",
      "Iteration: 3380, Total Loss: 0.10910749, Physics Loss: 0.00856690, BC Loss: 0.10054059\n",
      "Iteration: 3390, Total Loss: 0.10884220, Physics Loss: 0.00847489, BC Loss: 0.10036731\n",
      "Iteration: 3400, Total Loss: 0.10862909, Physics Loss: 0.00893711, BC Loss: 0.09969198\n",
      "Iteration: 3410, Total Loss: 0.10845160, Physics Loss: 0.00912180, BC Loss: 0.09932981\n",
      "Iteration: 3420, Total Loss: 0.10822291, Physics Loss: 0.00913157, BC Loss: 0.09909134\n",
      "Iteration: 3430, Total Loss: 0.10781637, Physics Loss: 0.00942237, BC Loss: 0.09839399\n",
      "Iteration: 3440, Total Loss: 0.10745527, Physics Loss: 0.00892587, BC Loss: 0.09852940\n",
      "Iteration: 3450, Total Loss: 0.10723365, Physics Loss: 0.00867797, BC Loss: 0.09855568\n",
      "Iteration: 3460, Total Loss: 0.10680141, Physics Loss: 0.00905641, BC Loss: 0.09774500\n",
      "Iteration: 3470, Total Loss: 0.10660088, Physics Loss: 0.00903346, BC Loss: 0.09756742\n",
      "Iteration: 3480, Total Loss: 0.10631167, Physics Loss: 0.00915025, BC Loss: 0.09716143\n",
      "Iteration: 3490, Total Loss: 0.10616379, Physics Loss: 0.00906288, BC Loss: 0.09710091\n",
      "Iteration: 3500, Total Loss: 0.10599601, Physics Loss: 0.00904365, BC Loss: 0.09695236\n",
      "Iteration: 3510, Total Loss: 0.10574442, Physics Loss: 0.00876015, BC Loss: 0.09698427\n",
      "Iteration: 3520, Total Loss: 0.10547374, Physics Loss: 0.00916261, BC Loss: 0.09631113\n",
      "Iteration: 3530, Total Loss: 0.10511542, Physics Loss: 0.00902817, BC Loss: 0.09608725\n",
      "Iteration: 3540, Total Loss: 0.10467838, Physics Loss: 0.00885468, BC Loss: 0.09582370\n",
      "Iteration: 3550, Total Loss: 0.10442898, Physics Loss: 0.00882248, BC Loss: 0.09560650\n",
      "Iteration: 3560, Total Loss: 0.10426639, Physics Loss: 0.00874531, BC Loss: 0.09552108\n",
      "Iteration: 3570, Total Loss: 0.10393287, Physics Loss: 0.00815040, BC Loss: 0.09578247\n",
      "Iteration: 3580, Total Loss: 0.10371947, Physics Loss: 0.00814090, BC Loss: 0.09557857\n",
      "Iteration: 3590, Total Loss: 0.10342605, Physics Loss: 0.00800143, BC Loss: 0.09542462\n",
      "Iteration: 3600, Total Loss: 0.10319507, Physics Loss: 0.00794022, BC Loss: 0.09525485\n",
      "Iteration: 3610, Total Loss: 0.10286759, Physics Loss: 0.00801100, BC Loss: 0.09485658\n",
      "Iteration: 3620, Total Loss: 0.10265657, Physics Loss: 0.00804053, BC Loss: 0.09461605\n",
      "Iteration: 3630, Total Loss: 0.10239285, Physics Loss: 0.00815272, BC Loss: 0.09424014\n",
      "Iteration: 3640, Total Loss: 0.10191511, Physics Loss: 0.00801664, BC Loss: 0.09389848\n",
      "Iteration: 3650, Total Loss: 0.10163144, Physics Loss: 0.00804581, BC Loss: 0.09358563\n",
      "Iteration: 3660, Total Loss: 0.10137746, Physics Loss: 0.00788436, BC Loss: 0.09349310\n",
      "Iteration: 3670, Total Loss: 0.10112827, Physics Loss: 0.00789029, BC Loss: 0.09323798\n",
      "Iteration: 3680, Total Loss: 0.10089897, Physics Loss: 0.00777323, BC Loss: 0.09312573\n",
      "Iteration: 3690, Total Loss: 0.10065269, Physics Loss: 0.00777980, BC Loss: 0.09287290\n",
      "Iteration: 3700, Total Loss: 0.10040058, Physics Loss: 0.00792339, BC Loss: 0.09247719\n",
      "Iteration: 3710, Total Loss: 0.10011256, Physics Loss: 0.00781945, BC Loss: 0.09229311\n",
      "Iteration: 3720, Total Loss: 0.09993766, Physics Loss: 0.00758690, BC Loss: 0.09235076\n",
      "Iteration: 3730, Total Loss: 0.09961733, Physics Loss: 0.00741224, BC Loss: 0.09220509\n",
      "Iteration: 3740, Total Loss: 0.09916480, Physics Loss: 0.00757811, BC Loss: 0.09158669\n",
      "Iteration: 3750, Total Loss: 0.09894500, Physics Loss: 0.00756307, BC Loss: 0.09138193\n",
      "Iteration: 3760, Total Loss: 0.09873215, Physics Loss: 0.00768648, BC Loss: 0.09104567\n",
      "Iteration: 3770, Total Loss: 0.09841596, Physics Loss: 0.00762377, BC Loss: 0.09079219\n",
      "Iteration: 3780, Total Loss: 0.09817137, Physics Loss: 0.00739618, BC Loss: 0.09077519\n",
      "Iteration: 3790, Total Loss: 0.09793538, Physics Loss: 0.00757593, BC Loss: 0.09035944\n",
      "Iteration: 3800, Total Loss: 0.09767596, Physics Loss: 0.00745285, BC Loss: 0.09022311\n",
      "Iteration: 3810, Total Loss: 0.09732758, Physics Loss: 0.00741095, BC Loss: 0.08991662\n",
      "Iteration: 3820, Total Loss: 0.09696935, Physics Loss: 0.00746949, BC Loss: 0.08949986\n",
      "Iteration: 3830, Total Loss: 0.09674361, Physics Loss: 0.00719972, BC Loss: 0.08954389\n",
      "Iteration: 3840, Total Loss: 0.09660079, Physics Loss: 0.00720308, BC Loss: 0.08939771\n",
      "Iteration: 3850, Total Loss: 0.09638225, Physics Loss: 0.00702743, BC Loss: 0.08935481\n",
      "Iteration: 3860, Total Loss: 0.09616284, Physics Loss: 0.00695892, BC Loss: 0.08920392\n",
      "Iteration: 3870, Total Loss: 0.09598117, Physics Loss: 0.00690458, BC Loss: 0.08907660\n",
      "Iteration: 3880, Total Loss: 0.09578623, Physics Loss: 0.00680073, BC Loss: 0.08898550\n",
      "Iteration: 3890, Total Loss: 0.09554804, Physics Loss: 0.00662980, BC Loss: 0.08891824\n",
      "Iteration: 3900, Total Loss: 0.09519141, Physics Loss: 0.00662222, BC Loss: 0.08856919\n",
      "Iteration: 3910, Total Loss: 0.09495317, Physics Loss: 0.00653145, BC Loss: 0.08842172\n",
      "Iteration: 3920, Total Loss: 0.09480183, Physics Loss: 0.00649644, BC Loss: 0.08830538\n",
      "Iteration: 3930, Total Loss: 0.09466885, Physics Loss: 0.00635364, BC Loss: 0.08831522\n",
      "Iteration: 3940, Total Loss: 0.09456680, Physics Loss: 0.00632197, BC Loss: 0.08824483\n",
      "Iteration: 3950, Total Loss: 0.09443940, Physics Loss: 0.00637834, BC Loss: 0.08806106\n",
      "Iteration: 3960, Total Loss: 0.09423234, Physics Loss: 0.00638650, BC Loss: 0.08784585\n",
      "Iteration: 3970, Total Loss: 0.09405493, Physics Loss: 0.00620736, BC Loss: 0.08784758\n",
      "Iteration: 3980, Total Loss: 0.09385066, Physics Loss: 0.00627770, BC Loss: 0.08757296\n",
      "Iteration: 3990, Total Loss: 0.09374575, Physics Loss: 0.00638162, BC Loss: 0.08736413\n",
      "Iteration: 4000, Total Loss: 0.09361903, Physics Loss: 0.00638354, BC Loss: 0.08723548\n",
      "Iteration: 4010, Total Loss: 0.28288794, Physics Loss: 0.19026668, BC Loss: 0.09262127\n",
      "Iteration: 4020, Total Loss: 0.19850284, Physics Loss: 0.09495534, BC Loss: 0.10354750\n",
      "Iteration: 4030, Total Loss: 0.16369233, Physics Loss: 0.05570304, BC Loss: 0.10798930\n",
      "Iteration: 4040, Total Loss: 0.13873595, Physics Loss: 0.03453234, BC Loss: 0.10420360\n",
      "Iteration: 4050, Total Loss: 0.12922585, Physics Loss: 0.02712912, BC Loss: 0.10209674\n",
      "Iteration: 4060, Total Loss: 0.12311319, Physics Loss: 0.02266818, BC Loss: 0.10044501\n",
      "Iteration: 4070, Total Loss: 0.11728359, Physics Loss: 0.01747333, BC Loss: 0.09981026\n",
      "Iteration: 4080, Total Loss: 0.11560182, Physics Loss: 0.01635013, BC Loss: 0.09925169\n",
      "Iteration: 4090, Total Loss: 0.11301410, Physics Loss: 0.01423747, BC Loss: 0.09877664\n",
      "Iteration: 4100, Total Loss: 0.11084067, Physics Loss: 0.01237568, BC Loss: 0.09846500\n",
      "Iteration: 4110, Total Loss: 0.10852992, Physics Loss: 0.01206984, BC Loss: 0.09646007\n",
      "Iteration: 4120, Total Loss: 0.10704394, Physics Loss: 0.01124665, BC Loss: 0.09579729\n",
      "Iteration: 4130, Total Loss: 0.10521077, Physics Loss: 0.01126757, BC Loss: 0.09394319\n",
      "Iteration: 4140, Total Loss: 0.10350476, Physics Loss: 0.01119905, BC Loss: 0.09230571\n",
      "Iteration: 4150, Total Loss: 0.10189635, Physics Loss: 0.01092030, BC Loss: 0.09097605\n",
      "Iteration: 4160, Total Loss: 0.10056448, Physics Loss: 0.01077379, BC Loss: 0.08979069\n",
      "Iteration: 4170, Total Loss: 0.09938212, Physics Loss: 0.01091598, BC Loss: 0.08846615\n",
      "Iteration: 4180, Total Loss: 0.09803129, Physics Loss: 0.01121134, BC Loss: 0.08681995\n",
      "Iteration: 4190, Total Loss: 0.09673169, Physics Loss: 0.01097297, BC Loss: 0.08575872\n",
      "Iteration: 4200, Total Loss: 0.09537992, Physics Loss: 0.01011292, BC Loss: 0.08526701\n",
      "Iteration: 4210, Total Loss: 0.09445664, Physics Loss: 0.01029185, BC Loss: 0.08416479\n",
      "Iteration: 4220, Total Loss: 0.09369393, Physics Loss: 0.01010079, BC Loss: 0.08359313\n",
      "Iteration: 4230, Total Loss: 0.09300228, Physics Loss: 0.01014864, BC Loss: 0.08285365\n",
      "Iteration: 4240, Total Loss: 0.09199703, Physics Loss: 0.01004741, BC Loss: 0.08194962\n",
      "Iteration: 4250, Total Loss: 0.09133007, Physics Loss: 0.01005932, BC Loss: 0.08127074\n",
      "Iteration: 4260, Total Loss: 0.09059916, Physics Loss: 0.00976420, BC Loss: 0.08083495\n",
      "Iteration: 4270, Total Loss: 0.08994877, Physics Loss: 0.00939115, BC Loss: 0.08055763\n",
      "Iteration: 4280, Total Loss: 0.08928411, Physics Loss: 0.00951059, BC Loss: 0.07977352\n",
      "Iteration: 4290, Total Loss: 0.08880991, Physics Loss: 0.00898132, BC Loss: 0.07982860\n",
      "Iteration: 4300, Total Loss: 0.08842085, Physics Loss: 0.00839251, BC Loss: 0.08002834\n",
      "Iteration: 4310, Total Loss: 0.08813775, Physics Loss: 0.00827742, BC Loss: 0.07986033\n",
      "Iteration: 4320, Total Loss: 0.08784936, Physics Loss: 0.00843711, BC Loss: 0.07941226\n",
      "Iteration: 4330, Total Loss: 0.08729491, Physics Loss: 0.00846747, BC Loss: 0.07882745\n",
      "Iteration: 4340, Total Loss: 0.08691426, Physics Loss: 0.00820831, BC Loss: 0.07870596\n",
      "Iteration: 4350, Total Loss: 0.08660289, Physics Loss: 0.00816625, BC Loss: 0.07843664\n",
      "Iteration: 4360, Total Loss: 0.08639786, Physics Loss: 0.00802251, BC Loss: 0.07837534\n",
      "Iteration: 4370, Total Loss: 0.08589251, Physics Loss: 0.00817927, BC Loss: 0.07771324\n",
      "Iteration: 4380, Total Loss: 0.08555032, Physics Loss: 0.00800899, BC Loss: 0.07754134\n",
      "Iteration: 4390, Total Loss: 0.08513048, Physics Loss: 0.00832634, BC Loss: 0.07680413\n",
      "Iteration: 4400, Total Loss: 0.08463046, Physics Loss: 0.00863618, BC Loss: 0.07599428\n",
      "Iteration: 4410, Total Loss: 0.08449549, Physics Loss: 0.00865533, BC Loss: 0.07584016\n",
      "Iteration: 4420, Total Loss: 0.08416634, Physics Loss: 0.00864972, BC Loss: 0.07551663\n",
      "Iteration: 4430, Total Loss: 0.08388045, Physics Loss: 0.00860711, BC Loss: 0.07527333\n",
      "Iteration: 4440, Total Loss: 0.08329745, Physics Loss: 0.00866040, BC Loss: 0.07463705\n",
      "Iteration: 4450, Total Loss: 0.08286006, Physics Loss: 0.00797972, BC Loss: 0.07488034\n",
      "Iteration: 4460, Total Loss: 0.08232144, Physics Loss: 0.00807733, BC Loss: 0.07424410\n",
      "Iteration: 4470, Total Loss: 0.08164445, Physics Loss: 0.00763936, BC Loss: 0.07400509\n",
      "Iteration: 4480, Total Loss: 0.08126204, Physics Loss: 0.00816091, BC Loss: 0.07310113\n",
      "Iteration: 4490, Total Loss: 0.08096415, Physics Loss: 0.00805716, BC Loss: 0.07290699\n",
      "Iteration: 4500, Total Loss: 0.08079961, Physics Loss: 0.00813832, BC Loss: 0.07266130\n",
      "Iteration: 4510, Total Loss: 0.08049333, Physics Loss: 0.00821547, BC Loss: 0.07227786\n",
      "Iteration: 4520, Total Loss: 0.08019636, Physics Loss: 0.00821784, BC Loss: 0.07197852\n",
      "Iteration: 4530, Total Loss: 0.07986677, Physics Loss: 0.00802285, BC Loss: 0.07184391\n",
      "Iteration: 4540, Total Loss: 0.07951307, Physics Loss: 0.00794609, BC Loss: 0.07156698\n",
      "Iteration: 4550, Total Loss: 0.07925420, Physics Loss: 0.00799442, BC Loss: 0.07125977\n",
      "Iteration: 4560, Total Loss: 0.07897636, Physics Loss: 0.00778646, BC Loss: 0.07118991\n",
      "Iteration: 4570, Total Loss: 0.07876741, Physics Loss: 0.00773728, BC Loss: 0.07103013\n",
      "Iteration: 4580, Total Loss: 0.07857730, Physics Loss: 0.00773347, BC Loss: 0.07084383\n",
      "Iteration: 4590, Total Loss: 0.07813916, Physics Loss: 0.00768615, BC Loss: 0.07045300\n",
      "Iteration: 4600, Total Loss: 0.07788112, Physics Loss: 0.00754541, BC Loss: 0.07033572\n",
      "Iteration: 4610, Total Loss: 0.07758604, Physics Loss: 0.00729805, BC Loss: 0.07028799\n",
      "Iteration: 4620, Total Loss: 0.07737133, Physics Loss: 0.00740190, BC Loss: 0.06996943\n",
      "Iteration: 4630, Total Loss: 0.07699452, Physics Loss: 0.00758629, BC Loss: 0.06940822\n",
      "Iteration: 4640, Total Loss: 0.07665969, Physics Loss: 0.00760773, BC Loss: 0.06905197\n",
      "Iteration: 4650, Total Loss: 0.07645737, Physics Loss: 0.00775017, BC Loss: 0.06870719\n",
      "Iteration: 4660, Total Loss: 0.07614635, Physics Loss: 0.00767757, BC Loss: 0.06846878\n",
      "Iteration: 4670, Total Loss: 0.07603145, Physics Loss: 0.00759935, BC Loss: 0.06843209\n",
      "Iteration: 4680, Total Loss: 0.07580848, Physics Loss: 0.00787151, BC Loss: 0.06793697\n",
      "Iteration: 4690, Total Loss: 0.07558890, Physics Loss: 0.00836671, BC Loss: 0.06722219\n",
      "Iteration: 4700, Total Loss: 0.07536203, Physics Loss: 0.00864275, BC Loss: 0.06671928\n",
      "Iteration: 4710, Total Loss: 0.07502997, Physics Loss: 0.00869491, BC Loss: 0.06633506\n",
      "Iteration: 4720, Total Loss: 0.07471316, Physics Loss: 0.00830069, BC Loss: 0.06641247\n",
      "Iteration: 4730, Total Loss: 0.07455012, Physics Loss: 0.00867872, BC Loss: 0.06587140\n",
      "Iteration: 4740, Total Loss: 0.07426008, Physics Loss: 0.00872658, BC Loss: 0.06553350\n",
      "Iteration: 4750, Total Loss: 0.07399932, Physics Loss: 0.00892122, BC Loss: 0.06507811\n",
      "Iteration: 4760, Total Loss: 0.07368047, Physics Loss: 0.00874129, BC Loss: 0.06493918\n",
      "Iteration: 4770, Total Loss: 0.07347737, Physics Loss: 0.00864019, BC Loss: 0.06483719\n",
      "Iteration: 4780, Total Loss: 0.07323916, Physics Loss: 0.00868581, BC Loss: 0.06455336\n",
      "Iteration: 4790, Total Loss: 0.07294475, Physics Loss: 0.00822204, BC Loss: 0.06472270\n",
      "Iteration: 4800, Total Loss: 0.07255420, Physics Loss: 0.00801351, BC Loss: 0.06454068\n",
      "Iteration: 4810, Total Loss: 0.07227724, Physics Loss: 0.00793657, BC Loss: 0.06434067\n",
      "Iteration: 4820, Total Loss: 0.07198127, Physics Loss: 0.00797857, BC Loss: 0.06400271\n",
      "Iteration: 4830, Total Loss: 0.07165216, Physics Loss: 0.00771316, BC Loss: 0.06393900\n",
      "Iteration: 4840, Total Loss: 0.07139725, Physics Loss: 0.00761729, BC Loss: 0.06377996\n",
      "Iteration: 4850, Total Loss: 0.07105027, Physics Loss: 0.00764720, BC Loss: 0.06340307\n",
      "Iteration: 4860, Total Loss: 0.07076316, Physics Loss: 0.00730100, BC Loss: 0.06346217\n",
      "Iteration: 4870, Total Loss: 0.07042721, Physics Loss: 0.00750279, BC Loss: 0.06292442\n",
      "Iteration: 4880, Total Loss: 0.07018520, Physics Loss: 0.00726703, BC Loss: 0.06291817\n",
      "Iteration: 4890, Total Loss: 0.06993923, Physics Loss: 0.00736039, BC Loss: 0.06257883\n",
      "Iteration: 4900, Total Loss: 0.06975210, Physics Loss: 0.00742524, BC Loss: 0.06232686\n",
      "Iteration: 4910, Total Loss: 0.06947555, Physics Loss: 0.00725764, BC Loss: 0.06221791\n",
      "Iteration: 4920, Total Loss: 0.06925755, Physics Loss: 0.00750174, BC Loss: 0.06175581\n",
      "Iteration: 4930, Total Loss: 0.06901573, Physics Loss: 0.00723121, BC Loss: 0.06178451\n",
      "Iteration: 4940, Total Loss: 0.06866246, Physics Loss: 0.00724591, BC Loss: 0.06141656\n",
      "Iteration: 4950, Total Loss: 0.06838635, Physics Loss: 0.00729422, BC Loss: 0.06109213\n",
      "Iteration: 4960, Total Loss: 0.06814441, Physics Loss: 0.00736308, BC Loss: 0.06078133\n",
      "Iteration: 4970, Total Loss: 0.06791033, Physics Loss: 0.00708210, BC Loss: 0.06082823\n",
      "Iteration: 4980, Total Loss: 0.06763277, Physics Loss: 0.00725627, BC Loss: 0.06037650\n",
      "Iteration: 4990, Total Loss: 0.06734511, Physics Loss: 0.00721327, BC Loss: 0.06013184\n",
      "Iteration: 5000, Total Loss: 0.06703478, Physics Loss: 0.00698072, BC Loss: 0.06005407\n",
      "Iteration: 5010, Total Loss: 31.51228523, Physics Loss: 31.42218018, BC Loss: 0.09010419\n",
      "Iteration: 5020, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5030, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5040, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5050, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5060, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5070, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5080, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5090, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5100, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5110, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5120, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5130, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5140, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5150, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5160, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5170, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5180, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5190, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5200, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5210, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5220, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5230, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5240, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5250, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5260, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5270, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5280, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5290, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5300, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5310, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5320, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5330, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5340, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5350, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5360, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5370, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5380, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5390, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5400, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5410, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5420, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5430, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5440, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5450, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5460, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5470, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5480, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5490, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5500, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5510, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5520, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5530, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5540, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5550, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5560, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5570, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5580, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5590, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5600, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5610, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5620, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5630, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5640, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5650, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5660, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5670, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5680, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5690, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5700, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5710, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5720, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5730, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5740, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5750, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5760, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5770, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5780, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5790, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5800, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5810, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5820, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5830, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5840, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5850, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5860, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5870, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5880, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5890, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5900, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5910, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5920, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5930, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5940, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5950, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5960, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5970, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5980, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 5990, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6000, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6010, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6020, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6030, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6040, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6050, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6060, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6070, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6080, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6090, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6100, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6110, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6120, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6130, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6140, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6150, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6160, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6170, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6180, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6190, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6200, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6210, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6220, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6230, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6240, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6250, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6260, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6270, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6280, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6290, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6300, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6310, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6320, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6330, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6340, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6350, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6360, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6370, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6380, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6390, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6400, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6410, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6420, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6430, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6440, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6450, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6460, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6470, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6480, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6490, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6500, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6510, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6520, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6530, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6540, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6550, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6560, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6570, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6580, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6590, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6600, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6610, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6620, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6630, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6640, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6650, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6660, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6670, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6680, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6690, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6700, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6710, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6720, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6730, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6740, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6750, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6760, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6770, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6780, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6790, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6800, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6810, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6820, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6830, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6840, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6850, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6860, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6870, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6880, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6890, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6900, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6910, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6920, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6930, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6940, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6950, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6960, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6970, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6980, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 6990, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7000, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7010, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7020, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7030, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7040, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7050, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7060, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7070, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7080, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7090, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7100, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7110, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7120, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7130, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7140, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7150, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7160, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7170, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7180, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7190, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7200, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7210, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7220, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7230, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7240, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7250, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7260, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7270, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7280, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7290, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7300, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7310, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7320, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7330, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7340, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7350, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7360, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7370, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7380, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7390, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7400, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7410, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7420, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7430, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7440, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7450, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7460, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7470, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7480, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7490, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7500, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7510, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7520, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7530, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7540, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7550, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7560, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7570, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7580, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7590, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7600, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7610, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7620, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7630, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7640, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7650, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7660, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7670, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7680, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7690, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7700, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7710, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7720, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7730, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7740, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7750, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7760, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7770, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7780, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7790, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7800, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7810, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7820, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7830, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7840, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7850, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7860, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7870, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7880, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7890, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7900, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7910, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7920, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7930, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7940, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7950, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7960, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7970, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7980, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 7990, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8000, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8010, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8020, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8030, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8040, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8050, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8060, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8070, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8080, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8090, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8100, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8110, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8120, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8130, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8140, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8150, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8160, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8170, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8180, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8190, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8200, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8210, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8220, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8230, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8240, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8250, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8260, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8270, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8280, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8290, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8300, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8310, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8320, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8330, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8340, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8350, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8360, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8370, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8380, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8390, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8400, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8410, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8420, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8430, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8440, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8450, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8460, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8470, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8480, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8490, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8500, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8510, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8520, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8530, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8540, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8550, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8560, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8570, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8580, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8590, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8600, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8610, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8620, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8630, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8640, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8650, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8660, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8670, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8680, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8690, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8700, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8710, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8720, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8730, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8740, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8750, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8760, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8770, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8780, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8790, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8800, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8810, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8820, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8830, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8840, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8850, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8860, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8870, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8880, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8890, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8900, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8910, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8920, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8930, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8940, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8950, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8960, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8970, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8980, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 8990, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9000, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9010, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9020, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9030, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9040, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9050, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9060, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9070, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9080, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9090, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9100, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9110, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9120, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9130, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9140, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9150, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9160, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9170, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9180, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9190, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9200, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9210, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9220, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9230, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9240, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9250, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9260, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9270, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9280, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9290, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9300, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9310, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9320, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9330, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9340, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9350, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9360, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9370, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9380, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9390, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9400, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9410, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9420, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9430, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9440, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9450, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9460, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9470, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9480, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9490, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9500, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9510, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9520, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9530, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9540, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9550, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9560, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9570, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9580, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9590, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9600, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9610, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9620, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9630, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9640, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9650, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9660, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9670, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9680, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9690, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9700, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9710, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9720, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9730, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9740, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9750, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9760, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9770, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9780, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9790, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9800, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9810, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9820, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9830, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9840, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9850, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9860, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9870, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9880, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9890, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9900, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9910, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9920, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9930, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9940, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9950, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9960, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9970, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9980, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 9990, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 10000, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 10010, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 10020, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 10030, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 10040, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 10050, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 10060, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 10070, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "Iteration: 10080, Total Loss: nan, Physics Loss: nan, BC Loss: nan\n",
      "训练完成!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# 检查CUDA是否可用\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 物理参数\n",
    "nu = 0.01  # 运动粘度\n",
    "rho = 1.0  # 密度\n",
    "inlet_velocity = 1.0  # 入口速度\n",
    "\n",
    "class PINN_NavierStokes():\n",
    "    def __init__(self, layers, cylinder_center=(0.5, 0.2), cylinder_radius=0.05):\n",
    "        # 网络结构\n",
    "        self.layers = layers\n",
    "        self.cylinder_center = cylinder_center\n",
    "        self.cylinder_radius = cylinder_radius\n",
    "        \n",
    "        # 初始化网络\n",
    "        self.network()\n",
    "        \n",
    "        # 将网络移动到GPU\n",
    "        self.net = self.net.to(device)\n",
    "        \n",
    "        # 优化器\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.net.parameters(), \n",
    "            lr=0.1, \n",
    "            max_iter=1000, \n",
    "            max_eval=1000,\n",
    "            history_size=50, \n",
    "            tolerance_grad=1e-07, \n",
    "            tolerance_change=1e-09,\n",
    "            line_search_fn=\"strong_wolfe\"\n",
    "        )\n",
    "        \n",
    "        self.mse = nn.MSELoss()\n",
    "        self.iter = 0\n",
    "\n",
    "    def network(self):\n",
    "        \"\"\"构建神经网络\"\"\"\n",
    "        layers_list = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            layer = nn.Linear(self.layers[i], self.layers[i+1])\n",
    "            # 初始化权重\n",
    "            if i < len(self.layers) - 2:\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "            else:\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "            layers_list.append(layer)\n",
    "            if i < len(self.layers) - 2:  # 最后一层不需要激活函数\n",
    "                layers_list.append(nn.Tanh())\n",
    "        \n",
    "        self.net = nn.Sequential(*layers_list)\n",
    "\n",
    "    def governing_equations(self, x, y, t):\n",
    "        \"\"\"计算Navier-Stokes方程的残差\"\"\"\n",
    "        # 确保输入张量需要梯度\n",
    "        x = x.requires_grad_(True)\n",
    "        y = y.requires_grad_(True)\n",
    "        t = t.requires_grad_(True)\n",
    "        \n",
    "        # 网络输出: [u, v, p] (速度分量和压力)\n",
    "        input_tensor = torch.cat([x, y, t], dim=1)\n",
    "        output = self.net(input_tensor)\n",
    "        u, v, p = output[:, 0:1], output[:, 1:2], output[:, 2:3]\n",
    "        \n",
    "        # 计算速度的偏导数\n",
    "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "        u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y), create_graph=True)[0]\n",
    "        \n",
    "        v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "        v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "        v_t = torch.autograd.grad(v, t, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "        v_xx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(v_x), create_graph=True)[0]\n",
    "        v_yy = torch.autograd.grad(v_y, y, grad_outputs=torch.ones_like(v_y), create_graph=True)[0]\n",
    "        \n",
    "        # 压力梯度\n",
    "        p_x = torch.autograd.grad(p, x, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "        p_y = torch.autograd.grad(p, y, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "        \n",
    "        # Navier-Stokes方程残差\n",
    "        f = u_t + u * u_x + v * u_y + (1.0/rho) * p_x - nu * (u_xx + u_yy)\n",
    "        g = v_t + u * v_x + v * v_y + (1.0/rho) * p_y - nu * (v_xx + v_yy)\n",
    "        \n",
    "        # 连续性方程残差 (质量守恒)\n",
    "        continuity = u_x + v_y\n",
    "        \n",
    "        return u, v, p, f, g, continuity\n",
    "\n",
    "    def boundary_conditions(self, x, y, t):\n",
    "        \"\"\"计算边界条件损失\"\"\"\n",
    "        # 确保输入张量需要梯度\n",
    "        x = x.requires_grad_(True)\n",
    "        y = y.requires_grad_(True)\n",
    "        t = t.requires_grad_(True)\n",
    "        \n",
    "        # 网络输出\n",
    "        input_tensor = torch.cat([x, y, t], dim=1)\n",
    "        output = self.net(input_tensor)\n",
    "        u, v, p = output[:, 0:1], output[:, 1:2], output[:, 2:3]\n",
    "        \n",
    "        # 计算到圆柱中心的距离\n",
    "        dist_to_cylinder = torch.sqrt((x - self.cylinder_center[0])**2 + (y - self.cylinder_center[1])**2)\n",
    "        cylinder_mask = dist_to_cylinder <= self.cylinder_radius + 0.005\n",
    "        \n",
    "        # 入口边界 (x=0)\n",
    "        inlet_mask = torch.abs(x) < 0.005\n",
    "        \n",
    "        # 出口边界 (x=1) - 自然边界条件，压力梯度为0\n",
    "        outlet_mask = torch.abs(x - 1.0) < 0.005\n",
    "        \n",
    "        # 上下边界 (y=0, y=0.4) - 无滑移条件\n",
    "        wall_mask = (torch.abs(y) < 0.005) | (torch.abs(y - 0.4) < 0.005)\n",
    "        \n",
    "        # 计算边界损失\n",
    "        bc_loss = 0.0\n",
    "        \n",
    "        # 圆柱边界: 无滑移条件\n",
    "        if cylinder_mask.any():\n",
    "            u_cyl = u[cylinder_mask]\n",
    "            v_cyl = v[cylinder_mask]\n",
    "            bc_loss += torch.mean(u_cyl**2) + torch.mean(v_cyl**2)\n",
    "        \n",
    "        # 入口边界: 指定入口速度\n",
    "        if inlet_mask.any():\n",
    "            u_inlet = u[inlet_mask]\n",
    "            v_inlet = v[inlet_mask]\n",
    "            bc_loss += torch.mean((u_inlet - inlet_velocity)**2) + torch.mean(v_inlet**2)\n",
    "        \n",
    "        # 壁面边界: 无滑移条件\n",
    "        if wall_mask.any():\n",
    "            u_wall = u[wall_mask]\n",
    "            v_wall = v[wall_mask]\n",
    "            bc_loss += torch.mean(u_wall**2) + torch.mean(v_wall**2)\n",
    "            \n",
    "        return bc_loss\n",
    "\n",
    "    def closure(self, x_col, y_col, t_col, x_bc, y_bc, t_bc):\n",
    "        \"\"\"损失函数\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # 物理方程损失 (残差)\n",
    "        _, _, _, f, g, continuity = self.governing_equations(x_col, y_col, t_col)\n",
    "        physics_loss = torch.mean(f**2) + torch.mean(g**2) + torch.mean(continuity**2)\n",
    "        \n",
    "        # 边界条件损失\n",
    "        bc_loss = self.boundary_conditions(x_bc, y_bc, t_bc)\n",
    "        \n",
    "        # 总损失\n",
    "        total_loss = physics_loss + bc_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        \n",
    "        self.iter += 1\n",
    "        if self.iter % 10 == 0:\n",
    "            print(f'Iteration: {self.iter}, Total Loss: {total_loss.item():.8f}, '\n",
    "                  f'Physics Loss: {physics_loss.item():.8f}, BC Loss: {bc_loss.item():.8f}')\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    def train(self, epochs=100):\n",
    "        \"\"\"训练函数\"\"\"\n",
    "        self.net.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # 生成训练点\n",
    "            # 物理域内的点 (用于物理方程约束)\n",
    "            n_col = 500  # 配置点数量\n",
    "            x_col = torch.rand(n_col, 1, device=device) * 1.0  # x: 0 to 1\n",
    "            y_col = torch.rand(n_col, 1, device=device) * 0.4  # y: 0 to 0.4\n",
    "            t_col = torch.rand(n_col, 1, device=device) * 0.5  # t: 0 to 0.5\n",
    "            \n",
    "            # 边界点 (用于边界条件约束)\n",
    "            n_bc = 400\n",
    "            # 圆柱边界采样\n",
    "            theta = torch.rand(n_bc//4, 1, device=device) * 2 * np.pi\n",
    "            x_cyl = self.cylinder_center[0] + self.cylinder_radius * torch.cos(theta)\n",
    "            y_cyl = self.cylinder_center[1] + self.cylinder_radius * torch.sin(theta)\n",
    "            \n",
    "            # 入口边界 (x=0)\n",
    "            y_inlet = torch.rand(n_bc//4, 1, device=device) * 0.4\n",
    "            x_inlet = torch.zeros_like(y_inlet)\n",
    "            \n",
    "            # 出口边界 (x=1)\n",
    "            y_outlet = torch.rand(n_bc//4, 1, device=device) * 0.4\n",
    "            x_outlet = torch.ones_like(y_outlet)\n",
    "            \n",
    "            # 上下边界\n",
    "            x_wall = torch.rand(n_bc//2, 1, device=device)\n",
    "            y_wall_top = torch.ones(n_bc//4, 1, device=device) * 0.4\n",
    "            y_wall_bottom = torch.zeros(n_bc//4, 1, device=device)\n",
    "            \n",
    "            # 合并边界点\n",
    "            x_bc = torch.cat([x_cyl, x_inlet, x_outlet, x_wall[:n_bc//4], x_wall[n_bc//4:]], dim=0)\n",
    "            y_bc = torch.cat([y_cyl, y_inlet, y_outlet, y_wall_top, y_wall_bottom], dim=0)\n",
    "            t_bc = torch.rand_like(x_bc) * 0.5\n",
    "            \n",
    "            # 调用优化器\n",
    "            def closure_wrapper():\n",
    "                return self.closure(x_col, y_col, t_col, x_bc, y_bc, t_bc)\n",
    "            \n",
    "            self.optimizer.step(closure_wrapper)\n",
    "\n",
    "    def predict(self, x, y, t):\n",
    "        \"\"\"预测速度场和压力场\"\"\"\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            x_tensor = torch.tensor(x, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "            y_tensor = torch.tensor(y, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "            t_tensor = torch.tensor(t, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "            \n",
    "            input_tensor = torch.cat([x_tensor, y_tensor, t_tensor], dim=1)\n",
    "            output = self.net(input_tensor)\n",
    "            u, v, p = output[:, 0:1], output[:, 1:2], output[:, 2:3]\n",
    "            \n",
    "        return u.cpu().numpy(), v.cpu().numpy(), p.cpu().numpy()\n",
    "\n",
    "# 创建PINN模型\n",
    "layers = [3, 50, 50, 50, 50, 50, 3]  # [输入: x,y,t -> 输出: u, v, p]\n",
    "pinn = PINN_NavierStokes(layers)\n",
    "\n",
    "# 训练模型\n",
    "print(\"开始训练PINN模型...\")\n",
    "try:\n",
    "    pinn.train(epochs=10)  # 减少epoch数以加快训练\n",
    "    print(\"训练完成!\")\n",
    "except Exception as e:\n",
    "    print(f\"训练过程中出现错误: {e}\")\n",
    "\n",
    "# 创建网格用于可视化\n",
    "x_range = np.linspace(0, 1, 100)\n",
    "y_range = np.linspace(0, 0.4, 50)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "x_flat = X.flatten()\n",
    "y_flat = Y.flatten()\n",
    "\n",
    "# 静态可视化 - 速度流线图\n",
    "def plot_flow_field():\n",
    "    fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 计算t=0.25时刻的场\n",
    "    t_static = np.ones_like(x_flat) * 0.25\n",
    "    try:\n",
    "        u_static, v_static, p_static = pinn.predict(x_flat, y_flat, t_static)\n",
    "        \n",
    "        U_static = u_static.reshape(X.shape)\n",
    "        V_static = v_static.reshape(Y.shape)\n",
    "        P_static = p_static.reshape(X.shape)\n",
    "        \n",
    "        # 左图：压力场\n",
    "        contour1 = ax1.contourf(X, Y, P_static, levels=50, cmap='viridis')\n",
    "        ax1.streamplot(X, Y, U_static.reshape(X.shape), V_static.reshape(Y.shape), \n",
    "                      color='white', density=1.5, linewidth=0.8, arrowsize=1)\n",
    "        cylinder1 = plt.Circle(pinn.cylinder_center, pinn.cylinder_radius, color='red', zorder=5)\n",
    "        ax1.add_patch(cylinder1)\n",
    "        ax1.set_xlabel('x')\n",
    "        ax1.set_ylabel('y')\n",
    "        ax1.set_title('Pressure Field with Velocity Streamlines')\n",
    "        plt.colorbar(contour1, ax=ax1)\n",
    "        \n",
    "        # 右图：速度幅值\n",
    "        speed = np.sqrt(U_static**2 + V_static**2)\n",
    "        contour2 = ax2.contourf(X, Y, speed, levels=50, cmap='plasma')\n",
    "        ax2.streamplot(X, Y, U_static, V_static, color='white', density=1.5, linewidth=0.8, arrowsize=1)\n",
    "        cylinder2 = plt.Circle(pinn.cylinder_center, pinn.cylinder_radius, color='red', zorder=5)\n",
    "        ax2.add_patch(cylinder2)\n",
    "        ax2.set_xlabel('x')\n",
    "        ax2.set_ylabel('y')\n",
    "        ax2.set_title('Velocity Magnitude with Streamlines')\n",
    "        plt.colorbar(contour2, ax=ax2)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"可视化过程中出现错误: {e}\")\n",
    "        print(\"这可能是因为模型训练不充分或网络输出不稳定\")\n",
    "\n",
    "plot_flow_field()\n",
    "\n",
    "# 如果训练成功，可以尝试动画\n",
    "try:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    def animate(t_frame):\n",
    "        ax.clear()\n",
    "        \n",
    "        # 计算当前时间步的速度场和压力场\n",
    "        t_flat = np.ones_like(x_flat) * t_frame * 0.05  # 时间从0到1.5变化\n",
    "        \n",
    "        u_pred, v_pred, p_pred = pinn.predict(x_flat, y_flat, t_flat)\n",
    "        \n",
    "        # 重塑为网格形式\n",
    "        U = u_pred.reshape(X.shape)\n",
    "        V = v_pred.reshape(Y.shape)\n",
    "        P = p_pred.reshape(X.shape)\n",
    "        \n",
    "        # 绘制压力场\n",
    "        contour = ax.contourf(X, Y, P, levels=50, cmap='viridis')\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('y')\n",
    "        ax.set_title(f'Pressure Field at t={t_frame*0.05:.2f}')\n",
    "        \n",
    "        # 绘制圆柱\n",
    "        cylinder = plt.Circle(pinn.cylinder_center, pinn.cylinder_radius, color='red', zorder=5)\n",
    "        ax.add_patch(cylinder)\n",
    "        \n",
    "        # 可选：绘制速度矢量\n",
    "        skip = 5  # 每5个点绘制一个矢量\n",
    "        ax.quiver(X[::skip, ::skip], Y[::skip, ::skip], \n",
    "                  U[::skip, ::skip], V[::skip, ::skip], \n",
    "                  alpha=0.5, scale=5, color='white', width=0.002)\n",
    "        \n",
    "        if t_frame == 0:  # 只在第一帧添加颜色条\n",
    "            plt.colorbar(contour, ax=ax)\n",
    "    \n",
    "    # 创建动画\n",
    "    ani = animation.FuncAnimation(fig, animate, frames=30, interval=300, repeat=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"动画创建失败: {e}\")\n",
    "    print(\"可能是由于模型训练不充分导致的数值不稳定\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1331a858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_312620/2173895397.py:322: UserWarning: Adding colorbar to a different Figure <Figure size 1000x600 with 2 Axes> than <Figure size 640x480 with 0 Axes> which fig.colorbar is called on.\n",
      "  plt.colorbar(contour, ax=ax)\n",
      "/tmp/ipykernel_312620/2173895397.py:322: UserWarning: Adding colorbar to a different Figure <Figure size 1000x600 with 3 Axes> than <Figure size 640x480 with 0 Axes> which fig.colorbar is called on.\n",
      "  plt.colorbar(contour, ax=ax)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.animation import PillowWriter\n",
    "\n",
    "ani.save('karman_vortex_street.gif', writer=PillowWriter(fps=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d84cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf58264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8f5557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a15560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a89c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069af841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bfbfd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e4c2a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecda7a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "data = scipy.io.loadmat('./NS_PINN/cylinder_wake.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f69b0364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Fri Sep 22 23:02:15 2017',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'X_star': array([[ 1.        , -2.        ],\n",
       "        [ 1.07070707, -2.        ],\n",
       "        [ 1.14141414, -2.        ],\n",
       "        ...,\n",
       "        [ 7.85858586,  2.        ],\n",
       "        [ 7.92929293,  2.        ],\n",
       "        [ 8.        ,  2.        ]]),\n",
       " 't': array([[ 0. ],\n",
       "        [ 0.1],\n",
       "        [ 0.2],\n",
       "        [ 0.3],\n",
       "        [ 0.4],\n",
       "        [ 0.5],\n",
       "        [ 0.6],\n",
       "        [ 0.7],\n",
       "        [ 0.8],\n",
       "        [ 0.9],\n",
       "        [ 1. ],\n",
       "        [ 1.1],\n",
       "        [ 1.2],\n",
       "        [ 1.3],\n",
       "        [ 1.4],\n",
       "        [ 1.5],\n",
       "        [ 1.6],\n",
       "        [ 1.7],\n",
       "        [ 1.8],\n",
       "        [ 1.9],\n",
       "        [ 2. ],\n",
       "        [ 2.1],\n",
       "        [ 2.2],\n",
       "        [ 2.3],\n",
       "        [ 2.4],\n",
       "        [ 2.5],\n",
       "        [ 2.6],\n",
       "        [ 2.7],\n",
       "        [ 2.8],\n",
       "        [ 2.9],\n",
       "        [ 3. ],\n",
       "        [ 3.1],\n",
       "        [ 3.2],\n",
       "        [ 3.3],\n",
       "        [ 3.4],\n",
       "        [ 3.5],\n",
       "        [ 3.6],\n",
       "        [ 3.7],\n",
       "        [ 3.8],\n",
       "        [ 3.9],\n",
       "        [ 4. ],\n",
       "        [ 4.1],\n",
       "        [ 4.2],\n",
       "        [ 4.3],\n",
       "        [ 4.4],\n",
       "        [ 4.5],\n",
       "        [ 4.6],\n",
       "        [ 4.7],\n",
       "        [ 4.8],\n",
       "        [ 4.9],\n",
       "        [ 5. ],\n",
       "        [ 5.1],\n",
       "        [ 5.2],\n",
       "        [ 5.3],\n",
       "        [ 5.4],\n",
       "        [ 5.5],\n",
       "        [ 5.6],\n",
       "        [ 5.7],\n",
       "        [ 5.8],\n",
       "        [ 5.9],\n",
       "        [ 6. ],\n",
       "        [ 6.1],\n",
       "        [ 6.2],\n",
       "        [ 6.3],\n",
       "        [ 6.4],\n",
       "        [ 6.5],\n",
       "        [ 6.6],\n",
       "        [ 6.7],\n",
       "        [ 6.8],\n",
       "        [ 6.9],\n",
       "        [ 7. ],\n",
       "        [ 7.1],\n",
       "        [ 7.2],\n",
       "        [ 7.3],\n",
       "        [ 7.4],\n",
       "        [ 7.5],\n",
       "        [ 7.6],\n",
       "        [ 7.7],\n",
       "        [ 7.8],\n",
       "        [ 7.9],\n",
       "        [ 8. ],\n",
       "        [ 8.1],\n",
       "        [ 8.2],\n",
       "        [ 8.3],\n",
       "        [ 8.4],\n",
       "        [ 8.5],\n",
       "        [ 8.6],\n",
       "        [ 8.7],\n",
       "        [ 8.8],\n",
       "        [ 8.9],\n",
       "        [ 9. ],\n",
       "        [ 9.1],\n",
       "        [ 9.2],\n",
       "        [ 9.3],\n",
       "        [ 9.4],\n",
       "        [ 9.5],\n",
       "        [ 9.6],\n",
       "        [ 9.7],\n",
       "        [ 9.8],\n",
       "        [ 9.9],\n",
       "        [10. ],\n",
       "        [10.1],\n",
       "        [10.2],\n",
       "        [10.3],\n",
       "        [10.4],\n",
       "        [10.5],\n",
       "        [10.6],\n",
       "        [10.7],\n",
       "        [10.8],\n",
       "        [10.9],\n",
       "        [11. ],\n",
       "        [11.1],\n",
       "        [11.2],\n",
       "        [11.3],\n",
       "        [11.4],\n",
       "        [11.5],\n",
       "        [11.6],\n",
       "        [11.7],\n",
       "        [11.8],\n",
       "        [11.9],\n",
       "        [12. ],\n",
       "        [12.1],\n",
       "        [12.2],\n",
       "        [12.3],\n",
       "        [12.4],\n",
       "        [12.5],\n",
       "        [12.6],\n",
       "        [12.7],\n",
       "        [12.8],\n",
       "        [12.9],\n",
       "        [13. ],\n",
       "        [13.1],\n",
       "        [13.2],\n",
       "        [13.3],\n",
       "        [13.4],\n",
       "        [13.5],\n",
       "        [13.6],\n",
       "        [13.7],\n",
       "        [13.8],\n",
       "        [13.9],\n",
       "        [14. ],\n",
       "        [14.1],\n",
       "        [14.2],\n",
       "        [14.3],\n",
       "        [14.4],\n",
       "        [14.5],\n",
       "        [14.6],\n",
       "        [14.7],\n",
       "        [14.8],\n",
       "        [14.9],\n",
       "        [15. ],\n",
       "        [15.1],\n",
       "        [15.2],\n",
       "        [15.3],\n",
       "        [15.4],\n",
       "        [15.5],\n",
       "        [15.6],\n",
       "        [15.7],\n",
       "        [15.8],\n",
       "        [15.9],\n",
       "        [16. ],\n",
       "        [16.1],\n",
       "        [16.2],\n",
       "        [16.3],\n",
       "        [16.4],\n",
       "        [16.5],\n",
       "        [16.6],\n",
       "        [16.7],\n",
       "        [16.8],\n",
       "        [16.9],\n",
       "        [17. ],\n",
       "        [17.1],\n",
       "        [17.2],\n",
       "        [17.3],\n",
       "        [17.4],\n",
       "        [17.5],\n",
       "        [17.6],\n",
       "        [17.7],\n",
       "        [17.8],\n",
       "        [17.9],\n",
       "        [18. ],\n",
       "        [18.1],\n",
       "        [18.2],\n",
       "        [18.3],\n",
       "        [18.4],\n",
       "        [18.5],\n",
       "        [18.6],\n",
       "        [18.7],\n",
       "        [18.8],\n",
       "        [18.9],\n",
       "        [19. ],\n",
       "        [19.1],\n",
       "        [19.2],\n",
       "        [19.3],\n",
       "        [19.4],\n",
       "        [19.5],\n",
       "        [19.6],\n",
       "        [19.7],\n",
       "        [19.8],\n",
       "        [19.9]]),\n",
       " 'U_star': array([[[ 1.11419216e+00,  1.11755721e+00,  1.11956933e+00, ...,\n",
       "           1.16105653e+00,  1.16209316e+00,  1.16287446e+00],\n",
       "         [-4.09649775e-03,  1.09212754e-03,  3.28231641e-03, ...,\n",
       "          -1.14679740e-02, -1.46226631e-02, -1.78649950e-02]],\n",
       " \n",
       "        [[ 1.11102707e+00,  1.11424311e+00,  1.11623549e+00, ...,\n",
       "           1.16119046e+00,  1.16250501e+00,  1.16355997e+00],\n",
       "         [ 3.93130751e-04,  6.09231658e-03,  8.54240777e-03, ...,\n",
       "          -3.69331211e-03, -6.90960992e-03, -1.02375054e-02]],\n",
       " \n",
       "        [[ 1.10748452e+00,  1.11049848e+00,  1.11244362e+00, ...,\n",
       "           1.16080365e+00,  1.16241577e+00,  1.16375737e+00],\n",
       "         [ 4.42777717e-03,  1.06585027e-02,  1.33831464e-02, ...,\n",
       "           4.14245483e-03,  8.86993440e-04, -2.50609725e-03]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 1.04183122e+00,  1.07052197e+00,  1.08570786e+00, ...,\n",
       "           1.03561691e+00,  1.01482352e+00,  9.95607444e-01],\n",
       "         [-1.53233747e-01, -1.52906494e-01, -1.50934775e-01, ...,\n",
       "           1.70410243e-01,  1.74234015e-01,  1.74978509e-01]],\n",
       " \n",
       "        [[ 1.03144583e+00,  1.05940483e+00,  1.07424671e+00, ...,\n",
       "           1.05086925e+00,  1.02921296e+00,  1.00907934e+00],\n",
       "         [-1.52570327e-01, -1.53932097e-01, -1.52973975e-01, ...,\n",
       "           1.65974219e-01,  1.72307867e-01,  1.75206081e-01]],\n",
       " \n",
       "        [[ 1.02128975e+00,  1.04848021e+00,  1.06294889e+00, ...,\n",
       "           1.06667284e+00,  1.04430400e+00,  1.02333582e+00],\n",
       "         [-1.51330184e-01, -1.54252780e-01, -1.54219310e-01, ...,\n",
       "           1.59519620e-01,  1.68613705e-01,  1.73914810e-01]]]),\n",
       " 'p_star': array([[-0.10815457, -0.11115509, -0.1123679 , ..., -0.10035143,\n",
       "         -0.09845376, -0.09658143],\n",
       "        [-0.10560371, -0.10881921, -0.1101648 , ..., -0.10137256,\n",
       "         -0.09959008, -0.09781616],\n",
       "        [-0.10257633, -0.10599348, -0.10746686, ..., -0.10204929,\n",
       "         -0.10039933, -0.09874095],\n",
       "        ...,\n",
       "        [ 0.0066028 ,  0.00272234,  0.00081532, ...,  0.01152894,\n",
       "          0.01414437,  0.01651662],\n",
       "        [ 0.007892  ,  0.00402109,  0.00209731, ...,  0.00927242,\n",
       "          0.01200894,  0.01451151],\n",
       "        [ 0.00915729,  0.00530504,  0.00337089, ...,  0.00691424,\n",
       "          0.00975565,  0.01238328]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76759390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'X_star', 't', 'U_star', 'p_star'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "352a0b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['X_star'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab11d28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['t'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc4ef61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83328ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
