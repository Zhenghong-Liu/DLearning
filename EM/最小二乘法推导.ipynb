{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最小二乘法\n",
    "\n",
    "考虑有一组m个数据点(x1, y1), (x2, y2),……, (xm, ym)\n",
    "\n",
    "模型函数表示为： \n",
    "$$ y(x) = f(x, \\beta)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1)$$\n",
    "\n",
    "其中 $\\beta = (\\beta_1, \\beta_2, \\dots, \\beta_n)$ 表示待拟合参数，n表示待拟合参数的数量，其中m > n\n",
    "\n",
    "模型`残差`定义为： \n",
    "$$r_i = y_i - f(x_i, \\beta)\\ \\ \\ \\ for\\ i = 1, 2, \\dots, m\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (2)$$\n",
    "\n",
    "最小二乘法的目标就是通过最小化残差平方和RSS来估计 $\\beta$\n",
    "\n",
    "$$RSS(\\beta) = \\| r \\|^2 = r^Tr = \\sum_{i=1}^n r_i^2$$\n",
    "\n",
    "## 推导线性最小二乘法公式\n",
    "\n",
    "在线性最小二乘法中，拟合的数学表达式：\n",
    "$$y = \\sum_{j=1}^n f_j(x)\\beta_j = X\\beta$$\n",
    "其中 $\\beta$ 是一个n个元素的列向量，X是一个m行n列的矩阵，m是数据点个数，n是拟合参数的个数。\n",
    "\n",
    "矩阵求导公式, 下面要用到：\n",
    "\n",
    "公式1: $$\\frac{dAX}{dX} = A^T$$\n",
    "\n",
    "公式2: $$\\frac{dX^TA}{dX} = A$$\n",
    "\n",
    "公式3: $$\\frac{dX^TAX}{dX} = (A + A^T)X$$\n",
    "\n",
    "最小二乘法要求对 $\\beta$ 求解，使得RSS最小，即：$$ \\frac{\\partial RSS(\\beta)}{\\partial \\beta} = 0$$\n",
    "\n",
    "即：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial r^Tr}{\\partial \\beta}\n",
    "     = \\frac{\\partial ((y - f(x, \\beta))^T(y - f(x,\\beta)))}{\\partial \\beta} \\\\[4mm]\n",
    "     = \\frac{\\partial ((y - X\\beta)^T(y - X\\beta))}{\\partial \\beta} \\\\[4mm]\n",
    "     = \\frac{\\partial ((y^T -\\beta^TX^T)(y - X\\beta))}{\\partial \\beta}\\\\[4mm]\n",
    "     = \\frac{\\partial(y^Ty - y^TX\\beta-\\beta^TX^Ty+\\beta^TX^TX\\beta)}{\\partial \\beta}\\\\[4mm]\n",
    "     = \\frac{\\partial y^Ty}{\\partial \\beta} - \\frac{\\partial y^TX\\beta}{\\partial \\beta}\n",
    "     - \\frac{\\partial \\beta^TX^Ty}{\\partial \\beta} + \\frac{\\partial \\beta^TX^TX\\beta}{\\partial \\beta}\n",
    "     = 0\n",
    "\\end{align}     \n",
    "$$\n",
    "\n",
    "根据矩阵的求导公式，即可得到：\n",
    "$$\n",
    "-X^Ty -X^Ty + (X^TX + (X^TX)^T)\\beta = 0\n",
    "$$\n",
    "简化可得： $$X^TX\\beta = X^Ty$$\n",
    "因此 $$\\beta = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "方法2====================================================================================================================\n",
    "$$\n",
    "\\frac{\\partial r^Tr}{\\partial \\beta}\n",
    "     = \\frac{\\partial \\| y - X\\beta \\|^2}{\\partial \\beta} \\\\[4mm]\n",
    "     = \\frac{\\partial (y^Ty - 2y^TX\\beta + \\beta^TX^TX\\beta)}{\\partial \\beta} \\\\[4mm]\n",
    "     = 0 - 2X^Ty + (X^TX + (X^TX)^T)\\beta = 0\n",
    "$$\n",
    "简化可得： $$X^TX\\beta = X^Ty$$\n",
    "因此 $$\\beta = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "至此，线性最小二乘法的公式推导完毕。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非线性最小二乘法\n",
    "对于线性最小二乘法我们可以分析他的导函数形式，但是对于复杂的情况，导函数不好写或者根本写不出来，这时需要采用迭代的方式求取。\n",
    "\n",
    "**迭代方式**\n",
    "\n",
    "1. 给定初始值$X_0$\n",
    "2. 对于第k次迭代，寻找一个合适的增量 $\\Delta X_k$, 使得 $\\| f(X_k + \\Delta X_k) \\|^2$最小\n",
    "3. 若$\\Delta X_k$足够小，则停止\n",
    "4. 否则， 令 $X_{k+1} = X_k + \\Delta X_k$, 返回2\n",
    "\n",
    "**如何寻找增量**\n",
    "\n",
    "泰勒展开：\n",
    "$$\\| f(\\beta_k + \\Delta \\beta) \\|^2 \\approx \\| f(\\beta_k)\\|^2 + J(\\beta_k)\\Delta \\beta + \\frac{1}{2}\\Delta \\beta^TH(\\beta_k)\\Delta \\beta$$\n",
    "\n",
    "其中H是m行m列的Hessian矩阵，存在关系有 $H = J^TJ$ ,其中J是雅可比矩阵\n",
    "\n",
    "### 梯度下降法： 保留一阶\n",
    "\n",
    "即 $$\\| f(\\beta_k + \\Delta \\beta) \\|^2 \\approx \\| f(\\beta_k)\\|^2 + J(\\beta_k)\\Delta \\beta$$\n",
    "\n",
    "$J_F(x)$ 即为F(x)的梯度，沿着引力势梯度相反的方向高度下降的最快。\n",
    "\n",
    "从某个初值 $\\beta_0$出发， 每次都取 $\\Delta \\beta = -\\alpha J^T(\\beta)$ ，$\\alpha$ 是一个控制步长大小的系数。\n",
    "\n",
    "\n",
    "### 牛顿迭代法： 保留二阶。\n",
    "$$\\| f(\\beta_k + \\Delta \\beta) \\|^2 \\approx \\| f(\\beta_k)\\|^2 + J(\\beta_k)\\Delta \\beta + \\frac{1}{2}\\Delta \\beta^TH(\\beta_k)\\Delta \\beta$$\n",
    "\n",
    "将上式对 $\\Delta \\beta$ 求偏导，令其为0，则有关于 $\\Delta \\beta$的方程\n",
    "$$J(\\beta_k) + \\frac{1}{2}[H(\\beta_k + H(\\beta_k)^T)]\\Delta \\beta = 0$$\n",
    "\n",
    "hessian矩阵为对称矩阵， $H^T = H$， 因此\n",
    "$$ J(\\beta_k) + H(\\beta_k)\\Delta \\beta = 0$$\n",
    "即\n",
    "$$\\Delta \\beta = -H^{-1}J$$\n",
    "\n",
    "\n",
    "### 高斯-牛顿法\n",
    "\n",
    "把$r_i$作泰勒展开：\n",
    "$$r_i(\\beta + \\Delta \\beta) \\approx r_i(\\beta) + J_i(\\beta)\\Delta \\beta$$\n",
    "则：\n",
    "$$r(\\beta + \\Delta \\beta)  = r(\\beta) + J_r(\\beta)\\Delta \\beta$$\n",
    "\n",
    "因此：\n",
    "$$\n",
    "\\frac{\\partial \\| r(\\beta) + J(\\beta)\\Delta \\beta \\|^2}{\\partial \\Delta \\beta} \\\\[4mm]\n",
    "= \\frac{\\partial (\\|f(\\beta)\\|^2 + 2r(\\beta)^TJ(\\beta)\\Delta \\beta + \\Delta \\beta^TJ(\\beta)^TJ(\\beta)\\Delta \\beta)}{\\partial \\Delta \\beta} \\\\[4mm]\n",
    "= 2J(\\beta)^Tr(\\beta) + 2J(\\beta)^TJ(\\beta)\\Delta \\beta = 0\n",
    "$$\n",
    "\n",
    "因此：\n",
    "$$\n",
    "J(\\beta)^TJ(\\beta)\\Delta \\beta = -J(\\beta)^Tr(\\beta)\n",
    "$$\n",
    "\n",
    "从而：\n",
    "$$\n",
    "\\Delta \\beta = -(J^TJ)^{-1}J^Tr\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拟合参数的协方差矩阵的计算\n",
    "\n",
    "$\\Delta \\beta$的协方差矩阵可以写为：\n",
    "$$\n",
    "\\text{Cov}(\\boldsymbol{\\Delta \\beta}) = \\mathbb{E}[\\Delta \\boldsymbol{\\beta} \\Delta \\boldsymbol{\\beta}^\\top] \\\\[4mm]\n",
    "= (J^TJ)^{-1}J^Trr^TJ[(J^TJ)^{-1}]^T\n",
    "$$\n",
    "\n",
    "由于hessian矩阵是对称矩阵，则它的逆也是对称矩阵，因此：\n",
    "$$[(J^TJ)^{-1}]^T = (J^TJ)^{-1}$$\n",
    "\n",
    "又由于$rr^T = \\sigma^2I$, \n",
    "\n",
    "这个公式的存在其实是因为那个高斯噪声$\\epsilon_i$的分布，那个的分布就是残差的期望分布，每个变量相互独立，协方差为0，只有对角线上元素为$\\sigma^2$\n",
    "\n",
    "将上面的两个公式带入，化简可得：\n",
    "\n",
    "$$\n",
    "\\text{Cov}(\\boldsymbol{\\Delta \\beta}) =  \\sigma^2(J^TJ)^{-1}J^TJ(J^TJ)^{-1}\n",
    "$$\n",
    "\n",
    "由于$(J^TJ)^{-1}J^TJ$等于单位矩阵，则：\n",
    "$$\n",
    "\\text{Cov}(\\boldsymbol{\\Delta \\beta}) =  \\sigma^2(J^TJ)^{-1}\n",
    "$$\n",
    "\n",
    "在统计学意义上，$\\beta$参数不确定性来源于 $\\Delta \\beta$\n",
    "则：\n",
    "$$\\text{Cov}(\\boldsymbol{\\beta}) = \\text{Cov}(\\boldsymbol{\\Delta \\beta}) =  \\sigma^2(J^TJ)^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：\n",
    "\n",
    "【1】https://www.bilibili.com/video/BV1xh4y1Z7ku/?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=099edd8ba094e7ddc14190b99d15a5fe\n",
    "\n",
    "【2】https://www.bilibili.com/video/BV1134y1k7gv/?spm_id_from=333.788.top_right_bar_window_custom_collection.content.click&vd_source=099edd8ba094e7ddc14190b99d15a5fe\n",
    "\n",
    "【3】https://blog.csdn.net/wangqy3811457/article/details/123186754\n",
    "\n",
    "【4】https://en.wikipedia.org/wiki/Non-linear_least_squares\n",
    "\n",
    "【5】https://en.wikipedia.org/wiki/Linear_least_squares\n",
    "\n",
    "有博主推荐的一篇论文，我还没看：\n",
    "\n",
    "推荐一篇超级好的论文：http://www2.imm.dtu.dk/pubdb/edoc/imm3215.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
