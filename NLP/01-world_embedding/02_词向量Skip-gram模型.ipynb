{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量Skip-gram模型\n",
    "\n",
    "这节训练词向量，实现Skip-gram模型。\n",
    "\n",
    "有以下几个知识点：\n",
    "- 用Skip-thought模型训练词向量\n",
    "- 学习使用PyTorch dataset和dataloader\n",
    "- 学习定义PyTorch模型\n",
    "- 学习torch.nn中常见的Module\n",
    "    - Embedding\n",
    "- 学习常见的PyTorch operations\n",
    "    - bmm\n",
    "    - logsigmoid\n",
    "- 保存和读取PyTorch模型\n",
    "\n",
    "下面复现论文[Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)中训练词向量的方法，实现Skip-gram模型，使用noice contrastive sampling的目标函数，没有用论文中subsampling方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch.nn.parameter import Parameter #参数更新和优化参数\n",
    "\n",
    "from collections import Counter #计数器，统计词频\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity #余弦相似度函数\n",
    "USE_MPS = torch.backends.mps.is_available() #判断是否支持MPS\n",
    "print(USE_MPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(53113)\n",
    "np.random.seed(53113)\n",
    "torch.manual_seed(53113)\n",
    "torch.cuda.manual_seed(53113)\n",
    "\n",
    "#设置超参数\n",
    "K = 100 #负样本随机采样数量\n",
    "C = 3 #指定周围三个单词进行预测\n",
    "NUM_EPOCHS = 5 #训练轮数，默认10轮\n",
    "MAX_VOCAB_SIZE = 30_000 #词汇表大小\n",
    "BATCH_SIZE = 32 #批处理大小\n",
    "LEARNING_RATE = 0.2 #学习率\n",
    "EMBEDDING_DIM = 100 #词向量维度\n",
    "\n",
    "LOG_FILE = \"word_embedding.log\"\n",
    "\n",
    "#tokenize函数，将文本转换为单词列表\n",
    "def tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从文本文件中读取所有的文字，通过这些文本创建一个vocabulary\n",
    "- 由于单词数量可能太大，我们只选取最常见的MAX_VOCAB_SIZE个单词\n",
    "- 我们添加一个UNK单词表示所有不常见的单词\n",
    "- 我们需要记录单词到index的mapping，以及index到单词的mapping，单词的count，单词的(normalized) frequency，以及单词总数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : PREFACE\n",
      "\n",
      "\n",
      "SUPPOSING that Truth is a woman--what then? Is there not ground\n",
      "for suspecting that all philosophers, in so far as they have been\n",
      "dogmatists, have failed to understand women--that the terrib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17683"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/nietzsche.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(f\"text : {text[:200]}\")\n",
    "\n",
    "#将文本转换为单词列表\n",
    "text = tokenize(text.lower())\n",
    "\n",
    "#统计词频，字典格式， 把(MAX_VOCAB_SIZE-1)个最常见的单词取出来，-1为unk表示不常见的单词（unknow）\n",
    "vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1))\n",
    "\n",
    "#unk表示不常见的单词 = 总词数 - 常见词数\n",
    "vocab[\"<unk>\"] = len(text) - np.sum(list(vocab.values()))\n",
    "\n",
    "#取出字典的所有单词key\n",
    "idx_to_word = [word for word in vocab.keys()]\n",
    "\n",
    "#取出所有单词和对应的索引， 最常见的单词索引为0\n",
    "word_to_idx = {word: i for i, word in enumerate(idx_to_word)}\n",
    "\n",
    "#所有单词的频数values\n",
    "word_counts = np.array(list(vocab.values()), dtype=np.float32)\n",
    "\n",
    "#所有单词的频率\n",
    "word_freqs = word_counts / np.sum(word_counts)\n",
    "\n",
    "#论文里乘以3/4次方\n",
    "word_freqs = word_freqs ** (3.0/4.0)\n",
    "\n",
    "#归一化，重新计算频率\n",
    "word_freqs = word_freqs / np.sum(word_freqs)\n",
    "\n",
    "VOCAB_SIZE = len(idx_to_word) #词汇表大小 30000=MAX_VOCAB_SIZE\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现Dataloader\n",
    "\n",
    "一个dataloader需要以下内容：\n",
    "\n",
    "- 把所有text编码成数字，然后用subsampling预处理这些文字。\n",
    "- 保存vocabulary，单词count，normalized word frequency\n",
    "- 每个iteration sample一个中心词\n",
    "- 根据当前的中心词返回context单词\n",
    "- 根据中心词sample一些negative单词\n",
    "- 返回单词的counts\n",
    "\n",
    "这里有一个好的tutorial介绍如何使用[PyTorch dataloader](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "为了使用dataloader，我们需要定义以下两个function:\n",
    "\n",
    "- ```__len__``` function需要返回整个数据集中有多少个item\n",
    "- ```__getitem__``` 根据给定的index返回一个item\n",
    "\n",
    "有了dataloader之后，我们可以轻松随机打乱整个数据集，拿到一个batch的数据等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(Dataset):\n",
    "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs, word_counts) -> None:\n",
    "        \"\"\"\n",
    "        text: a list of words, all text from the data\n",
    "        word_to_idx: the dictionary from word to idx\n",
    "        idx_to_word: idx to word mapping\n",
    "        word_freqs: the frequency of each word\n",
    "        word_counts: the word counts\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #字典get方法，函数返回指定键的值，如果值不在字典中返回默认值（第二个参数）。\n",
    "        #取出text中每个单词对应的索引，不在字典里的单词返回unk的索引\n",
    "        self.text_encoded = [word_to_idx.get(word, word_to_idx[\"<unk>\"]) for word in text]\n",
    "\n",
    "        self.text_encoded = torch.tensor(self.text_encoded).long() #转换为张量\n",
    "\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = idx_to_word\n",
    "        self.word_freqs = torch.tensor(word_freqs)\n",
    "        self.word_counts = torch.tensor(word_counts)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        返回三个部分的数据：\n",
    "        - 中心词\n",
    "        - 这个单词附近的positive单词\n",
    "        - 随机采样的K单词作为negative sample\n",
    "        \"\"\"\n",
    "        center_word = self.text_encoded[idx]\n",
    "\n",
    "        #周围词索引，比如C=3，取中心词前后三个单词， idx=0时，pos_indices=[-3, -2, -1, 1, 2, 3]\n",
    "        pos_indices = list(range(idx - C, idx)) + list(range(idx + 1, idx + C + 1))\n",
    "\n",
    "        #对于不在文本中的索引，进行处理\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices]\n",
    "\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "\n",
    "        #负样本采样，根据频率采样K个单词\n",
    "        #torch.multinomial 用于从多项分布中抽取样本。多项分布是一种描述多个可能结果的概率分布，例如抛硬币、掷骰子等。\n",
    "        #参数1：权重，参数2：采样次数，参数3：是否有放回采样\n",
    "        #输出的采样结果就是word_freqs中的索引\n",
    "        #每个正样本对应K个负样本\n",
    "        neg_words = torch.multinomial(self.word_freqs, num_samples= K * pos_words.shape[0], replacement=True)\n",
    "\n",
    "        return center_word, pos_words, neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(5907),\n",
       " tensor([17681,     1,  5902,   705,     7,   178]),\n",
       " tensor([    1,   750,   276,  2561,  4582,    26, 11822, 11679,  7987,    25,\n",
       "           824,    41,  7631,  4893,   255,  2434,  6848,    74,  1037, 16634,\n",
       "           207, 11917,  9472,  6514,  5626, 10230,     5,   570,  1652,   791,\n",
       "           100,     1,    58,    32,    46,   662,  1640,  5649,  3275,  4782,\n",
       "         15518,   707, 13781,    94,    21,  4434,   104,  2089,     1,   142,\n",
       "          1030,  1971, 17135,  9696,  2042,   612,  7202,     8,    93,     8,\n",
       "         14046, 14720, 13009,   607,  9530,  1390,  7264,  7326,  7230, 13614,\n",
       "          3710,  5503,   522, 11194,  1596,  1143,   107,    87,  9148,    21,\n",
       "         14841,  1905,    39,  5270,     5,    78,  1720,    52,  1324,     8,\n",
       "          1764,     3,   423,  2516,    62,    33,  2609,   173,    80,   552,\n",
       "          6815,  8455,  1633,    47,    59,   481,   624,   392,  4906, 10137,\n",
       "             2,   312,  1162,  1253, 17486,   738,    33,    12,    21, 13424,\n",
       "            84, 10782,     8,   902,  9482,   284,   150, 14490, 15078,   413,\n",
       "             1,  4206,  5689,  2222, 16661,     0,    15,  2421,   287,  1885,\n",
       "          1596,  5351,  2550,   115,    13,  3206, 16721,     0,  5558,  5500,\n",
       "          3736,  4075,    16,  2283, 15756,    89,  1130, 14569,  7780, 15900,\n",
       "          5039, 13475,   433,   129,  1989, 14847,    81,   165,  3394,   216,\n",
       "           306,     2,    50,  1216,  5129,    58,     5,  4194,   171,   768,\n",
       "         17296,    43,    91,   467,  4424,  6203, 13091,     0,   368, 17487,\n",
       "             0,  4507,  2912,   835,   225,     0,   291,  2120,     8,     5,\n",
       "             1,  1869,  3652,     5,     1, 10474,  7586, 10440,   381,   755,\n",
       "         14100, 10890,  4638,  3547,    19,     0,  8569,   416,  2379,  3711,\n",
       "           339, 14888,  5336,   122, 10668, 14578,    18,  5061,  3539,  9386,\n",
       "          4402,  3977,   427,    33,   198,     5,  1788,  6109,  5660,  1339,\n",
       "             3,   181, 12618, 14236,     2,   439,    68,  2541,  1920,  1260,\n",
       "            43,  2059,    12,    98,     5,  9134,  3493,     7,  1158, 15517,\n",
       "           134,  9762,    19, 15208,     8,  6597,  2561,     0,  2806,  2055,\n",
       "         10608,    24,  4037,  1095,  1124,  1176,     1,   125,   230,  7736,\n",
       "           881, 16324, 17511, 12765,     1,  7054,  5661,   175,    19,     3,\n",
       "          1046,  8059, 11648,    21, 16731,   964,   236,  1610,   219,  4602,\n",
       "          5578,    20,     1,    31,  1342,    33,  9519,   334,  3029,  5104,\n",
       "           500,   542,   332, 14185,  6065,   334,   752,  5982,  8715,     4,\n",
       "          2730,    45,  2927,   654,  7174,   638,   861,    39,     4,  2699,\n",
       "           285,  3010,   299, 13369,  6627,  3223, 12904,  4361,  9198,  4021,\n",
       "          3215, 14384,  1965,     9,   261,  3856,    34,   377,   346,   137,\n",
       "           112, 10152,    30,  1647,  6099,  1596, 14911,  2338,  3487,  9053,\n",
       "           148, 13978,    21,   686,  1331,  7515,  1030,     2,   533,  4389,\n",
       "          1298,  4581,    59,    80,  2719,    78,   898,    31, 13133,  2210,\n",
       "         10460,  8030,   259, 15129,  2073,  3288, 12024, 13221,   179,   823,\n",
       "         14738,  3565,  9166,     2,    41,   675, 15357,  1906,     5,   642,\n",
       "            34,    48,  7048,  7244,  3747,  1393,  1068,     0,   462,  4193,\n",
       "          7833,   428,  8300,  3692,     8,   111,   999,    25,  3885,  2743,\n",
       "          5969,  6004,    47,  4003,  2786,     1,   308, 17097, 17035,  1634,\n",
       "             5,  7329,   160, 10796,     6,    66,  6172,  4490,    33,  2331,\n",
       "           385,    17,  2097,    24, 12778,    15,   132,  8454,     2,    39,\n",
       "          1954,    61,  3121,   119,  1975,   877,  1971,   106,   582,  2368,\n",
       "            96,   290,  7280,  9064,   118, 13673,   341,  1072,     5,  9013,\n",
       "           170,   355,     0,   129,  7076,   306,  3027,  5634,    37,  1670,\n",
       "          7819,   103,   180, 11752,   354,   557,  4375,   870,  4758,  4062,\n",
       "           189, 17482,  1067,  7009,   767,   498,   171, 12086,  4960,  6740,\n",
       "          1472,    13,     9,    70,   754, 10394,  1059,   559, 10987,  6036,\n",
       "          4018,  1615,    95,  5275,   681,    42, 11092,  2823,    60,   863,\n",
       "            23,  2915,  3952,  1642,  4394,   208,  5776,  4750,    33,  2879,\n",
       "         14197,  5320, 15207,  3830, 14248,  2095,     5,  6408,    62,  6428,\n",
       "           250,  6155,  1976, 17631,   431,     9,   704,     0,     4,   891,\n",
       "            86,  2042,    20,  1086,    66,   456,   261,  7632,   399,  1147,\n",
       "          3949,  1581,    13,  1382,  9331,  2135, 13694, 11435,  2319, 17063,\n",
       "           723, 12968,  9657,   208,   614,    22,     8,   965,  1203,  4128,\n",
       "           562,  9339,  4497,   345, 14461,   133,   805,  8448,    53,  2861,\n",
       "         15628,  2777, 15295, 17623,  5646,  2733,    82,   105,    33,   479])]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = WordEmbeddingDataset(text, word_to_idx, idx_to_word, word_freqs, word_counts)\n",
    "\n",
    "list(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x12de3a010>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,   438,    37,  1208,   152,     3,   638,     7,    23,    40,\n",
      "          471,  1162,  4644,     2,   243,     0,  1809,     1,   421,   263,\n",
      "            1,  9750,    28, 11882,    25,   441,    45,  6198,  5798,   277,\n",
      "          184,     5]) tensor([[ 5682,    13,    33,   304,   135,    27],\n",
      "        [  224,     4,   353,  3129,  1609,     9],\n",
      "        [  133,    51,   512,     4,    95,    46],\n",
      "        [ 1078,  8250,    27,  4387,     0,   155],\n",
      "        [   62,    17,   406,  7968,  7969,   629],\n",
      "        [   16,   415,  2649,    76,  3310,     6],\n",
      "        [   34,  4251,    59,  8582,  8583,     8],\n",
      "        [ 4719,     5,   152,   179,    41,   933],\n",
      "        [    7,     0,  3191,  3904,    10,    17],\n",
      "        [    6,   207,  1476,     6,     3,    12],\n",
      "        [   44,    91,     1,     2,  1539,     7],\n",
      "        [    1,    44,   361,    32, 11925,  5073],\n",
      "        [    1,    22,   418,    10,    79,   750],\n",
      "        [ 2443,     0,   596,    46,   187,   148],\n",
      "        [    0,   389,    11,    84,    27,     3],\n",
      "        [17311,    60,    97, 17312,   232,    83],\n",
      "        [ 1564,     0,  4840,     1,    17,   384],\n",
      "        [    0,   129,  1657,     0, 17426,    38],\n",
      "        [   20,   143,     5, 16276,     3,  1033],\n",
      "        [    5,    39,   348,  2498,    10,   250],\n",
      "        [10600,     4,   860,     0, 10601,     1],\n",
      "        [ 1038,     1,    17,     2,    32,  3210],\n",
      "        [  150,    54,  1855,    37,    13,   815],\n",
      "        [    0,  1999,    20, 11883,    35,    18],\n",
      "        [   28,   132,  4628, 11482,     4,   252],\n",
      "        [    1,     0,   712,     1,     5,   247],\n",
      "        [   90,  9485,     6,    74,  9486,     4],\n",
      "        [ 6196,  6197,     2,  1426,   544,   128],\n",
      "        [ 1105,     3,    16,     0,   421,   663],\n",
      "        [  268,     5,   963,     1,     0,  3078],\n",
      "        [16760, 16761,    14,    10,     0,  1294],\n",
      "        [  722,    30,  3564,  2017,  5830,    83]]) tensor([[  500,  2657,     9,  ...,  5847,    69,   855],\n",
      "        [ 1427,  3922,   135,  ...,     3,  1361, 15325],\n",
      "        [   65, 15490,     0,  ...,  5871,  2163,     0],\n",
      "        ...,\n",
      "        [ 9770,     3,     3,  ...,    10,    27,    18],\n",
      "        [    0,  4329,    72,  ...,   268,  2232,   178],\n",
      "        [ 4980,   682,   243,  ..., 15869, 16781,   124]])\n"
     ]
    }
   ],
   "source": [
    "for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "    print(input_labels, pos_labels, neg_labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义PyTorch模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size #词汇表大小, 30000\n",
    "        self.embed_size = embed_size #词向量维度, 100\n",
    "\n",
    "        initrange = 0.5 / self.embed_size #初始化范围\n",
    "\n",
    "        #模型输入nn.Embedding(30000, 100)\n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "        #初始化权重\n",
    "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "        #模型输出nn.Embedding(30000, 100)\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False) #输出层, sparse=False表示不使用稀疏张量\n",
    "        #初始化权重\n",
    "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        \"\"\"\n",
    "        input_labels: 中心词，[batch_size]\n",
    "        pos_labels: 中心词周围的正样本，[batch_size, (window_size * 2)]\n",
    "        neg_labels: 中心词周围的负样本，[batch_size, (window_size * 2 * K)]\n",
    "\n",
    "        return: loss, [batch_size]\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = input_labels.size(0)\n",
    "\n",
    "        input_embedding = self.in_embed(input_labels) #中心词的词向量，[batch_size, embed_size]\n",
    "        pos_embedding = self.out_embed(pos_labels) #正样本的词向量，[batch_size, (window_size * 2), embed_size]\n",
    "        neg_embedding = self.out_embed(neg_labels) #负样本的词向量，[batch_size, (window_size * 2 * K), embed_size]\n",
    "\n",
    "        #torch.bmm()为batch间的矩阵乘法【batch_size, n, m】 * 【batch_size, m, p】 = 【batch_size, n, p】\n",
    "        log_pos = torch.bmm(\n",
    "            pos_embedding, input_embedding.unsqueeze(2)\n",
    "        ).squeeze() #正样本的相似度，[batch_size, (window_size * 2)]\n",
    "\n",
    "        log_neg = torch.bmm(\n",
    "            neg_embedding, -input_embedding.unsqueeze(2)\n",
    "        ).squeeze() #负样本的相似度，[batch_size, (window_size * 2 * K)]\n",
    "\n",
    "        #下面loss计算就是论文里的公式\n",
    "        log_pos = F.logsigmoid(log_pos).sum(1) #正样本的相似度，[batch_size]\n",
    "        log_neg = F.logsigmoid(log_neg).sum(1) #负样本的相似度，[batch_size]\n",
    "        loss = log_pos + log_neg\n",
    "\n",
    "        return -loss\n",
    "    \n",
    "    def input_embeddings(self): #取出self.in_embed的权重参数\n",
    "        return self.in_embed.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个模型以及把模型移动到GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EmbeddingModel(\n",
       "  (in_embed): Embedding(17683, 100)\n",
       "  (out_embed): Embedding(17683, 100)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "device = torch.device(\"mps\" if USE_MPS else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss: 420.0471496582031\n",
      "epoch: 0, iter: 100, loss: 420.0234375\n",
      "epoch: 0, iter: 200, loss: 418.30194091796875\n",
      "epoch: 0, iter: 300, loss: 407.61260986328125\n",
      "epoch: 0, iter: 400, loss: 394.5010986328125\n",
      "epoch: 0, iter: 500, loss: 374.0609130859375\n",
      "epoch: 0, iter: 600, loss: 346.4581298828125\n",
      "epoch: 0, iter: 700, loss: 328.9640808105469\n",
      "epoch: 0, iter: 800, loss: 303.1462097167969\n",
      "epoch: 0, iter: 900, loss: 289.53759765625\n",
      "epoch: 0, iter: 1000, loss: 274.593505859375\n",
      "epoch: 0, iter: 1100, loss: 231.27932739257812\n",
      "epoch: 0, iter: 1200, loss: 231.7281494140625\n",
      "epoch: 0, iter: 1300, loss: 248.93341064453125\n",
      "epoch: 0, iter: 1400, loss: 215.7095947265625\n",
      "epoch: 0, iter: 1500, loss: 223.58456420898438\n",
      "epoch: 0, iter: 1600, loss: 251.60345458984375\n",
      "epoch: 0, iter: 1700, loss: 224.01065063476562\n",
      "epoch: 0, iter: 1800, loss: 229.17547607421875\n",
      "epoch: 0, iter: 1900, loss: 186.27651977539062\n",
      "epoch: 0, iter: 2000, loss: 181.14459228515625\n",
      "epoch: 0, iter: 2100, loss: 226.13726806640625\n",
      "epoch: 0, iter: 2200, loss: 216.6129150390625\n",
      "epoch: 0, iter: 2300, loss: 200.3657989501953\n",
      "epoch: 0, iter: 2400, loss: 209.68984985351562\n",
      "epoch: 0, iter: 2500, loss: 206.17886352539062\n",
      "epoch: 0, iter: 2600, loss: 196.86309814453125\n",
      "epoch: 0, iter: 2700, loss: 161.92904663085938\n",
      "epoch: 0, iter: 2800, loss: 166.1490478515625\n",
      "epoch: 0, iter: 2900, loss: 138.61090087890625\n",
      "epoch: 0, iter: 3000, loss: 146.23135375976562\n",
      "epoch: 1, iter: 0, loss: 187.0752410888672\n",
      "epoch: 1, iter: 100, loss: 184.0169677734375\n",
      "epoch: 1, iter: 200, loss: 158.63003540039062\n",
      "epoch: 1, iter: 300, loss: 139.31253051757812\n",
      "epoch: 1, iter: 400, loss: 142.9498748779297\n",
      "epoch: 1, iter: 500, loss: 117.89866638183594\n",
      "epoch: 1, iter: 600, loss: 177.89944458007812\n",
      "epoch: 1, iter: 700, loss: 192.3863067626953\n",
      "epoch: 1, iter: 800, loss: 145.07363891601562\n",
      "epoch: 1, iter: 900, loss: 160.9195556640625\n",
      "epoch: 1, iter: 1000, loss: 185.9518585205078\n",
      "epoch: 1, iter: 1100, loss: 207.79196166992188\n",
      "epoch: 1, iter: 1200, loss: 147.83580017089844\n",
      "epoch: 1, iter: 1300, loss: 144.10638427734375\n",
      "epoch: 1, iter: 1400, loss: 130.8276824951172\n",
      "epoch: 1, iter: 1500, loss: 158.5374298095703\n",
      "epoch: 1, iter: 1600, loss: 138.86106872558594\n",
      "epoch: 1, iter: 1700, loss: 161.096435546875\n",
      "epoch: 1, iter: 1800, loss: 150.11965942382812\n",
      "epoch: 1, iter: 1900, loss: 167.25845336914062\n",
      "epoch: 1, iter: 2000, loss: 89.55720520019531\n",
      "epoch: 1, iter: 2100, loss: 112.64010620117188\n",
      "epoch: 1, iter: 2200, loss: 132.55667114257812\n",
      "epoch: 1, iter: 2300, loss: 154.93614196777344\n",
      "epoch: 1, iter: 2400, loss: 105.91964721679688\n",
      "epoch: 1, iter: 2500, loss: 117.18961334228516\n",
      "epoch: 1, iter: 2600, loss: 96.28223419189453\n",
      "epoch: 1, iter: 2700, loss: 162.12672424316406\n",
      "epoch: 1, iter: 2800, loss: 122.0201187133789\n",
      "epoch: 1, iter: 2900, loss: 99.76404571533203\n",
      "epoch: 1, iter: 3000, loss: 175.53704833984375\n",
      "epoch: 2, iter: 0, loss: 89.88870239257812\n",
      "epoch: 2, iter: 100, loss: 110.08212280273438\n",
      "epoch: 2, iter: 200, loss: 93.57530975341797\n",
      "epoch: 2, iter: 300, loss: 113.12884521484375\n",
      "epoch: 2, iter: 400, loss: 97.49568176269531\n",
      "epoch: 2, iter: 500, loss: 109.80403137207031\n",
      "epoch: 2, iter: 600, loss: 133.93528747558594\n",
      "epoch: 2, iter: 700, loss: 107.55178833007812\n",
      "epoch: 2, iter: 800, loss: 124.14435577392578\n",
      "epoch: 2, iter: 900, loss: 117.75508880615234\n",
      "epoch: 2, iter: 1000, loss: 105.80783081054688\n",
      "epoch: 2, iter: 1100, loss: 83.11984252929688\n",
      "epoch: 2, iter: 1200, loss: 96.0816650390625\n",
      "epoch: 2, iter: 1300, loss: 97.72998046875\n",
      "epoch: 2, iter: 1400, loss: 104.3873062133789\n",
      "epoch: 2, iter: 1500, loss: 168.2381591796875\n",
      "epoch: 2, iter: 1600, loss: 93.9853286743164\n",
      "epoch: 2, iter: 1700, loss: 96.66755676269531\n",
      "epoch: 2, iter: 1800, loss: 150.3348388671875\n",
      "epoch: 2, iter: 1900, loss: 81.44549560546875\n",
      "epoch: 2, iter: 2000, loss: 99.25091552734375\n",
      "epoch: 2, iter: 2100, loss: 145.4596710205078\n",
      "epoch: 2, iter: 2200, loss: 85.70270538330078\n",
      "epoch: 2, iter: 2300, loss: 115.55366516113281\n",
      "epoch: 2, iter: 2400, loss: 88.03506469726562\n",
      "epoch: 2, iter: 2500, loss: 68.24478149414062\n",
      "epoch: 2, iter: 2600, loss: 78.76028442382812\n",
      "epoch: 2, iter: 2700, loss: 102.81237030029297\n",
      "epoch: 2, iter: 2800, loss: 112.96200561523438\n",
      "epoch: 2, iter: 2900, loss: 104.53266143798828\n",
      "epoch: 2, iter: 3000, loss: 113.27169799804688\n",
      "epoch: 3, iter: 0, loss: 79.69527435302734\n",
      "epoch: 3, iter: 100, loss: 73.02318572998047\n",
      "epoch: 3, iter: 200, loss: 105.57353973388672\n",
      "epoch: 3, iter: 300, loss: 90.04251861572266\n",
      "epoch: 3, iter: 400, loss: 63.82659912109375\n",
      "epoch: 3, iter: 500, loss: 81.25020599365234\n",
      "epoch: 3, iter: 600, loss: 73.15397644042969\n",
      "epoch: 3, iter: 700, loss: 67.85693359375\n",
      "epoch: 3, iter: 800, loss: 61.28373336791992\n",
      "epoch: 3, iter: 900, loss: 86.51591491699219\n",
      "epoch: 3, iter: 1000, loss: 88.7081069946289\n",
      "epoch: 3, iter: 1100, loss: 89.95672607421875\n",
      "epoch: 3, iter: 1200, loss: 59.3470573425293\n",
      "epoch: 3, iter: 1300, loss: 92.983642578125\n",
      "epoch: 3, iter: 1400, loss: 119.44815063476562\n",
      "epoch: 3, iter: 1500, loss: 74.55072021484375\n",
      "epoch: 3, iter: 1600, loss: 99.1767349243164\n",
      "epoch: 3, iter: 1700, loss: 65.4992446899414\n",
      "epoch: 3, iter: 1800, loss: 92.40434265136719\n",
      "epoch: 3, iter: 1900, loss: 84.37477111816406\n",
      "epoch: 3, iter: 2000, loss: 79.86512756347656\n",
      "epoch: 3, iter: 2100, loss: 77.55592346191406\n",
      "epoch: 3, iter: 2200, loss: 54.03835678100586\n",
      "epoch: 3, iter: 2300, loss: 67.18266296386719\n",
      "epoch: 3, iter: 2400, loss: 68.07566833496094\n",
      "epoch: 3, iter: 2500, loss: 81.14492797851562\n",
      "epoch: 3, iter: 2600, loss: 76.92108154296875\n",
      "epoch: 3, iter: 2700, loss: 85.87832641601562\n",
      "epoch: 3, iter: 2800, loss: 70.81500244140625\n",
      "epoch: 3, iter: 2900, loss: 60.84741973876953\n",
      "epoch: 3, iter: 3000, loss: 96.2248764038086\n",
      "epoch: 4, iter: 0, loss: 69.13727569580078\n",
      "epoch: 4, iter: 100, loss: 77.83709716796875\n",
      "epoch: 4, iter: 200, loss: 64.78965759277344\n",
      "epoch: 4, iter: 300, loss: 74.25255584716797\n",
      "epoch: 4, iter: 400, loss: 63.148094177246094\n",
      "epoch: 4, iter: 500, loss: 69.1775131225586\n",
      "epoch: 4, iter: 600, loss: 56.79887008666992\n",
      "epoch: 4, iter: 700, loss: 82.0590591430664\n",
      "epoch: 4, iter: 800, loss: 80.70555114746094\n",
      "epoch: 4, iter: 900, loss: 55.878475189208984\n",
      "epoch: 4, iter: 1000, loss: 59.206424713134766\n",
      "epoch: 4, iter: 1100, loss: 63.92481994628906\n",
      "epoch: 4, iter: 1200, loss: 53.847877502441406\n",
      "epoch: 4, iter: 1300, loss: 62.38463592529297\n",
      "epoch: 4, iter: 1400, loss: 50.825469970703125\n",
      "epoch: 4, iter: 1500, loss: 48.800594329833984\n",
      "epoch: 4, iter: 1600, loss: 57.541961669921875\n",
      "epoch: 4, iter: 1700, loss: 60.29571533203125\n",
      "epoch: 4, iter: 1800, loss: 71.89193725585938\n",
      "epoch: 4, iter: 1900, loss: 58.48644256591797\n",
      "epoch: 4, iter: 2000, loss: 75.7051010131836\n",
      "epoch: 4, iter: 2100, loss: 74.37660217285156\n",
      "epoch: 4, iter: 2200, loss: 59.53166961669922\n",
      "epoch: 4, iter: 2300, loss: 70.23677062988281\n",
      "epoch: 4, iter: 2400, loss: 72.44316101074219\n",
      "epoch: 4, iter: 2500, loss: 59.88732147216797\n",
      "epoch: 4, iter: 2600, loss: 55.41706466674805\n",
      "epoch: 4, iter: 2700, loss: 61.426734924316406\n",
      "epoch: 4, iter: 2800, loss: 94.69232177734375\n",
      "epoch: 4, iter: 2900, loss: 49.277706146240234\n",
      "epoch: 4, iter: 3000, loss: 90.16461944580078\n",
      "epoch: 5, iter: 0, loss: 65.89755249023438\n",
      "epoch: 5, iter: 100, loss: 58.53551483154297\n",
      "epoch: 5, iter: 200, loss: 53.627601623535156\n",
      "epoch: 5, iter: 300, loss: 62.1762580871582\n",
      "epoch: 5, iter: 400, loss: 62.93130111694336\n",
      "epoch: 5, iter: 500, loss: 47.967838287353516\n",
      "epoch: 5, iter: 600, loss: 55.71522903442383\n",
      "epoch: 5, iter: 700, loss: 60.15607452392578\n",
      "epoch: 5, iter: 800, loss: 54.880584716796875\n",
      "epoch: 5, iter: 900, loss: 60.94223403930664\n",
      "epoch: 5, iter: 1000, loss: 54.8632926940918\n",
      "epoch: 5, iter: 1100, loss: 53.3792724609375\n",
      "epoch: 5, iter: 1200, loss: 57.387229919433594\n",
      "epoch: 5, iter: 1300, loss: 58.85321044921875\n",
      "epoch: 5, iter: 1400, loss: 65.47052001953125\n",
      "epoch: 5, iter: 1500, loss: 65.5833740234375\n",
      "epoch: 5, iter: 1600, loss: 45.20808410644531\n",
      "epoch: 5, iter: 1700, loss: 64.74453735351562\n",
      "epoch: 5, iter: 1800, loss: 55.30313491821289\n",
      "epoch: 5, iter: 1900, loss: 58.15568542480469\n",
      "epoch: 5, iter: 2000, loss: 60.914302825927734\n",
      "epoch: 5, iter: 2100, loss: 56.8473014831543\n",
      "epoch: 5, iter: 2200, loss: 62.469757080078125\n",
      "epoch: 5, iter: 2300, loss: 70.14598083496094\n",
      "epoch: 5, iter: 2400, loss: 50.463584899902344\n",
      "epoch: 5, iter: 2500, loss: 50.06127166748047\n",
      "epoch: 5, iter: 2600, loss: 55.40074920654297\n",
      "epoch: 5, iter: 2700, loss: 62.193756103515625\n",
      "epoch: 5, iter: 2800, loss: 54.02826690673828\n",
      "epoch: 5, iter: 2900, loss: 61.5507698059082\n",
      "epoch: 5, iter: 3000, loss: 77.57686614990234\n",
      "epoch: 6, iter: 0, loss: 42.99665069580078\n",
      "epoch: 6, iter: 100, loss: 50.36634063720703\n",
      "epoch: 6, iter: 200, loss: 63.333595275878906\n",
      "epoch: 6, iter: 300, loss: 48.6094970703125\n",
      "epoch: 6, iter: 400, loss: 50.56120300292969\n",
      "epoch: 6, iter: 500, loss: 55.7255973815918\n",
      "epoch: 6, iter: 600, loss: 46.03121566772461\n",
      "epoch: 6, iter: 700, loss: 49.35342788696289\n",
      "epoch: 6, iter: 800, loss: 58.44690704345703\n",
      "epoch: 6, iter: 900, loss: 54.089271545410156\n",
      "epoch: 6, iter: 1000, loss: 53.75886535644531\n",
      "epoch: 6, iter: 1100, loss: 44.15789031982422\n",
      "epoch: 6, iter: 1200, loss: 54.390071868896484\n",
      "epoch: 6, iter: 1300, loss: 60.489356994628906\n",
      "epoch: 6, iter: 1400, loss: 44.47316360473633\n",
      "epoch: 6, iter: 1500, loss: 63.565975189208984\n",
      "epoch: 6, iter: 1600, loss: 63.4573974609375\n",
      "epoch: 6, iter: 1700, loss: 46.32621765136719\n",
      "epoch: 6, iter: 1800, loss: 52.43311309814453\n",
      "epoch: 6, iter: 1900, loss: 46.78199005126953\n",
      "epoch: 6, iter: 2000, loss: 61.267173767089844\n",
      "epoch: 6, iter: 2100, loss: 39.45374298095703\n",
      "epoch: 6, iter: 2200, loss: 50.84312438964844\n",
      "epoch: 6, iter: 2300, loss: 40.87816619873047\n",
      "epoch: 6, iter: 2400, loss: 41.93110275268555\n",
      "epoch: 6, iter: 2500, loss: 43.292083740234375\n",
      "epoch: 6, iter: 2600, loss: 62.88045883178711\n",
      "epoch: 6, iter: 2700, loss: 55.10488510131836\n",
      "epoch: 6, iter: 2800, loss: 52.50654602050781\n",
      "epoch: 6, iter: 2900, loss: 64.16690826416016\n",
      "epoch: 6, iter: 3000, loss: 61.196006774902344\n",
      "epoch: 7, iter: 0, loss: 40.786521911621094\n",
      "epoch: 7, iter: 100, loss: 38.679969787597656\n",
      "epoch: 7, iter: 200, loss: 44.85698318481445\n",
      "epoch: 7, iter: 300, loss: 55.604942321777344\n",
      "epoch: 7, iter: 400, loss: 51.47999572753906\n",
      "epoch: 7, iter: 500, loss: 53.18812561035156\n",
      "epoch: 7, iter: 600, loss: 59.28692626953125\n",
      "epoch: 7, iter: 700, loss: 57.131683349609375\n",
      "epoch: 7, iter: 800, loss: 56.65379333496094\n",
      "epoch: 7, iter: 900, loss: 51.55809783935547\n",
      "epoch: 7, iter: 1000, loss: 49.586280822753906\n",
      "epoch: 7, iter: 1100, loss: 48.74125671386719\n",
      "epoch: 7, iter: 1200, loss: 44.256248474121094\n",
      "epoch: 7, iter: 1300, loss: 45.96855163574219\n",
      "epoch: 7, iter: 1400, loss: 63.12324142456055\n",
      "epoch: 7, iter: 1500, loss: 42.89213562011719\n",
      "epoch: 7, iter: 1600, loss: 50.134674072265625\n",
      "epoch: 7, iter: 1700, loss: 44.556915283203125\n",
      "epoch: 7, iter: 1800, loss: 44.35560607910156\n",
      "epoch: 7, iter: 1900, loss: 54.12218475341797\n",
      "epoch: 7, iter: 2000, loss: 46.757450103759766\n",
      "epoch: 7, iter: 2100, loss: 43.9707145690918\n",
      "epoch: 7, iter: 2200, loss: 45.95193099975586\n",
      "epoch: 7, iter: 2300, loss: 47.849647521972656\n",
      "epoch: 7, iter: 2400, loss: 51.68872833251953\n",
      "epoch: 7, iter: 2500, loss: 48.96925354003906\n",
      "epoch: 7, iter: 2600, loss: 54.293113708496094\n",
      "epoch: 7, iter: 2700, loss: 49.064788818359375\n",
      "epoch: 7, iter: 2800, loss: 47.456756591796875\n",
      "epoch: 7, iter: 2900, loss: 43.60325622558594\n",
      "epoch: 7, iter: 3000, loss: 45.61703872680664\n",
      "epoch: 8, iter: 0, loss: 44.21935272216797\n",
      "epoch: 8, iter: 100, loss: 47.6325798034668\n",
      "epoch: 8, iter: 200, loss: 45.23426055908203\n",
      "epoch: 8, iter: 300, loss: 45.1248779296875\n",
      "epoch: 8, iter: 400, loss: 45.96766662597656\n",
      "epoch: 8, iter: 500, loss: 49.62939453125\n",
      "epoch: 8, iter: 600, loss: 48.77604675292969\n",
      "epoch: 8, iter: 700, loss: 50.520896911621094\n",
      "epoch: 8, iter: 800, loss: 49.16623306274414\n",
      "epoch: 8, iter: 900, loss: 45.209861755371094\n",
      "epoch: 8, iter: 1000, loss: 45.25394058227539\n",
      "epoch: 8, iter: 1100, loss: 48.603851318359375\n",
      "epoch: 8, iter: 1200, loss: 49.03818130493164\n",
      "epoch: 8, iter: 1300, loss: 45.91222381591797\n",
      "epoch: 8, iter: 1400, loss: 39.136314392089844\n",
      "epoch: 8, iter: 1500, loss: 39.330116271972656\n",
      "epoch: 8, iter: 1600, loss: 37.45347595214844\n",
      "epoch: 8, iter: 1700, loss: 41.221702575683594\n",
      "epoch: 8, iter: 1800, loss: 51.125083923339844\n",
      "epoch: 8, iter: 1900, loss: 49.39088821411133\n",
      "epoch: 8, iter: 2000, loss: 42.35623550415039\n",
      "epoch: 8, iter: 2100, loss: 43.092498779296875\n",
      "epoch: 8, iter: 2200, loss: 45.70805358886719\n",
      "epoch: 8, iter: 2300, loss: 46.435157775878906\n",
      "epoch: 8, iter: 2400, loss: 49.79899597167969\n",
      "epoch: 8, iter: 2500, loss: 43.499420166015625\n",
      "epoch: 8, iter: 2600, loss: 48.16661834716797\n",
      "epoch: 8, iter: 2700, loss: 43.690277099609375\n",
      "epoch: 8, iter: 2800, loss: 44.20481491088867\n",
      "epoch: 8, iter: 2900, loss: 47.83104705810547\n",
      "epoch: 8, iter: 3000, loss: 44.52569580078125\n",
      "epoch: 9, iter: 0, loss: 43.52394104003906\n",
      "epoch: 9, iter: 100, loss: 40.75816345214844\n",
      "epoch: 9, iter: 200, loss: 39.2113037109375\n",
      "epoch: 9, iter: 300, loss: 44.10498809814453\n",
      "epoch: 9, iter: 400, loss: 40.35525894165039\n",
      "epoch: 9, iter: 500, loss: 41.52171325683594\n",
      "epoch: 9, iter: 600, loss: 42.200523376464844\n",
      "epoch: 9, iter: 700, loss: 44.28025817871094\n",
      "epoch: 9, iter: 800, loss: 42.500587463378906\n",
      "epoch: 9, iter: 900, loss: 40.38679122924805\n",
      "epoch: 9, iter: 1000, loss: 44.393428802490234\n",
      "epoch: 9, iter: 1100, loss: 40.422157287597656\n",
      "epoch: 9, iter: 1200, loss: 46.214595794677734\n",
      "epoch: 9, iter: 1300, loss: 43.88323211669922\n",
      "epoch: 9, iter: 1400, loss: 42.45865249633789\n",
      "epoch: 9, iter: 1500, loss: 41.63866424560547\n",
      "epoch: 9, iter: 1600, loss: 49.21788024902344\n",
      "epoch: 9, iter: 1700, loss: 45.186771392822266\n",
      "epoch: 9, iter: 1800, loss: 40.399131774902344\n",
      "epoch: 9, iter: 1900, loss: 42.978599548339844\n",
      "epoch: 9, iter: 2000, loss: 43.57008361816406\n",
      "epoch: 9, iter: 2100, loss: 41.758872985839844\n",
      "epoch: 9, iter: 2200, loss: 45.587623596191406\n",
      "epoch: 9, iter: 2300, loss: 41.15325164794922\n",
      "epoch: 9, iter: 2400, loss: 38.97963333129883\n",
      "epoch: 9, iter: 2500, loss: 45.31475067138672\n",
      "epoch: 9, iter: 2600, loss: 40.31114959716797\n",
      "epoch: 9, iter: 2700, loss: 38.80238342285156\n",
      "epoch: 9, iter: 2800, loss: 42.04762268066406\n",
      "epoch: 9, iter: 2900, loss: 40.876800537109375\n",
      "epoch: 9, iter: 3000, loss: 44.19206237792969\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        #转为longtensor\n",
    "        input_labels = input_labels.long()\n",
    "        pos_labels = pos_labels.long()\n",
    "        neg_labels = neg_labels.long()\n",
    "\n",
    "        input_labels = input_labels.to(device)\n",
    "        pos_labels = pos_labels.to(device)\n",
    "        neg_labels = neg_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            with open(LOG_FILE, \"a\") as file:\n",
    "                file.write(f\"epoch: {epoch}, iter: {i}, loss: {loss.item()}\\n\")\n",
    "            \n",
    "            print(f\"epoch: {epoch}, iter: {i}, loss: {loss.item()}\")\n",
    "\n",
    "#保存模型\n",
    "embedding_weights = model.input_embeddings()\n",
    "np.save(f\"embedding-{EMBEDDING_DIM}\", embedding_weights)\n",
    "torch.save(model.state_dict(), f\"embedding-{EMBEDDING_DIM}.th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingModel(\n",
       "  (in_embed): Embedding(17683, 100)\n",
       "  (out_embed): Embedding(17683, 100)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(f\"embedding-{EMBEDDING_DIM}.th\")\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是评估模型的代码，以及训练模型的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def evaluate(filename, embedding_weights):\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"{filename} not found\")\n",
    "        return\n",
    "    if filename.endswith(\".csv\"):\n",
    "        data = pd.read_csv(filename, sep=\",\")\n",
    "    else:\n",
    "        data = pd.read_csv(filename, sep=\"\\t\")\n",
    "\n",
    "    print(data.head())\n",
    "    human_similarity = []\n",
    "    model_similarity = []\n",
    "    for i in range(len(data)):\n",
    "        word1, word2 = data.iloc[i, 0], data.iloc[i, 1]\n",
    "        if word1 not in word_to_idx or word2 not in word_to_idx:\n",
    "            continue\n",
    "        else:\n",
    "            word1_idx, word2_idx = word_to_idx[word1], word_to_idx[word2]\n",
    "            #embedding_weights是模型的权重参数,是一个矩阵\n",
    "            #当我们输入单词的索引时，在计算embedding时，会取出embedding_weights中对应索引的行\n",
    "            #这个行就是对应单词的词向量\n",
    "            word1_embed, word2_embed = embedding_weights[[word1_idx]], embedding_weights[[word2_idx]]\n",
    "            model_similarity.append(\n",
    "                float(sklearn.metrics.pairwise.cosine_similarity(word1_embed, word2_embed)[0,0]) #cosine_similarity返回的是矩阵,但这里由于左右都是一个样本，所以取[0,0]就是这对样本的相似度\n",
    "            )\n",
    "            human_similarity.append(float(data.iloc[i, 2]))\n",
    "\n",
    "    return scipy.stats.spearmanr(human_similarity, model_similarity) #计算相关系数, 返回相关系数和p值\n",
    "\n",
    "def find_nearest(word):\n",
    "    if word not in word_to_idx:\n",
    "        return\n",
    "    index = word_to_idx[word]\n",
    "    embedding = embedding_weights[index]\n",
    "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights]) #计算该单词与其他单词的余弦相似度\n",
    "    return [idx_to_word[i] for i in cos_dis.argsort()[:10]] #返回最相似的10个单词\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在 Simplex-999 数据集上做评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     old          new  1.58\n",
      "0  smart  intelligent  9.20\n",
      "1   hard    difficult  8.77\n",
      "2  happy     cheerful  9.55\n",
      "3   hard         easy  0.95\n",
      "4   fast        rapid  8.75\n",
      "simlex-999 SignificanceResult(statistic=np.float64(0.12613471770213477), pvalue=np.float64(0.017906650696305405))\n"
     ]
    }
   ],
   "source": [
    "embedding_weights = model.input_embeddings()\n",
    "print(\"simlex-999\", evaluate(\"data/en-simlex-999.txt\", embedding_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 寻找nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good ['good', 'things', 'other', 'made', 'just', 'well', 'being', 'very', 'life', 'because']\n",
      "green ['green', 'scholar,', 'treasure', 'impregnated', 'mysteries', 'tastes', 'permeated', 'indicate', 'surrenders', 'granting']\n",
      "like ['like', 'once', 'about', 'yet', 'long', 'longer', 'then', 'precisely', 'too', 'is,']\n",
      "america None\n",
      "chicago None\n",
      "work ['work', 'two', 'while', 'delight', 'therefore,', 'powerful', 'within', 'metaphysical', 'effect', 'former']\n",
      "building ['building', 'produced,', 'still;', 'leaves,', 'dupers', 'ten', 'falsification', 'forms,', 'disposing,', 'poetical']\n",
      "computer None\n",
      "language ['language', 'words', 'and,', 'profound', 'cases', 'common', 'lower', 'both', 'here,', 'music']\n"
     ]
    }
   ],
   "source": [
    "for word in [\"good\", \"green\", \"like\", \"america\", \"chicago\", \"work\", \"building\", \"computer\", \"language\"]:\n",
    "    print(word, find_nearest(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单词之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clair,\n",
      "but--dreadful\n",
      "102.\n",
      "e\n",
      "moments-la,\n",
      "suspects,\n",
      "realm--what\n",
      "wealthy\n",
      "hat\n",
      "now--better\n"
     ]
    }
   ],
   "source": [
    "man_idx = word_to_idx[\"man\"]\n",
    "king_idx = word_to_idx[\"king\"]\n",
    "woman_idx = word_to_idx[\"woman\"]\n",
    "embedding = embedding_weights[woman_idx] - embedding_weights[man_idx] + embedding_weights[king_idx]\n",
    "cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "for i in cos_dis.argsort()[:10]:\n",
    "    print(idx_to_word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(LOG_FILE)\n",
    "os.remove(f\"embedding-{EMBEDDING_DIM}.npy\")\n",
    "os.remove(f\"embedding-{EMBEDDING_DIM}.th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonTry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
