{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据李沐老师的d2l的网站，写的Transformer\n",
    "\n",
    "我还用这个Transformer去替换调包Transformer实现caption任务的那个ipynb。\n",
    "\n",
    "后来发现这个Transformer特别容易过拟合，推理效果很差，要么就是一个单词经常出现，要么就是只会输出1-2的单词。\n",
    "\n",
    "所以我不知道，这个模型代码写的对不对。但仍然可以作为自己实现Transformer的参考。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataloader, dataset\n",
    "\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # 位置编码\n",
    "    def __init__(self, num_hiddens, drouput, max_len= 100):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(drouput)\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) / torch.pow(\n",
    "            10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens\n",
    "        )\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        #X: [batch_size, num_steps, dim]\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X) # 使用dropout来避免模型对P（位置编码）太敏感"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_lens, value=0):\n",
    "    \"\"\"\n",
    "    在序列中屏蔽不相关的项\n",
    "    \"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_lens[:, None]\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def masked_softmax(X, valid_lens=None):\n",
    "    \"\"\"\n",
    "    在某些情况下，并非所有的值都应该被纳入到注意力池化中。例如文本序列中的<PAD>。\n",
    "    通过最后一个轴上掩蔽元素来执行softmax操作\n",
    "    \"\"\"\n",
    "    if valid_lens is None:\n",
    "        return F.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1]) # 如果valid_lens是标量，那么就认为所有的句子长度都是这个标量，故填充成为1维向量。\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)  # flatten成为1维度向量\n",
    "        # 最后一个轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax的结果为0\n",
    "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=1e-6)\n",
    "        return F.softmax(X.reshape(shape), dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        \"\"\"\n",
    "        queries: [batch, num_steps, hidden_dim]\n",
    "        keys: [batch, m_keys, hidden_dim]\n",
    "        values: [batch, m_keys, vocab_dim]\n",
    "        \"\"\"\n",
    "        attention_scores = torch.bmm(queries, keys.transpose(2, 1)) / math.sqrt(queries.shape[-1])\n",
    "        alpha = masked_softmax(attention_scores, valid_lens) # [batch, num_steps, m_keys]\n",
    "        return torch.bmm(self.dropout(alpha), values) # [batch, num_steps, vocab_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 了解一下这个sequence_mask函数\n",
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "sequence_mask(X, torch.tensor([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "    # 虽然多头注意力模型写的是每个之前都有一个全连接，但是我们可以只写一个全连接，之后分成不同的头\n",
    "    # X: [batch, num_keys, num_hiddens] => [batch*num_heads, num_keys， num_hiddens/num_heads]\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    # X: [batch*num_heads, 查询的个数， num_hiddens/num_heads] => [batch, 查询的个数， num_hiddens]\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, query_size, key_size, value_size, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        self.num_heads = num_heads\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias) # q, k, v首先需要先做投影，之后再做自注意力池化\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
    "        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "\n",
    "    \n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        \"\"\"\n",
    "        queries, keys, values的形状 #[batch, 查询或者“键-值”的个数， num_hiddens]\n",
    "        valid_lens 的形状： [batch] 或者 [batch, 查询的个数]\n",
    "        经过变换之后：\n",
    "        queries, keys, values的形状 #[batch*num_heads, 查询或者“键-值”的个数， num_hiddens/num_heads]\n",
    "        \"\"\"\n",
    "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0\n",
    "            )\n",
    "\n",
    "        # output: [batch*num_heads, 查询的个数， num_hiddens/num_heads]\n",
    "        output = self.attention(queries, keys, values, valid_lens) # head和batch合并之后，就可以直接用到一个attention里面了，就是用一个bmm就可以计算了。\n",
    "\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "\n",
    "        return self.W_o(output_concat)\n",
    "    \n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return None  #TODO： 这里添加一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (W_q): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_k): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_v): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (attention): DotProductAttention(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens, num_heads= 100, 5\n",
    "attetion = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens, num_hiddens, num_heads, 0.5)\n",
    "attetion.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 100])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, num_queries = 2, 4\n",
    "num_kvpairs, valid_lens = 6, torch.tensor([3, 2])\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "attetion(X, Y, Y, valid_lens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"基于位置的前馈网络\"\"\"\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X: [batch, num_steps, ffn_num_input] => [batch, num_steps, ffn_num_outputs]\n",
    "        return self.dense2(self.relu(self.dense1(X)))\n",
    "    \n",
    "\n",
    "ffn = PositionWiseFFN(4, 4, 8)\n",
    "ffn.eval()\n",
    "ffn(torch.ones((2, 3, 4))).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm: tensor([[-1.0000,  1.0000],\n",
      "        [-1.0000,  1.0000]], grad_fn=<NativeLayerNormBackward0>) \n",
      "batch norm: tensor([[-1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000]], grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = nn.LayerNorm(2)\n",
    "bn = nn.BatchNorm1d(2)\n",
    "X = torch.tensor([[1, 2], [2, 3]], dtype=torch.float32)\n",
    "\n",
    "print('layer norm:', ln(X), '\\nbatch norm:', bn(X)) # 可以发现layer norm是在同一个样本上的norm使其均值为0，方差为1， batch norm是多个样本之间同一个数据维度上的norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, normalize_shape, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalize_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(self.dropout(Y) + X)\n",
    "    \n",
    "add_norm = AddNorm([3, 4], 0.5)\n",
    "add_norm.eval()\n",
    "add_norm(torch.ones((2, 3, 4)), torch.ones((2, 3, 4))).shape\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, query_size, key_size, value_size, num_hiddens, normalize_shape,\n",
    "                 ffn_num_input, ffn_num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(query_size, key_size, value_size, num_hiddens, num_heads, dropout, bias=bias)\n",
    "        self.addnorm1 = AddNorm(normalize_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens) # feed forward network 的输出dim也是num_hiddens，这有利于残差相加\n",
    "        self.addnorm2 = AddNorm(normalize_shape, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        return self.addnorm2(Y, self.ffn(Y))\n",
    "    \n",
    "\n",
    "X = torch.ones((2, 100, 24))\n",
    "valid_lens = torch.tensor([3, 2])\n",
    "encoder_blk = EncoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5)\n",
    "encoder_blk.eval()\n",
    "encoder_blk(X, valid_lens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, query_size, key_size, value_size, num_hiddens, normalize_shape,\n",
    "                 ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i),\n",
    "                                 EncoderBlock(query_size, key_size, value_size, num_hiddens, normalize_shape,\n",
    "                                              ffn_num_input, ffn_num_hiddens, num_heads, dropout, bias))\n",
    "            \n",
    "    def forward(self, X, valid_lens, *args):\n",
    "        # 因为位置编码值在-1和1之间\n",
    "        # 因此嵌入值乘以嵌入维度的平方根进行缩放\n",
    "        # 然后再与位置编码相加\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)) # embedding层dim越大，每个值就越小，所以这里对postion encoding进行了缩放去匹配embedding的大小。\n",
    "        self.attention_weights = [None] * len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens)\n",
    "            self.attention_weights[i] = blk.attention.attention_weights\n",
    "        return X\n",
    "    \n",
    "\n",
    "encoder = TransformerEncoder(200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)\n",
    "encoder.eval()\n",
    "encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, query_size, key_size, value_size, num_hiddens, normalize_shape,\n",
    "                 ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.i = i #解码器中第i个块\n",
    "        self.masked_attention = MultiHeadAttention(query_size, key_size, value_size, num_hiddens, num_heads, dropout, bias)\n",
    "        self.addnorm1 = AddNorm(normalize_shape, dropout)\n",
    "        self.attention = MultiHeadAttention(query_size, key_size, value_size, num_hiddens, num_heads, dropout, bias)\n",
    "        self.addnorm2 = AddNorm(normalize_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm3 = AddNorm(normalize_shape, dropout)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "        # 训练阶段，输出序列的所有词元都在同一时间处理\n",
    "        # 因此state[2][self.i]初始化为None\n",
    "        # 预测阶段，输出序列是通过词元一个接着一个解码的\n",
    "        # 因此state[2][self.i]包含着直到当前时间不第i个块解码的输出表示。\n",
    "        if state[2][self.i] is None:\n",
    "            key_values = X # 训练\n",
    "        else:\n",
    "            key_values = torch.cat((state[2][self.i], X), axis=1)\n",
    "        state[2][self.i] = key_values\n",
    "        if self.training:\n",
    "            batch_size, num_steps, _ = X.shape\n",
    "            dec_valid_lens = torch.arange(1, num_steps+1, device=X.device).repeat(batch_size, 1)\n",
    "        else:\n",
    "            dec_valid_lens = None\n",
    "        \n",
    "        Y = self.addnorm1(X, self.masked_attention(X, key_values, key_values, dec_valid_lens))\n",
    "        Z = self.addnorm2(Y, self.attention(Y, enc_outputs, enc_outputs, enc_valid_lens))\n",
    "        return self.addnorm3(Z, self.ffn(Z)), state\n",
    "    \n",
    "\n",
    "decoder_blk = DecoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5, 0, bias=False)\n",
    "decoder_blk.eval()\n",
    "X = torch.ones((2, 100, 24))\n",
    "state = [encoder_blk(X, valid_lens), valid_lens, [None]]\n",
    "decoder_blk(X, state)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, query_size, key_size, value_size, num_hiddens, normalize_shape,\n",
    "                 ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i),\n",
    "                                 DecoderBlock(query_size, key_size, value_size, num_hiddens, normalize_shape, \n",
    "                                              ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, bias))\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        self._attention_weights = [[None] * len(self.blks) for _ in range(2)]\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X, state = blk(X, state)\n",
    "            # 解码器自注意力权重\n",
    "            self._attention_weights[0][i] = blk.masked_attention.attention.attention_weights\n",
    "            # \"编码器-解码器\"自注意力权重\n",
    "            self._attention_weights[1][i] = blk.attention.attention.attention_weights\n",
    "\n",
    "        return self.dense(X), state\n",
    "    \n",
    "    @property\n",
    "    def attetion_weights(self):\n",
    "        return self._attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
